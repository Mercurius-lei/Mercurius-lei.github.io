{"posts":[{"title":"Elasticsearch","text":"","link":"/2023/09/25/database_elasticsearch/"},{"title":"算法基础","text":"1. 常用函数123456789101112131415161718192021222324// 字符串相关String s;// 字符串长度int length = s.length();// 获取指定index的字符char c = s.charAt(index);// 字符串转char数组char[] cList = s.toCharArray();// char数组转字符串String s1 = new String(c);// String数组转ListString[] s2 =new String[] {&quot;hello&quot;, &quot;world&quot;};List&lt;String&gt; l2 = Arrays.asList(s2);// List转String数组String[] s3 = l2.toArray(new String[0]);// 获取子字符串String childString = s.substring(startIndex, endIndex);","link":"/2023/09/25/algorithm_base/"},{"title":"MySQL","text":"1. 索引2. 事务2.1 事务特点：ACID从业务角度出发，对数据库的一组操作要求保持4个特征： Atomicity（原子性）：一个事务必须被视为一个不可分割的最小工作单元，整个事务中的所有操作要么全部提交成功，要么全部失败回滚，对于一个事务来说，不可能只执行其中的一部分操作。 Consistency（一致性）：数据库总是从一个一致性状态转换到另一个一致状态。下面的银行列子会说到。 Isolation（隔离性）：通常来说，一个事务所做的修改在最终提交以前，对其他事务是不可见的。注意这里的“通常来说”，后面的事务隔离级级别会说到。 Durability（持久性）：一旦事务提交，则其所做的修改就会永久保存到数据库中。此时即使系统崩溃，修改的数据也不会丢失。（持久性的安全性与刷新日志级别也存在一定关系，不同的级别对应不同的数据安全级别） 注意： （1）事务的隔离性是通过锁、MVCC等实现 （2）事务的原子性、一致性和持久性则是通过事务日志实现 3. MVCC3.1 并发事务带来的问题 更新丢失（Lost Update）：当两个或多个事务选择同一行，然后基于最初选定的值更新该行时，由于每个事务都不知道其他事务的存在，就会发生丢失更新问题 －－最后的更新覆盖了由其他事务所做的更新。例如，两个编辑人员制作了同一 文档的电子副本。每个编辑人员独立地更改其副本，然后保存更改后的副本，这样就覆盖了原始文档。 最后保存其更改副本的编辑人员覆盖另一个编辑人员所做的更改。如果在一个编辑人员完成并提交事务之前，另一个编辑人员不能访问同 一文件，则可避免此问题。 脏读（Dirty Reads）：一个事务正在对一条记录做修改，在这个事务完成并提交前， 这条记录的数据就处于不一致状态； 这时， 另一个事务也来读取同一条记录，如果不加控制，第二个事务读取了这些“脏”数据，并据此做进一步的处理，就会产生未提交的数据依赖关系。这种现象被形象地叫做”脏读”。 不可重复读（Non-Repeatable Reads）：一个事务在读取某些数据后的某个时间，再次读取以前读过的数据，却发现其读出的数据已经发生了改变、或某些记录已经被删除了！这种现象就叫做“不可重复读” 。 幻读 （Phantom Reads）： 一个事务按相同的查询条件重新读取以前检索过的数据，却发现其他事务插入了满足其查询条件的新数据，这种现象就称为“幻读” 。 注： （1）幻读和不可重复读的区别： 不可重复读的重点是修改：在同一事务中，同样的条件，第一次读的数据和第二次读的数据不一样。（因为中间有其他事务提交了修改） 幻读的重点在于新增或者删除：在同一事务中，同样的条件，第一次和第二次读出来的记录数不一样。（因为中间有其他事务提交了插入/删除） （2）“更新丢失”通常是应该完全避免的。但防止更新丢失，并不能单靠数据库事务控制器来解决，需要应用程序对要更新的数据加必要的锁来解决，因此，防止更新丢失应该是应用的责任。 “脏读” 、 “不可重复读”和“幻读” ，其实都是数据库读一致性问题，必须由数据库提供一定的事务隔离机制来解决。 3.2 幻读并未彻底解决MySQL InnoDB 引擎的可重复读隔离级别（默认隔离级），根据不同的查询方式，分别提出了避免幻读的方案： 针对 快照读 （普通 select 语句），是通过 MVCC 方式解决了幻读。 针对 当前读 （select … for update 等语句），是通过 next-key lock（记录锁+间隙锁）方式解决了幻读。 我举例了两个发生幻读场景的例子。 第一个例子：对于快照读， MVCC 并不能完全避免幻读现象。因为当事务 A 更新了一条事务 B 插入的记录，那么事务 A 前后两次查询的记录条目就不一样了，所以就发生幻读。 第二个例子：对于当前读，如果事务开启后，并没有执行当前读，而是先快照读，然后这期间如果其他事务插入了一条记录，那么事务后续使用当前读进行查询的时候，就会发现两次查询的记录条目就不一样了，所以就发生幻读。 所以，MySQL 可重复读隔离级别并没有彻底解决幻读，只是很大程度上避免了幻读现象的发生。 要避免这类特殊场景下发生幻读的现象的话，就是尽量在开启事务之后，马上执行 select … for update 这类当前读的语句，因为它会对记录加 next-key lock，从而避免其他事务插入一条新记录。","link":"/2023/09/25/database_mysql/"},{"title":"Sql优化","text":"数据库 1、 分库分库 就是将数据库中的数据分散到不同的数据库上，可以垂直分库，也可以水平分库。 垂直分库 就是把单一数据库按照业务进行划分，不同的业务使用不同的数据库，进而将一个数据库的压力分担到多个数据库。例如：说你将数据库中的用户表、订单表和商品表分别单独拆分为用户数据库、订单数据库和商品数据库。 水平分库 是把同一个表按一定规则拆分到不同的数据库中，每个库可以位于不同的服务器上，这样就实现了水平扩展，解决了单表的存储和性能瓶颈的问题。例如：订单表数据量太大，你对订单表进行了水平切分（水平分表），然后将切分后的 2 张订单表分别放在两个不同的数据库。 ##2、分表 分表 就是对单表的数据进行拆分，可以是垂直拆分，也可以是水平拆分。 垂直分表 是对数据表列的拆分，把一张列比较多的表拆分为多张表。例如：我们可以将用户信息表中的一些列单独抽出来作为一个表。 水平分表 是对数据表行的拆分，把一张行比较多的表拆分为多张表，可以解决单一表数据量过大的问题。例如：我们可以将用户信息表拆分成多个用户信息表，这样就可以避免单一表数据量过大对性能造成影响。 ##3、分库分表场景 单表的数据达到千万级别以上，数据库读写速度比较缓慢。 数据库中的数据占用的空间越来越大，备份时间越来越长。 应用的并发量太大。 ##4、分库分表引入的问题 join 操作 事务问题 分布式 ID 跨库聚合查询问题 …… ##5、分库分表后的数据迁移 （1）停机更新 （2）增量迁移 （3）双写 ##6、MySQL Online DDL MySQL 的 DDL 有很多种方法。 MySQL 本身自带三种方法，分别是：copy、inplace、instant。 copy 算法为最古老的算法，在 MySQL 5.5 及以下为默认算法。MySQL 会建立一个新的临时表，把源表的所有数据写入到临时表，在此期间无法对源表进行数据写入。MySQL 在完成临时表的写入之后，用临时表替换掉源表。这个算法主要被早期（&lt;=5.5）版本所使用。 从 MySQL 5.6 开始，引入了 inplace 算法并且默认使用。inplace 算法还包含两种类型：rebuild-table 和 not-rebuild-table。MySQL 使用 inplace 算法时，会自动判断，能使用 not-rebuild-table 的情况下会尽量使用，不能的时候才会使用 rebuild-table。当 DDL 涉及到主键和全文索引相关的操作时，无法使用 not-rebuild-table，必须使用 rebuild-table。其他情况下都会使用 not-rebuild-table。 nplace 算法的操作阶段主要分为三个： Prepare阶段： - 创建新的临时 frm 文件(与 InnoDB 无关)。 - 持有 EXCLUSIVE-MDL 锁，禁止读写。 - 根据 alter 类型，确定执行方式（copy，online-rebuild，online-not-rebuild）。 更新数据字典的内存对象。 - 分配 row_log 对象记录数据变更的增量（仅 rebuild 类型需要）。 - 生成新的临时ibd文件 new_table（仅rebuild类型需要）。 Execute 阶段： 降级EXCLUSIVE-MDL锁，允许读写。 扫描old_table聚集索引（主键）中的每一条记录 rec。 遍历new_table的聚集索引和二级索引，逐一处理。 根据 rec 构造对应的索引项。 将构造索引项插入 sort_buffer 块排序。 将 sort_buffer 块更新到 new_table 的索引上。 记录 online-ddl 执行过程中产生的增量（仅 rebuild 类型需要）。 重放 row_log 中的操作到 new_table 的索引上（not-rebuild 数据是在原表上更新）。 重放 row_log 中的DML操作到 new_table 的数据行上。 Commit阶段： 当前 Block 为 row_log 最后一个时，禁止读写，升级到 EXCLUSIVE-MDL 锁。 重做 row_log 中最后一部分增量。 更新 innodb 的数据字典表。 提交事务（刷事务的 redo 日志）。 修改统计信息。 rename 临时 ibd 文件，frm文件。 变更完成，释放 EXCLUSIVE-MDL 锁。 从 MySQL 8.0.12 开始，引入了 instant 算法并且默认使用。目前 instant 算法只支持增加列等少量 DDL 类型的操作（可以直接修改表的 metadata 数据，省掉了 rebuild 的过程，极大的缩短了 DDL 语句的执行时间），其他类型仍然会默认使用 inplace。 有一些第三方工具也可以实现 DDL 操作，最常见的是 percona 的 pt-online-schema-change 工具（简称为 pt-osc），和 github 的 gh-ost 工具，均支持 MySQL 5.5 以上的版本。 需要注意以下几个方面： （1）负载 所有方式对大表做 DDL 都会增加负载，只是程度的不同，主要为 IO 的负载。如果是 IO 使用非常高的实例，建议在 IO 较小的时间段执行 DDL 操作。 （2）额外空间占用 copy、inplace rebuild-table、gh-ost、pt-online-schema-change，都会将表完整复制一份出来再做 DDL 变更，因此会使用和原表空间一样大（甚至更大，如果是加列的操作的话）的额外空间，另外还会生成大量的临时日志。要特别注意剩余空间，确保空间充裕，不然可能导致 DDL 过程中磁盘写满。 （3）主从同步延时 所有方式做 DDL 均会引发主从同步延时。其中 copy 和 inplace 算法，只有主完成了 DDL 操作之后，binlog 才会同步给从库，从库才能开始操作 DDL，从库操作完 DDL 之后才能开始操作其他语句，因此会造成巨大的（大概两倍 DDL 操作时间）的延时。其他方法产生的延时较小，但仍然可能有几秒的延时。 （4）MDL 所有方式做 DDL 均会产生 MDL（metadata lock）。除了 copy 模式会有持续性的锁（DDL 的整个过程期间无法向该表写入任何数据）之外，其他方式的 MDL 均为短暂的锁。 除了 copy 模式之外的所有模式，MDL 如下： 在 DDL 的开始阶段，申请该表的 EXCLUSIVE-MDL 锁，禁止读写 降级 EXCLUSIVE-MDL 锁，允许读写 在 DDL 的最终 COMMIT 阶段，升级 EXCLUSIVE-MDL 锁，禁止读写 其中的阶段一和阶段三，其 MDL 的持续时间都是非常短暂的，也就是申请到了 MDL 锁之后会在很快的时间（一般小于一秒）处理完成相关操作并释放锁，一般情况下是不会影响业务的。只有阶段二是真正在处理数据，持续时间一般较长。 但是，有可能出现在阶段一和阶段三，无法申请到 MDL 的情况。这是因为 MDL 和所有的读写语句都可能会产生冲突，如果是在申请 MDL 的时候，之前有读写的事务一直没有执行完成（或者执行完成之后一直没有 COMMIT），MDL 就会无法立刻申请到，这个时候，DDL 语句，以及所有在该 DDL 语句之后的读写事务，都会阻塞并等待之前的读写事务完成，导致整个实例处于不可用状态。这个时候 SHOW PROCESSLIST 看到的语句状态为 waiting for metadata lock。 （5）其他 MySQL 的 inplace 算法虽然支持在 DDL 过程中间的读写，但是对写入的数据量有上限，不能超过 innodb_online_alter_log_max_size（默认为 128M）。如果超过上限可能导致执行失败。 7、尽量设置字段为not null（1）聚合函数不准确 对于NULL值的列，使用聚合函数的时候会忽略NULL值。 （2）对于NULL值的列，是不能使用=表达式进行判断的，下面对name的查询是不成立的，必须使用is NULL。 （3）NULL和其他任何值进行运算都是NULL，包括表达式的值也是NULL。 user表第二条记录age是NULL，所以+1之后还是NULL，name是NULL，进行concat运算之后结果还是NULL。 （4）对于distinct和group by来说，所有的NULL值都会被视为相等，对于order by来说升序NULL会排在最前 （5）索引列存在NULL会导致优化器在做索引选择的时候更复杂，更加难以优化。~~（MySQL版本已优化） ## 8、MySQL in vs exists（版本优化已趋于接近，讨论无意义） - IN查询在内部表和外部表上都可以使用到索引；- Exists查询仅在内部表上可以使用到索引；- 当子查询结果集很大，而外部表较小的时候，Exists的Block Nested Loop(Block 嵌套循环)的作用开始显现，并弥补外部表无法用到索引的缺陷，查询效率会优于IN。- 当子查询结果集较小，而外部表很大的时候，Exists的Block嵌套循环优化效果不明显，IN 的外表索引优势占主要作用，此时IN的查询效率会优于Exists。 8、SQL优化总结到 SQL 优化中，就如下三点： 最大化利用索引。 尽可能避免全表扫描。 减少无效数据的查询。 参考资料1、https://javaguide.cn/high-performance/read-and-write-separation-and-library-subtable.html#%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8 2、100亿数据平滑数据迁移,不影响服务：https://www.w3cschool.cn/architectroad/architectroad-data-smooth-migration.html 3、MySQL 5.7 特性：Online DDL：https://cloud.tencent.com/developer/article/1697076 4、为什么数据库字段要使用NOT NULL：https://cloud.tencent.com/developer/article/1812479","link":"/2023/09/25/database_optimize/"},{"title":"常用设计模式","text":"1、在超类中加上新的行为，会使得某些并不适合该行为的子类也具有该行为。 因此为了复用而使用继承，并不完美。 1. 设计原则设计原则1找到应用中可能需要变化之处，把它们独立出来，不要和那些不需要变化的代码混在一起。 设计原则2针对接口编程，而不是针对实现编程。 充分利用多态的特性，变量的声明类型应该是一个超类型，通常是一个抽象类或者是一个接口。 2. 设计模式详解2.10 观察者模式（1）定义 观察者模式(Observer Pattern)定义了对象间一种一对多的依赖关系，使得当每一个对象改变状态，则所有依赖于它的对象都会得到通知并自动更新。观察者模式是一种对象行为型模式。观察者模式的别名包括发布-订阅（Publish/Subscribe）模式、模型-视图（Model/View）模式、源-监听器（Source/Listener）模式或从属者（Dependents）模式。细究的话，发布订阅和观察者有些不同，可以理解成发布订阅模式属于广义上的观察者模式。 （2）UML图 角色介绍： Subject（目标）：被观察者，它是指被观察的对象。 从类图中可以看到，类中有一个用来存放观察者对象的Vector 容器（之所以使用Vector而不使用List，是因为多线程操作时，Vector在是安全的，而List则是不安全的），这个Vector容器是被观察者类的核心，另外还有三个方法：attach方法是向这个容器中添加观察者对象；detach方法是从容器中移除观察者对象；notify方法是依次调用观察者对象的对应方法。这个角色可以是接口，也可以是抽象类或者具体的类，因为很多情况下会与其他的模式混用，所以使用抽象类的情况比较多。 ConcreteSubject（具体目标）：具体目标是目标类的子类，通常它包含经常发生改变的数据，当它的状态发生改变时，向它的各个观察者发出通知。同时它还实现了在目标类中定义的抽象业务逻辑方法（如果有的话）。如果无须扩展目标类，则具体目标类可以省略。 Observer（观察者）：观察者将对观察目标的改变做出反应，观察者一般定义为接口，该接口声明了更新数据的方法 update()，因此又称为抽象观察者。 ConcreteObserver（具体观察者）：在具体观察者中维护一个指向具体目标对象的引用，它存储具体观察者的有关状态，这些状态需要和具体目标的状态保持一致；它实现了在抽象观察者 Observer 中定义的 update()方法。通常在实现时，可以调用具体目标类的 attach() 方法将自己添加到目标类的集合中或通过 detach() 方法将自己从目标类的集合中删除。 （2）优缺点 优点 降低了目标与观察者之间的耦合关系，两者之间是抽象耦合关系 目标与观察者之间建立了一套触发机制 支持广播通信 符合“开闭原则”的要求 缺点 目标与观察者之间的依赖关系并没有完全解除，而且有可能出现循环引用 当观察者对象很多时，通知的发布会花费很多时间，影响程序的效率 （3）相关实现 Spring 事件驱动模型也是观察者模式很经典的应用。就是我们常见的项目中最常见的事件监听器。 2.11 访问者模式（1）定义 访问者模式的使用场景： 对象结构比较稳定，但经常需要在此对象结构上定义新的操作。 需要对一个对象结构中的对象进行很多不同的并且不相关的操作，而需要避免这些操作“污染”这些对象的类，也不希望在增加新操作时修改这些类。 （2）UML图 角色介绍 Visitor：接口或者抽象类，定义了对每个 Element 访问的行为，它的参数就是被访问的元素，它的方法个数理论上与元素的个数是一样的，因此，访问者模式要求元素的类型要稳定，如果经常添加、移除元素类，必然会导致频繁地修改 Visitor 接口，如果出现这种情况，则说明不适合使用访问者模式。 ConcreteVisitor：具体的访问者，它需要给出对每一个元素类访问时所产生的具体行为。 Element：元素接口或者抽象类，它定义了一个接受访问者（accept）的方法，其意义是指每一个元素都要可以被访问者访问。 ElementA、ElementB：具体的元素类，它提供接受访问的具体实现，而这个具体的实现，通常情况下是使用访问者提供的访问该元素类的方法。 ObjectStructure：定义当中所提到的对象结构，对象结构是一个抽象表述，它内部管理了元素集合，并且可以迭代这些元素提供访问者访问。 （3）优缺点 我们要根据具体情况来评估是否适合使用访问者模式，例如，我们的对象结构是否足够稳定，是否需要经常定义新的操作，使用访问者模式是否能优化我们的代码，而不是使我们的代码变得更复杂。 优点 各角色职责分离，符合单一职责原则通过UML类图和上面的示例可以看出来，Visitor、ConcreteVisitor、Element 、ObjectStructure，职责单一，各司其责。 具有优秀的扩展性如果需要增加新的访问者，增加实现类 ConcreteVisitor 就可以快速扩展。 使得数据结构和作用于结构上的操作解耦，使得操作集合可以独立变化员工属性（数据结构）和CEO、CTO访问者（数据操作）的解耦。 灵活性 缺点 具体元素对访问者公布细节，违反了迪米特原则CEO、CTO需要调用具体员工的方法。 具体元素变更时导致修改成本大变更员工属性时，多个访问者都要修改。 违反了依赖倒置原则，为了达到“区别对待”而依赖了具体类，没有以来抽象访问者 visit 方法中，依赖了具体员工的具体方法。 参考文献（1）观察者模式：https://juejin.cn/post/6844904100459446285 （2）访问者模式：https://www.jianshu.com/p/1f1049d0a0f4","link":"/2023/09/25/design_pattern/"},{"title":"分布式ID","text":"","link":"/2023/09/25/distributed_id/"},{"title":"分布式缓存","text":"1. 缓存的应用场景 高频访问的数据：限于磁盘 I/O 的瓶颈，对于高频访问的数据，需要缓存起来提高性能，降低下游数据库的压力冲击； 复杂运算的结果：对于需要耗费 CPU、经过复杂运算才能获得的结果，需要缓存来，做到“一劳长时间逸”，如 count(id)统计论坛在线人数； 读多写少：每次读都需要 select 甚至 join 很多表，数据库压力大，由于写得少，容易做到数据的一致性，非常适合缓存的应用； 一致性要求低：由于缓存的数据来源于数据库，在高并发时数据不一致性就比较凸显，不一致的问题可以解决但代价不菲； 2. 缓存读写策略2.1 旁路缓存模式（1）介绍 Cache Aside Pattern 是我们平时使用比较多的一个缓存读写模式，比较适合读请求比较多的场景。Cache Aside Pattern 中服务端需要同时维系 db 和 cache，并且是以 db 的结果为准。 写： 先更新 db 然后直接删除 cache 读： 从 cache 中读取数据，读取到就直接返回 cache 中读取不到的话，就从 db 中读取数据返回 再把数据放到 cache 中 （2）常见问题 问题一：在写数据的过程中，可以先删除 cache ，后更新 db 么？这样可能会造成 数据库（db）和缓存（Cache）数据不一致的问题。举例：请求 1 先写数据 A，请求 2 随后读数据 A 的话，就很有可能产生数据不一致性的问题。这个过程可以简单描述为：请求 1 先把 cache 中的 A 数据删除 -&gt; 请求 2 从 db 中读取数据-&gt;请求 1 再把 db 中的 A 数据更新 问题二：在写数据的过程中，先更新 db，后删除 cache 就没有问题了么？理论上来说还是可能会出现数据不一致性的问题，不过概率非常小，因为缓存的写入速度是比数据库的写入速度快很多。举例：请求 1 先读数据 A，请求 2 随后写数据 A，并且数据 A 在请求 1 请求之前不在缓存中的话，也有可能产生数据不一致性的问题。这个过程可以简单描述为：请求 1 从 db 读数据 A-&gt; 请求 2 更新 db 中的数据 A（此时缓存中无数据 A ，故不用执行删除缓存操作 ） -&gt; 请求 1 将数据 A 写入 cache （3）缺陷 缺陷一：首次请求数据一定不在 cache 的问题解决办法：可以将热点数据可以提前放入 cache 中。 缺陷二：写操作比较频繁的话导致 cache 中的数据会被频繁被删除，这样会影响缓存命中率解决办法： 数据库和缓存数据强一致场景：更新 db 的时候同样更新 cache，不过我们需要加一个锁/分布式锁来保证更新 cache 的时候不存在线程安全问题 可以短暂地允许数据库和缓存数据不一致的场景：更新 db 的时候同样更新 cache，但是给缓存加一个比较短的过期时间，这样的话就可以保证即使数据不一致的话影响也比较小 2.2 读写穿透模式（1）介绍 Read/Write Through Pattern 中服务端把 cache 视为主要数据存储，从中读取数据并将数据写入其中。cache 服务负责将此数据读取和写入 db，从而减轻了应用程序的职责。这种缓存读写策略小伙伴们应该也发现了在平时在开发过程中非常少见。抛去性能方面的影响，大概率是因为我们经常使用的分布式缓存 Redis 并没有提供 cache 将数据写入 db 的功能。 写（Write Through）： 先查 cache，cache 中不存在，直接更新 db cache 中存在，则先更新 cache，然后 cache 服务自己更新 db（同步更新 cache 和 db） 读(Read Through)： 从 cache 中读取数据，读取到就直接返回 读取不到的话，先从 db 加载，写入到 cache 后返回响应 （2）缺陷 缺陷一：首次请求数据一定不在 cache 的问题解决办法：同旁路缓存模式。 2.3 异步缓存写入模式（1）介绍 Write Behind Pattern 和 Read/Write Through Pattern 很相似，两者都是由 cache 服务来负责 cache 和 db 的读写。但是，两个又有很大的不同：Read/Write Through 是同步更新 cache 和 db，而 Write Behind 则是只更新缓存，不直接更新 db，而是改为异步批量的方式来更新 db。 很明显，这种方式对数据一致性带来了更大的挑战，比如 cache 数据可能还没异步更新 db 的话，cache 服务可能就就挂掉了。这种策略在我们平时开发过程中也非常非常少见，但是不代表它的应用场景少，比如消息队列中消息的异步写入磁盘、MySQL 的 Innodb Buffer Pool 机制都用到了这种策略。 Write Behind Pattern 下 db 的写性能非常高，非常适合一些数据经常变化又对数据一致性要求没那么高的场景，比如浏览量、点赞量。 3. 缓存常见问题3.1 缓存雪崩 Cache Avalanche缓存雪崩是指当大量缓存同时过期或缓存服务宕机，所有请求的都直接访问数据库，造成数据库高负载，影响性能，甚至数据库宕机，它和缓存击穿的区别在于失效 key 的数量。 针对 Redis 服务不可用的情况： 采用 Redis 集群，避免单机出现问题整个缓存服务都没办法使用。 限流，避免同时处理大量的请求。 针对热点缓存失效的情况： 设置不同的失效时间比如随机设置缓存的失效时间。 缓存永不失效（不太推荐，实用性太差）。 设置二级缓存。 缓存雪崩和缓存击穿有什么区别？ 缓存雪崩和缓存击穿比较像，但缓存雪崩导致的原因是缓存中的大量或者所有数据失效，缓存击穿导致的原因主要是某个热点数据不存在与缓存中（通常是因为缓存中的那份数据已经过期）。 3.2 缓存穿透 Cache Penetration缓存穿透是指数据库中没有符合条件的数据，缓存服务器中也就没有缓存数据，导致业务系统每次都绕过缓存服务器查询下游的数据库，缓存服务器完全失去了其应用的作用。如果黑客试图发起针对该 key 的大量访问攻击，数据库将不堪重负，最终可能导致崩溃宕机。 解决措施：（1）参数校验 最基本的就是首先做好参数校验，一些不合法的参数请求直接抛出异常信息返回给客户端。 （2）缓存无效key 如果缓存和数据库都查不到某个 key 的数据就写一个到 Redis 中去并设置过期时间，具体命令如下：SET key value EX 10086 。这种方式可以解决请求的 key 变化不频繁的情况，如果黑客恶意攻击，每次构建不同的请求 key，会导致 Redis 中缓存大量无效的 key 。很明显，这种方案并不能从根本上解决此问题。如果非要用这种方式来解决穿透问题的话，尽量将无效的 key 的过期时间设置短一点比如 1 分钟。 另外，这里多说一嘴，一般情况下我们是这样设计 key 的：表名:列名:主键名:主键值 。 （3）布隆过滤器(Bloom Filter) 需要注意的是布隆过滤器可能会存在误判的情况。总结来说就是：布隆过滤器说某个元素存在，小概率会误判。布隆过滤器说某个元素不在，那么这个元素一定不在。 3.3 缓存击穿 Cache Breakdown缓存击穿是指当某一 key 的缓存过期时大并发量的请求同时访问此 key，瞬间击穿缓存服务器直接访问数据库，让数据库处于负载的情况，缓存击穿一般发生在高并发的互联网应用场景。 解决措施： （1）设置热点数据永不过期或者过期时间比较长。 （2）针对热点数据提前预热，将其存入缓存中并设置合理的过期时间比如秒杀场景下的数据在秒杀结束之前不过期。 （3）请求数据库写数据到缓存之前，先获取互斥锁，保证只有一个请求会落到数据库上，减少数据库的压力。 缓存穿透和缓存击穿有什么区别？ 缓存穿透中，请求的 key 既不存在于缓存中，也不存在于数据库中。 缓存击穿中，请求的 key 对应的是 热点数据 ，该数据 存在于数据库中，但不存在于缓存中（通常是因为缓存中的那份数据已经过期） 。 3.4 缓存一致性以旁路缓存模式来说，需要增加 cache 更新重试机制（常用） ：如果 cache 服务当前不可用导致缓存删除失败的话，我们就隔一段时间进行重试，重试次数可以自己定。如果多次重试还是失败的话，我们可以把当前更新失败的 key 存入队列中，等缓存服务可用之后，再将缓存中对应的 key 删除即可。 todo：扩展学习：https://mp.weixin.qq.com/s?__biz=MzIyOTYxNDI5OA==&amp;mid=2247487312&amp;idx=1&amp;sn=fa19566f5729d6598155b5c676eee62d&amp;chksm=e8beb8e5dfc931f3e35655da9da0b61c79f2843101c130cf38996446975014f958a6481aacf1&amp;scene=178&amp;cur_album_id=1699766580538032128#rd。 参考文档（1）https://javaguide.cn/database/redis/3-commonly-used-cache-read-and-write-strategies.html","link":"/2023/09/27/distributed_cache/"},{"title":"分布式配置中心","text":"","link":"/2023/09/25/distributed_config/"},{"title":"分布式锁","text":"","link":"/2023/09/25/distributed_clock/"},{"title":"消息队列","text":"","link":"/2023/09/25/distributed_message_queue/"},{"title":"分布式事务","text":"","link":"/2023/09/25/distributed_transaction/"},{"title":"Zookeeper","text":"","link":"/2023/09/25/distributed_zookeeper/"},{"title":"Gradle","text":"","link":"/2023/09/25/java_gradle/"},{"title":"Maven","text":"Maven学习 1、介绍对于开发者来说，Maven 的主要作用主要有 3 个： 项目构建：提供标准的、跨平台的自动化项目构建方式。 依赖管理：方便快捷的管理项目依赖的资源（jar 包），避免资源间的版本冲突问题。 统一开发结构：提供标准的、统一的项目结构。 2、Maven 的依赖范围 compile：编译依赖范围（默认），使用此依赖范围对于编译、测试、运行三种都有效，即在编译、测试和运行的时候都要使用该依赖 Jar 包。 test：测试依赖范围，从字面意思就可以知道此依赖范围只能用于测试，而在编译和运行项目时无法使用此类依赖，典型的是 JUnit，它只用于编译测试代码和运行测试代码的时候才需要。 provided：此依赖范围，对于编译和测试有效，而对运行时无效。比如 servlet-api.jar 在 Tomcat 中已经提供了，我们只需要的是编译期提供而已。 runtime：运行时依赖范围，对于测试和运行有效，但是在编译主代码时无效，典型的就是 JDBC 驱动实现。 system：系统依赖范围，使用 system 范围的依赖时必须通过 systemPath 元素显示地指定依赖文件的路径，不依赖 Maven 仓库解析，所以可能会造成建构的不可移植。 3、Maven 依赖调解Maven 在遇到这种问题的时候，会遵循 路径最短优先 和 声明顺序优先 两大原则。 4、Maven生命周期Maven 定义了 3 个生命周期META-INF/plexus/components.xml： default 生命周期 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;phases&gt; &lt;!-- 验证项目是否正确，并且所有必要的信息可用于完成构建过程 --&gt; &lt;phase&gt;validate&lt;/phase&gt; &lt;!-- 建立初始化状态，例如设置属性 --&gt; &lt;phase&gt;initialize&lt;/phase&gt; &lt;!-- 生成要包含在编译阶段的源代码 --&gt; &lt;phase&gt;generate-sources&lt;/phase&gt; &lt;!-- 处理源代码 --&gt; &lt;phase&gt;process-sources&lt;/phase&gt; &lt;!-- 生成要包含在包中的资源 --&gt; &lt;phase&gt;generate-resources&lt;/phase&gt; &lt;!-- 将资源复制并处理到目标目录中，为打包阶段做好准备。 --&gt; &lt;phase&gt;process-resources&lt;/phase&gt; &lt;!-- 编译项目的源代码 --&gt; &lt;phase&gt;compile&lt;/phase&gt; &lt;!-- 对编译生成的文件进行后处理，例如对 Java 类进行字节码增强/优化 --&gt; &lt;phase&gt;process-classes&lt;/phase&gt; &lt;!-- 生成要包含在编译阶段的任何测试源代码 --&gt; &lt;phase&gt;generate-test-sources&lt;/phase&gt; &lt;!-- 处理测试源代码 --&gt; &lt;phase&gt;process-test-sources&lt;/phase&gt; &lt;!-- 生成要包含在编译阶段的测试源代码 --&gt; &lt;phase&gt;generate-test-resources&lt;/phase&gt; &lt;!-- 处理从测试代码文件编译生成的文件 --&gt; &lt;phase&gt;process-test-resources&lt;/phase&gt; &lt;!-- 编译测试源代码 --&gt; &lt;phase&gt;test-compile&lt;/phase&gt; &lt;!-- 处理从测试代码文件编译生成的文件 --&gt; &lt;phase&gt;process-test-classes&lt;/phase&gt; &lt;!-- 使用合适的单元测试框架（Junit 就是其中之一）运行测试 --&gt; &lt;phase&gt;test&lt;/phase&gt; &lt;!-- 在实际打包之前，执行任何的必要的操作为打包做准备 --&gt; &lt;phase&gt;prepare-package&lt;/phase&gt; &lt;!-- 获取已编译的代码并将其打包成可分发的格式，例如 JAR、WAR 或 EAR 文件 --&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;!-- 在执行集成测试之前执行所需的操作。 例如，设置所需的环境 --&gt; &lt;phase&gt;pre-integration-test&lt;/phase&gt; &lt;!-- 处理并在必要时部署软件包到集成测试可以运行的环境 --&gt; &lt;phase&gt;integration-test&lt;/phase&gt; &lt;!-- 执行集成测试后执行所需的操作。 例如，清理环境 --&gt; &lt;phase&gt;post-integration-test&lt;/phase&gt; &lt;!-- 运行任何检查以验证打的包是否有效并符合质量标准。 --&gt; &lt;phase&gt;verify&lt;/phase&gt; &lt;!-- 将包安装到本地仓库中，可以作为本地其他项目的依赖 --&gt; &lt;phase&gt;install&lt;/phase&gt; &lt;!-- 将最终的项目包复制到远程仓库中与其他开发者和项目共享 --&gt; &lt;phase&gt;deploy&lt;/phase&gt;&lt;/phases&gt; clean生命周期 site生命周期 todo： （1）provided依赖的包是否会打入最后的jar包？理论上不会，但是使用spring-boot-maven-plugin为了保证jar包能独立运行，最终仍会打包进去，有待验证。 （2）Gradle学习？","link":"/2023/09/25/java_maven/"},{"title":"Java IO","text":"","link":"/2023/09/25/java_io/"},{"title":"Mybatis","text":"","link":"/2023/09/25/java_mybatis/"},{"title":"Java版本新特性","text":"","link":"/2023/09/25/java_new_feature/"},{"title":"Spring &amp; SpringBoot","text":"SpringBoot学习 1、IoCIoC（Inversion of Control:控制反转） 是一种设计思想，而不是一个具体的技术实现。IoC 的思想就是将原本在程序中手动创建对象的控制权，交由 Spring 框架来管理。不过， IoC 并非 Spring 特有，在其他语言中也有应用。 为什么叫控制反转？ 控制：指的是对象创建（实例化、管理）的权力 反转：控制权交给外部环境（Spring 框架、IoC 容器） ##2、将一个类声明为 Bean 的注解 @Component：通用的注解，可标注任意类为 Spring 组件。如果一个 Bean 不知道属于哪个层，可以使用@Component 注解标注。 @Repository : 对应持久层即 Dao 层，主要用于数据库相关操作。 @Service : 对应服务层，主要涉及一些复杂的逻辑，需要用到 Dao 层。 @Controller : 对应 Spring MVC 控制层，主要用于接受用户请求并调用 Service 层返回数据给前端页面。 3、@Component 和 @Bean 的区别 @Component 注解作用于类，而@Bean注解作用于方法。 @Component通常是通过类路径扫描来自动侦测以及自动装配到 Spring 容器中（我们可以使用 @ComponentScan 注解定义要扫描的路径从中找出标识了需要装配的类自动装配到 Spring 的 bean 容器中）。@Bean 注解通常是我们在标有该注解的方法中定义产生这个 bean,@Bean告诉了 Spring 这是某个类的实例，当我需要用它的时候还给我。 @Bean 注解比 @Component 注解的自定义性更强，而且很多地方我们只能通过 @Bean 注解来注册 bean。比如当我们引用第三方库中的类需要装配到 Spring容器时，则只能通过 @Bean来实现。 ##4、@Autowired 和 @Resource 的区别是什么？ @Autowired 是 Spring 提供的注解，@Resource 是 JDK 提供的注解。 Autowired 默认的注入方式为byType（根据类型进行匹配），@Resource默认注入方式为 byName（根据名称进行匹配）。 当一个接口存在多个实现类的情况下，@Autowired 和@Resource都需要通过名称才能正确匹配到对应的 Bean。Autowired 可以通过 @Qualifier 注解来显式指定名称，@Resource可以通过 name 属性来显式指定名称。 @Autowired 支持在构造函数、方法、字段和参数上使用。@Resource 主要用于字段和方法上的注入，不支持在构造函数或参数上使用。 5、AOPAOP(Aspect-Oriented Programming:面向切面编程)能够将那些与业务无关，却为业务模块所共同调用的逻辑或责任（例如事务处理、日志管理、权限控制等）封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可拓展性和可维护性。 Spring AOP 就是基于动态代理的，如果要代理的对象，实现了某个接口，那么 Spring AOP 会使用 JDK Proxy（底层通过反射实现）或者CGLIB的动态代理（底层通过继承实现），去创建代理对象，而对于没有实现接口的对象，就无法使用 JDK Proxy 去进行代理了，这时候 Spring AOP 会使用 Cglib 生成一个被代理对象的子类来作为代理。 6、Spring AOP 和 AspectJ AOP 区别Spring AOP 属于运行时增强，而 AspectJ 是编译时增强。 Spring AOP 基于代理(Proxying)，而 AspectJ 基于字节码操作(Bytecode Manipulation)。 Spring AOP 已经集成了 AspectJ ，AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。AspectJ 相比于 Spring AOP 功能更加强大，但是 Spring AOP 相对来说更简单， 如果我们的切面比较少，那么两者性能差异不大。但是，当切面太多的话，最好选择 AspectJ ，它比 Spring AOP 快很多。 7、自动装配@SpringBootApplication看作是 @Configuration、@EnableAutoConfiguration、@ComponentScan 注解的集合。根据 SpringBoot 官网，这三个注解的作用分别是： @EnableAutoConfiguration：启用 SpringBoot 的自动配置机制。通过 SpringFactoriesLoader 最终加载META-INF/spring.factories中的自动配置类实现自动装配，自动配置类其实就是通过@Conditional按需加载的配置类，想要其生效必须引入spring-boot-starter-xxx包实现起步依赖 @Configuration：允许在上下文中注册额外的 bean 或导入其他配置类 @ComponentScan：扫描被@Component (@Service,@Controller)注解的 bean，注解默认会扫描启动类所在的包下所有的类 ，可以自定义不扫描某些 bean。如下图所示，容器中将排除TypeExcludeFilter和AutoConfigurationExcludeFilter。 ##8、Spring 框架中用到的设计模式 工厂设计模式 : Spring 使用工厂模式通过 BeanFactory、ApplicationContext 创建 bean 对象。 代理设计模式 : Spring AOP 功能的实现。 单例设计模式 : Spring 中的 Bean 默认都是单例的。 模板方法模式 : Spring 中 jdbcTemplate、hibernateTemplate 等以 Template 结尾的对数据库操作的类，它们就使用到了模板模式。 包装器设计模式 : 我们的项目需要连接多个数据库，而且不同的客户在每次访问中根据需要会去访问不同的数据库。这种模式让我们可以根据客户的需求能够动态切换不同的数据源。 观察者模式: Spring 事件驱动模型就是观察者模式很经典的一个应用。 适配器模式 :Spring AOP 的增强或通知(Advice)使用到了适配器模式、spring MVC 中也是用到了适配器模式适配Controller。 …… 参考资料1、https://javaguide.cn/system-design/framework/spring/spring-knowledge-and-questions-summary.html todo： （1）cglib实现原理？","link":"/2023/09/25/java_springboot/"},{"title":"Java线程池","text":"1. 线程池介绍顾名思义，线程池就是管理一系列线程的资源池，其提供了一种限制和管理线程资源的方式。每个线程池还维护一些基本统计信息，例如已完成任务的数量。 这里借用《Java 并发编程的艺术》书中的部分内容来总结一下使用线程池的好处： 降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。 提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。 线程池一般用于执行多个不相关联的耗时任务，没有多线程的情况下，任务顺序执行，使用了线程池的话可让多个不相关联的任务同时执行。 2. Java Executor框架Executor 框架是 Java5 之后引进的，在 Java 5 之后，通过 Executor 来启动线程比使用 Thread 的 start 方法更好，除了更易管理，效率更好（用线程池实现，节约开销）外，还有关键的一点：有助于避免 this 逃逸问题。 this 逃逸是指在构造函数返回之前其他线程就持有该对象的引用，调用尚未构造完全的对象的方法可能引发令人疑惑的错误。可参考：https://zhuanlan.zhihu.com/p/477481115 Executor 框架结构主要由三大部分组成： 任务(Runnable/Callable)：执行任务需要实现的 Runnable 接口 或 Callable接口。 任务的执行(Executor) 异步计算的结果(Future) 3. ThreadPoolExecutor 主要参数 corePoolSize: 任务队列未达到队列容量时，最大可以同时运行的线程数量。 maximumPoolSize: 任务队列中存放的任务达到队列容量的时候，当前可以同时运行的线程数量变为最大线程数。 workQueue: 新任务来的时候会先判断当前运行的线程数量是否达到核心线程数，如果达到的话，新任务就会被存放在队列中。 ThreadPoolExecutor其他常见参数 : keepAliveTime:线程池中的线程数量大于 corePoolSize 的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过了 keepAliveTime才会被回收销毁。 unit : keepAliveTime 参数的时间单位。 threadFactory :executor 创建新线程的时候会用到。 handler :饱和策略。 4. 饱和策略如果当前同时运行的线程数量达到最大线程数量并且队列也已经被放满了任务时，ThreadPoolTaskExecutor 定义一些策略: ThreadPoolExecutor.AbortPolicy：抛出 RejectedExecutionException来拒绝新任务的处理。 ThreadPoolExecutor.CallerRunsPolicy：调用执行自己的线程运行任务，也就是直接在调用execute方法的线程中运行(run)被拒绝的任务，如果执行程序已关闭，则会丢弃该任务。因此这种策略会降低对于新任务提交速度，影响程序的整体性能。如果您的应用程序可以承受此延迟并且你要求任何一个任务请求都要被执行的话，你可以选择这个策略。 ThreadPoolExecutor.DiscardPolicy：不处理新任务，直接丢弃掉。 ThreadPoolExecutor.DiscardOldestPolicy：此策略将丢弃最早的未处理的任务请求。 5. 线程池创建方式（1）通过ThreadPoolExecutor构造函数来创建（推荐） （2）通过 Executor 框架的工具类 Executors 来创建。 我们可以创建多种类型的 ThreadPoolExecutor： FixedThreadPool：该方法返回一个固定线程数量的线程池。该线程池中的线程数量始终不变。当有一个新的任务提交时，线程池中若有空闲线程，则立即执行。若没有，则新的任务会被暂存在一个任务队列中，待有线程空闲时，便处理在任务队列中的任务。 SingleThreadExecutor： 该方法返回一个只有一个线程的线程池。若多余一个任务被提交到该线程池，任务会被保存在一个任务队列中，待线程空闲，按先入先出的顺序执行队列中的任务。 CachedThreadPool： 该方法返回一个可根据实际情况调整线程数量的线程池。线程池的线程数量不确定，但若有空闲线程可以复用，则会优先使用可复用的线程。若所有线程均在工作，又有新的任务提交，则会创建新的线程处理任务。所有线程在当前任务执行完毕后，将返回线程池进行复用。 ScheduledThreadPool：该返回一个用来在给定的延迟后运行任务或者定期执行任务的线程池。 注：《阿里巴巴 Java 开发手册》强制线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 构造函数的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险（Executors创建的线程池，队列长度为Interger.MAX_VALUE，可能堆积大量的请求，导致 OOM）。 6. 阻塞队列不同的线程池会选用不同的阻塞队列，我们可以结合内置线程池来分析。 容量为 Integer.MAX_VALUE 的 LinkedBlockingQueue（无界队列）：FixedThreadPool 和 SingleThreadExector 。由于队列永远不会被放满，因此FixedThreadPool最多只能创建核心线程数的线程。 SynchronousQueue（同步队列）：CachedThreadPool 。SynchronousQueue 没有容量，不存储元素，目的是保证对于提交的任务，如果有空闲线程，则使用空闲线程来处理；否则新建一个线程来处理任务。也就是说，CachedThreadPool 的最大线程数是 Integer.MAX_VALUE ，可以理解为线程数是可以无限扩展的，可能会创建大量线程，从而导致 OOM。 DelayedWorkQueue（延迟阻塞队列）：ScheduledThreadPool 和 SingleThreadScheduledExecutor 。DelayedWorkQueue 的内部元素并不是按照放入的时间排序，而是会按照延迟的时间长短对任务进行排序，内部采用的是“堆”的数据结构，可以保证每次出队的任务都是当前队列中执行时间最靠前的。DelayedWorkQueue 添加元素满了之后会自动扩容原来容量的 1/2，即永远不会阻塞，最大扩容可达 Integer.MAX_VALUE，所以最多只能创建核心线程数的线程。 7. Runnable vs CallableRunnable自 Java 1.0 以来一直存在，但Callable仅在 Java 1.5 中引入,目的就是为了来处理Runnable不支持的用例。Runnable 接口不会返回结果或抛出检查异常，但是 Callable 接口可以。所以，如果任务不需要返回结果或抛出异常推荐使用 Runnable 接口，这样代码看起来会更加简洁。 8. execute() vs submit() execute()方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功与否； submit()方法用于提交需要返回值的任务。线程池会返回一个 Future 类型的对象，通过这个 Future 对象可以判断任务是否执行成功，并且可以通过 Future 的 get()方法来获取返回值，get()方法会阻塞当前线程直到任务完成，而使用 get（long timeout，TimeUnit unit）方法的话，如果在 timeout 时间内任务还没有执行完，就会抛出 java.util.concurrent.TimeoutException。 9. shutdown() vs shutdownNow() shutdown（） :关闭线程池，线程池的状态变为 SHUTDOWN。线程池不再接受新任务了，但是队列里的任务得执行完毕。 shutdownNow（） :关闭线程池，线程池的状态变为 STOP。线程池会终止当前正在运行的任务，并停止处理排队的任务并返回正在等待执行的 List。 10. isTerminated() vs isShutdown() isShutDown 当调用 shutdown() 方法后返回为 true。 isTerminated 当调用 shutdown() 方法后，并且所有提交的任务完成后返回为 true 11. 线程池和 ThreadLocal线程池和 ThreadLocal共用，可能会导致线程从ThreadLocal获取到的是旧值/脏数据。这是因为线程池会复用线程对象，与线程对象绑定的类的静态属性 ThreadLocal 变量也会被重用，这就导致一个线程可能获取到其他线程的ThreadLocal 值。 可考虑使用TransmittableThreadLocal， 项目地址：https://github.com/alibaba/transmittable-thread-local 。 参考文档（1）https://javaguide.cn/java/concurrent/java-thread-pool-summary.html","link":"/2023/09/25/java_threadpool/"},{"title":"Java日常使用","text":"字符串（1）使用 TypeReference序列化复合结构 12Map&lt;String, List&lt;String&gt;&gt; reqVO = JsonUtil.string2Obj(paramString, new TypeReference&lt;Map&lt;String, List&lt;String&gt;&gt;&gt;() {}); （2）正则处理 12// 手机号中间4位隐藏为 *phone.replaceAll(&quot;(\\\\d{3})\\\\d*(\\\\d{4})&quot;, &quot;$1****$2&quot;) 日期（1）LocalDataTime与其他类型互转 123456789// String to LocalDataTimeDateTimeFormatter DATE_TIME_FORMATTER = DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss&quot;);LocalDateTime startTime = LocalDateTime.parse(&quot;2024-02-06 12:00:00&quot;, DATE_TIME_FORMATTER);// LocalDataTime to StringString startTimeString = DATE_TIME_FORMATTER.format(startTime);// LocalDataTime to Long（时间戳，8位）long timestamp = LocalDateTime.now().toEpochSecond(ZoneOffset.of(&quot;+8&quot;)); （2）获取当天零点 1LocalDateTime.of(LocalDateTime.now().toLocalDate(), LocalTime.MIN); 数组（1）list to list 12345678// list 转 listList&lt;Integer&gt; l = Arrays.asList(2, 2, 3);List&lt;Integer&gt; l2 = l.stream().map(i -&gt; i + 1).collect(Collectors.toList());System.out.println(l2); // [3, 3, 4]// list 去重List&lt;Integer&gt; l3 = l.stream().distinct().collect(Collectors.toList());System.out.println(l3); // [2, 3] （2）list to map 123List&lt;Integer&gt; l = Arrays.asList(1, 2, 3);Map&lt;Integer, Integer&gt; m = l.stream().collect(Collectors.toMap(Function.identity(), Function.identity()));System.out.println(m); // {1=1, 2=2, 3=3} 集合（1）交集 123456789101112131415@Testpublic void test5() { String cacheRoomData = &quot;1,2,4&quot;; List&lt;Long&gt; finishedRoomIdList = Arrays.stream(cacheRoomData.split(&quot;,&quot;)) .map(s -&gt; Long.parseLong(s.trim())).collect(Collectors.toList()); // [1,2,4] List&lt;Long&gt; roomIdList = new ArrayList&lt;&gt;(Arrays.asList(1L,2L,3L,5L)); // [1,2,3,5] boolean updateCache = finishedRoomIdList.retainAll(roomIdList); // finishedRoomIdList 取交集 System.out.println(updateCache); // true System.out.println(finishedRoomIdList); // [1, 2] System.out.println(roomIdList); // [1, 2, 3, 5] roomIdList.removeAll(finishedRoomIdList); // roomIdList 取差集 System.out.println(finishedRoomIdList); // [1, 2] System.out.println(roomIdList); // [3, 5]} （2）初始化 12345// HashMap 初始化public static final Map&lt;String, String&gt; OCEAN_ENGINE_COOKIE_MAP = new HashMap&lt;String, String&gt;() {{ put(&quot;a&quot;, 1); put(&quot;b&quot;, 2);}}; （3）常规操作 12345678// HashMap computeIfAbsent: 如果没有对应元素，则进行初始化操作Map&lt;Integer, List&lt;String&gt;&gt; m = new HashMap&lt;&gt;();m.computeIfAbsent(1, k -&gt; new ArrayList&lt;&gt;()).add(&quot;a&quot;);m.computeIfAbsent(1, k -&gt; new ArrayList&lt;&gt;()).add(&quot;b&quot;);m.computeIfAbsent(2, k -&gt; new ArrayList&lt;&gt;()).add(&quot;c&quot;);m.computeIfAbsent(2, k -&gt; new ArrayList&lt;&gt;());m.computeIfAbsent(3, k -&gt; new ArrayList&lt;&gt;());System.out.println(m); // {1=[a, b], 2=[c], 3=[]} 参数校验（1）@Valid注解 常用注解如下: 注意在Controller参数前面加上@Valid123456import javax.validation.Valid;@PostMapping(&quot;/add&quot;)public void add(@Valid @RequestBody AddReqVO reqVO) { return;} 如果是多层嵌套，注意在每一层嵌套参数上加上@Valid123456789101112131415import javax.validation.Valid;import javax.validation.constraints.NotEmpty;public class AddReqVO { @NotEmpty(message = &quot;id不能为空&quot;) private String id; @Valid private Address address;}public static class Address { @NotEmpty(message = &quot;name不能为空&quot;) private String name;} 文件（1）MultipartFile 123456MultipartFile file;// 文件大小比较file.getSize() &gt; 1 * 1024 * 1024 // 1M大小// 文件名（包含文件后缀）String imageName = file.getOriginalFilename(); Mybatis（1）Mybatis Plus 添加 分页插件 1234567891011121314151617181920212223// 添加 config@Configurationpublic class MybatisPlusConfig { @Bean public MybatisPlusInterceptor mybatisPlusInterceptor(){ MybatisPlusInterceptor interceptor = new MybatisPlusInterceptor(); interceptor.addInnerInterceptor(new PaginationInnerInterceptor()); return interceptor; }}// LambdaQueryWrapper 使用// 注意，queryWrapper 存在缓存，如果需要重复使用 queryWrapper，使用前要调用 queryWrapper.clear();LambdaQueryWrapper&lt;Hello&gt; queryWrapper = new QueryWrapper&lt;Hello&gt;().lambda();queryWrapper.eq(Hello::getType, reqVO.getType()) .eq(Hello::getName, reqVO.getName()) .orderByDesc(Hello::getId());// Page 使用com.baomidou.mybatisplus.extension.plugins.pagination.Page&lt;Hello&gt; recordPage = new com.baomidou.mybatisplus.extension.plugins.pagination.Page&lt;&gt;(page, pageSize);com.baomidou.mybatisplus.extension.plugins.pagination.Page&lt;Hello&gt; recordList = this.getBaseMapper().selectPage(recordPage, queryWrapper); （2）MybatisPlusGenerator使用示例 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192import com.baomidou.mybatisplus.annotation.IdType;import com.baomidou.mybatisplus.generator.FastAutoGenerator;import com.baomidou.mybatisplus.generator.config.OutputFile;import com.baomidou.mybatisplus.generator.config.TemplateType;import com.baomidou.mybatisplus.generator.config.po.LikeTable;import com.baomidou.mybatisplus.generator.config.rules.NamingStrategy;import com.baomidou.mybatisplus.generator.engine.FreemarkerTemplateEngine;import java.util.HashMap;public class MybatisPlusGenerator { // 注意修改表名和作者名 public static final String TABLE_NAME = &quot;comment_record&quot;; public static final String AUTHOR = &quot;&quot;; // 暂时使用 tracking 数据库 public static final String MYSQL_HOST = &quot;jdbc:mysql://127.0.0.1:8080/tracking&quot; + &quot;?serverTimezone=Asia/Shanghai&amp;allowPublicKeyRetrieval=true&amp;verifyServerCertificate=false&amp;useSSL=false&quot;; public static final String MYSQL_USER_NAME = &quot;&quot;; public static final String MYSQL_USER_PASSWORD = &quot;&quot;; public static final String DATA_FORMAT = &quot;yyyy-MM-dd&quot;; public static final String PROJECT_NAME = &quot;commentservice&quot;; public static final String PARENT_PACKAGE = String.format(&quot;cn.com.hello.health.%s&quot;, PROJECT_NAME); public static final String SERVICE_PACKAGE = &quot;service.i&quot;; public static final String SERVICE_IMPL_PACKAGE = &quot;service.impl&quot;; public static final String ENTITY_PACKAGE = &quot;model.po&quot;; public static final String MAPPER_PACKAGE = &quot;mapper&quot;; public static final String SERVICE_PATH = String.format(&quot;src/main/java/cn/com/hello/%s/service/i&quot;, PROJECT_NAME); public static final String SERVICE_IMPL_PATH = String.format(&quot;src/main/java/cn/com/hello/%s/service/impl&quot;, PROJECT_NAME); public static final String ENTITY_PATH = String.format(&quot;src/main/java/cn/com/hello/%s/model/po&quot;, PROJECT_NAME); public static final String MAPPER_PATH = String.format(&quot;src/main/java/cn/com/hello/%s/mapper&quot;, PROJECT_NAME); public static final String XML_PATH = &quot;src/main/resources/mapper&quot;; public static void main(String[] args) { FastAutoGenerator .create(MYSQL_HOST, MYSQL_USER_NAME, MYSQL_USER_PASSWORD) .globalConfig(b -&gt; { //用户名，日期，swagger可选 b.author(AUTHOR).commentDate(DATA_FORMAT)// .enableSwagger() .disableOpenDir() .build(); }).packageConfig(b -&gt; { //只需要实体、mapper和xml b.parent(PARENT_PACKAGE) .service(SERVICE_PACKAGE) .serviceImpl(SERVICE_IMPL_PACKAGE) .entity(ENTITY_PACKAGE) .mapper(MAPPER_PACKAGE)// .xml(&quot;&quot;) .pathInfo(new HashMap&lt;OutputFile, String&gt;() {{ put(OutputFile.service, SERVICE_PATH); put(OutputFile.serviceImpl, SERVICE_IMPL_PATH); put(OutputFile.entity, ENTITY_PATH); // mapper接口的保存路径 put(OutputFile.mapper, MAPPER_PATH); // mapper.xml文件的保存路径 put(OutputFile.xml, XML_PATH); }}); }).templateConfig(b -&gt; b.disable(TemplateType.CONTROLLER// ,TemplateType.SERVICE// ,TemplateType.SERVICEIMPL // 阻止生成 mapper// ,TemplateType.MAPPER // 阻止生成 xml// ,TemplateType.XML )) .strategyConfig(b -&gt; { //表，驼峰命名，相关注解等更详细配置 TableField可选 b.addTablePrefix(&quot;_&quot;).likeTable(new LikeTable(TABLE_NAME)) .entityBuilder().naming(NamingStrategy.underline_to_camel).enableLombok().enableRemoveIsPrefix() // 开启链式模型。会在实体类前添加 [@Accessors(chain = true)] 注解。 // 用法如 [User user=new User().setAge(31).setName(&quot;snzl&quot;);]（这是Lombok的注解，需要添加Lombok依赖） .enableChainModel() .idType(IdType.AUTO)// .enableTableFieldAnnotation() .fileOverride() .mapperBuilder() // 开启 @Mapper 注解。 // 会在mapper接口上添加注解[@Mapper]// .enableMapperAnnotation() .enableBaseResultMap().enableBaseColumnList().fileOverride() .build(); })//engine可选，org.freemarker/org.apache.velocity .templateEngine(new FreemarkerTemplateEngine()) .execute(); }} easyexcel依赖引入 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;easyexcel&lt;/artifactId&gt; &lt;version&gt;3.2.1&lt;/version&gt;&lt;/dependency&gt; 实体类 123456789101112131415161718192021222324252627282930313233343536public class TestClass { @ExcelProperty(value = &quot;名称&quot;) private String name; // 按 String 读，转为 Long @ExcelProperty(value = &quot;编号&quot;, converter = LongStringConverter.class) private Long no; // 按 String 读，转为 LocalDate @ExcelProperty(value = &quot;日期&quot;,converter = LocalDateNumberConverter.class) private LocalDate date; // 按 String 读，转为 LocalDateTime @ExcelProperty(value = &quot;时间&quot;, converter = LocalDateTimeNumberConverter.class) private LocalDateTime time;}读取类```javaimport com.alibaba.excel.EasyExcel;@PostMapping(&quot;/upload-data&quot;)public void uploadData(@RequestParam(&quot;file&quot;) MultipartFile file) { if (file == null || file.isEmpty()) { return; } Data readResult = new Data(); try { EasyExcel.read(file.getInputStream(), UploadDataVO.class, new UploadDataListener(readResult)).sheet().doRead(); } catch (IOException e) { // }} UploadDataListener类 12345678910111213141516171819202122232425import com.alibaba.excel.context.AnalysisContext;import com.alibaba.excel.event.AnalysisEventListener;public class UploadDataListener extends AnalysisEventListener&lt;UploadDataVO&gt; { private Data readResult; public UploadDataListener(Data readResult) { this.readResult = readResult; } @Override public void invoke(UploadDataVO data, AnalysisContext context) { // data即为每行数据 // 获取当前行号 Integer currentRowNum = context.readRowHolder().getRowIndex() + 1; // 处理 readResult } @Override public void doAfterAllAnalysed(AnalysisContext context) { // 所有数据解析完成后可在此处执行其他操作 }}","link":"/2024/02/06/java_work/"},{"title":"Docker","text":"","link":"/2023/09/25/k8s_docker/"},{"title":"k8s监控","text":"1. 日志1.1 为什么选择loggie？Filebeat缺陷： 隔离性弱：由于所有的服务日志都会发送到全局唯一的Queue里，导致服务日志数据混在一起，在异常场景发生时，无法有隔离性的保障 不支持多个Output：有一些场景下，我们可能需要将不同服务的不同类型日志发送至不同的后端，但Filebeat无法使用同一个Agent去发送到不同的Kafka集群，只能在节点上部署多个Agent，导致维护和资源成本上升 可扩展性有限：相比Logstash/Flume等，Filebeat并非使用类似的input-&gt;queue-&gt;output的灵活多个pipeline设计，在对于日志数据的处理/过滤/增强上，依赖的是Filebeat有限的一些processor，可扩展性不足。同时Filebeat也无法作为中转聚合使用，在使用场景下大大受限，需要额外引入其他组件。另外类似日志报警等场景，Filebeat也无法满足 日志排障运维困境：Filebeat的metrics比较有限，很多时候我们想要排查诸如常见的日志是否有采集、采集的日志是否完整、发送是否有延迟等等排障场景，Filebeat没有提供相应的功能，十分影响线上的问题排查效率。而且Filebeat未提供Prometheus格式的监控指标，需要额外注入exporter 性能不够：虽然Filebeat性能尚可，但是在我们的实际使用时，遇到日志场景复杂、日志量大的情况时，存在吞吐量的瓶颈，无法满足实时性的需求。 总结一下，我们理想中的日志Agent是一个： 开箱即用：可快速部署容器化场景下的日志采集服务；有完善的文档与经验介绍； 高性能：比原生Filebeat性能高，资源占用少； 高可靠：隔离性稳定性更强；默认集成更多的监控指标，方便运维排障； 可扩展：基于微内核的架构，用户可方便快捷的写自己的插件，满足各种定制化需求； 1.2 Kubernetes下的日志采集在传统的使用虚拟机/云主机/物理机的时代，业务进程部署在固定的节点上，业务日志直接输出到宿主机上，运维只需要手动或者使用自动化工具把日志采集Agent部署在节点上，加一下Agent的配置，就可以开始采集日志了。 而在Kubernetes环境中，情况就没这么简单了： 动态迁移：在Kubernetes集群中经常存在Pod主动或者被动的迁移，频繁的销毁、创建，我们无法和传统的方式一样人为的给每个服务下发日志采集配置。 日志存储方式多样性：容器的日志存储方式有很多不同的类型，例如stdout、hostPath、emptyDir、pv等。 Kubernetes元信息：由于日志数据采集后会被集中存储，所以查询日志时，需要根据namespace、pod、container、node，甚至包括容器的环境变量、label等维度来检索、过滤，此时要求Agent感知并默认在日志里注入这些元信息。 1.3 Agent部署形式采集容器日志，Agent有两种部署方式： DaemonSet ：每个节点部署一个Agent Sidecar ：每个Pod增加一个Sidecar容器，运行日志Agent 两种部署方式的优劣都显而易见： 资源占用：DaemonSet每个节点上一个，而Sidecar每个Pod里一个，容器化形态下，往往一个Node上可能会跑很多的Pod，此时DaemonSet的方式远小于Sidecar，而且节点上Pod个数越多越明显 侵入性：Sidecar的方式，Agent需要注入到业务Pod中，不管是否有平台封装这一过程，还是采用Kubernetes webhook的方式默认注入，仍然改变了原本的部署方式 稳定性：日志采集在大部分的情况下，需要保障的是稳定性，最重要的是不能影响业务，如果采用Sidecar的方式，在Agent发生异常或者oom等情况，很容易对业务容器造成影响。另外，Agent比较多的时候，在连接数等方面会对下游服务比如Kafka造成一定的隐患。 隔离性：DaemonSet情况下，节点所有的日志都共用同一个Agent，而Sidecar方式，只会采集同一个Pod内的业务日志，此时Sidecar的隔离性理论上会好一些 性能：Sidecar由于只会采集该Pod里的日志，压力相对较小，极端情况下，达到Agent的性能瓶颈比DaemonSet方式概率也会小很多 1.4 日志系统架构与演进（1）小规模业务场景 每天的日志规模较小，比如只有几百G（预估500G以下）左右，日志的使用场景仅仅用于日常运维排查问题，可以采用Loggie直接发送至Elasticsearch集群的方式。 架构图如下所示： 优点： 架构简单，便于维护 缺点： 由于Elasticsearch的性能有限，在日志量级突然增大时，Agent直接发送可能会导致大量的重试或者失败，导致Elasticsearch不稳定 可扩展性较差 变种：因为一直以来ELK架构的流行，Elasticsearch是最常用的日志存储。如果有其他服务对Elasticsearch的依赖，或者有Elasticsearch的运维经验，Elasticsearch是一个还不错的选择。但是，Elasticsearch对资源和运维有一定的要求，在某些轻量级和资源敏感的环境下，可以考虑： 使用Loki存储 如果有相关的技术储备，还可以考虑发送至Clickhouse/Hive/Doris等。 （2）中型规模业务场景 在每天的日志量级稍大，比如在500G至1T的规模，架构和业务使用上有扩展性的考虑，可考虑引入Loggie中转集群。 优点： 中转机集群可以承担日志切分等能力 中转机集群有一定的缓冲能力 缺点： 缓冲能力相比消息队列较弱 （3）大型规模业务场景 如果日志量较大，比如1T以上场景，对性能与稳定性要求比较高，可考虑使用Kafka等消息队列集群。 需要注意的是，Kafka本身并不能直接发送至后端，所以这里需要考虑如何将Kafka的数据实时导入到后端存储中。这时候，我们可以选择一些组件消费Kafka，发送至后端，比如Loggie/Logstash/Kafka connect/Flink等。 但是Flink适合有自己的实时流平台或者运维能力的企业，否则可能引入更多运维成本。 优点： 使用消息队列比如Kafka，可以做到缓存和高峰期消峰 可以让更多的消费者消费Kafka，提供更多可扩展性 （4）超大型规模业务场景 几十TB至PB级，相比上面大规模场景，集群数量多，机房架构复杂，可以根据以上架构增加更多灵活的扩展。 比如： 使用Loggie的多Pipeline特性，将业务日志拆分发送至多个Kafka集群 在大规模架构下增加前置Loggie中转集群，提前进行分流和转发 最终我们可以基于Loggie，搭建一套生产级别的全链路日志数据平台。 1.5 日志存储选型对比综合对比，如下： 存储成本：Loki存储是裸数据，经过压缩后理论上空间是最小的。ES存储内容最多，因此存储成本比较高。而Hive、ClickHouse因只有列存，相对较小（对于比较随机的纯文本数据，列存理论上和字符串压缩接近）。 分析能力：Hive支持完整SQL92，并且没有计算量的限制，分析能力最强。ClickHouse支持的是有限SQL集，对常见的场景足够用。接下来是ES、Loki最弱。 查询速度：在建立索引情况下，ES是当之无愧的王者。基于MPP引擎的ClickHouse次之（对于带计算的分析，ClickHouse是最强的）。 分析成本：Loki最高，读取数据后大部分工作都需要外围完成。 查询成本：ES读取数据量最少，因此最优，接下来是ClickHouse，Hive和Loki。 2. Prometheus2.x 长尾问题使用rate或者increase函数去计算样本的平均增长速率，容易陷入“长尾问题”当中，其无法反应在时间窗口内样本数据的突发变化。 irate函数相比于rate函数提供了更高的灵敏度，不过当需要分析长期趋势或者在告警规则中，irate的这种灵敏度反而容易造成干扰。因此在长期趋势分析或者告警中更推荐使用rate函数。 rate、irate、increaserate() 此函数计算整个采样周期内每秒的增长率。 例如：rate(http_requests_total[5m]) 得出的是HTTP在5分钟窗口内，平均每秒的请求率。作为最常见的函数，它以可预测的每秒输出单位产生平滑的rate。 irate() 即 “瞬时rate”，此函数和rate()一样，计算每秒的增长率，但只对规定采样周期内的最后两个样本进行计算，而忽略前面所有样本。 例如：irate(http_requests_total[5m]) 选取规定5分钟窗口内的最后两个样本，并计算两者之间每秒的增长率。如果想让一个放大的图形显示出对rate变化的快速响应，那这个函数就很有用，它呈现的结果会比rate()的有更多的峰值。 increase() 此函数和 rate() 完全一样，只是它没有将最终单位转换为 “每秒”(1/s)。每个规定的采样周期就是它的最终单位。 例如：increase(http_requests_total[5m]) 得出的是5分钟的采样周期内处理完成的HTTP请求的总增长量（单位1/5m）。因此increase(foo[5m])/ (5 * 60) 等同于rate(foo[5m])。 这三个函数都有一个共同的要求，就是它们在规定的采样周期中都需要有至少两个样本才能运行。少于两个样本的序列将从结果中删除。 2.x Pull vs Push（1）介绍 pull模式：客户端使用library，变成exporter，然后prometheus server定时从exporter pull数据。 push模式：使用pushgateway，所有客户端push数据到pushgateway，然后prometheus server定时从pushgateway pull数据。 （2）对比 push模式的缺点：采用pushgateway的方式，如果某一个上报方挂了，pushgateway无法感知上报方的状态，所以这时候如果不做任何操作，那么prometheus依旧会从pushgateway中获取到旧的、不正确的数据。 pull的优点：采用exporter的方式，如果某一个exporter挂掉了，那么prometheus就pull不到数据，那么时序数据库没有新的数据产生。这是正确的。 所以pull模式是prometheus的推荐模式。 （3）数据时间戳 Pushgateway拒绝任何带有时间戳的推送，因为他pull抓取信息的时候会自己给信息加上时间戳。而exporter方式可以通过honor_timestamps配置来让Prometheus选择是否使用拉取数据中的时间戳。 2.x Alertmanager告警抑制功能较弱告警抑制的实现也是基于labels，但是是基于全局的，不是特定路由，而且只支持静态label，这个地方的设计其实不太好，有两个问题： 全局容易出现不同用户的规则互相影响，为了减少此种行为的发生，我们应该为每个路由设定一个抑制规则，同时必须包含路由的labels 静态label对label规范化增加了不必要的限制，所有数据都必须拥有指定的抑制labels才能使用 2.x Prometheus存储（1）本地存储 当前样本数据所在的块会被直接保存在内存中，不会持久化到磁盘中。为了确保 Prometheus 发生崩溃或重启时能够恢复数据，Prometheus 启动时会通过预写日志（write-ahead-log(WAL)）重新记录，从而恢复数据。 本地存储不可复制，无法构建集群，如果本地磁盘或节点出现故障，存储将无法扩展和迁移。因此我们只能把本地存储视为近期数据的短暂滑动窗口。如果你对数据持久化的要求不是很严格，可以使用本地磁盘存储多达数年的数据。 2.x Prometheus高可用prometheus 高可用有几种方案： 1.基本 HA：即两套 prometheus 采集完全一样的数据，外边挂负载均衡 2.HA + 远程存储：除了基础的多副本prometheus，还通过Remote write 写入到远程存储，解决存储持久化问题 3.联邦集群：即federation，按照功能进行分区，不同的 shard 采集不同的数据，由Global节点来统一存放，解决监控数据规模的问题。 4.使用thanos 或者victoriametrics，来解决全局查询、多副本数据 join 问题。 就算使用官方建议的多副本 + 联邦，由于prometheus的本地存储没有数据同步能力，要在保证可用性的前提下，再保持数据一致性是比较困难的，基础的 HA proxy 满足不了要求，比如： 集群的后端有 A 和 B 两个实例，A 和 B 之间没有数据同步。A 宕机一段时间，丢失了一部分数据，如果负载均衡正常轮询，请求打到A 上时，数据就会异常。 如果 A 和 B 的启动时间不同，时钟不同，那么采集同样的数据时间戳也不同，就不是多副本同样数据的概念了 就算用了远程存储，A 和 B 不能推送到同一个 tsdb，如果每人推送自己的 tsdb，数据查询走哪边就是问题了。 因此解决方案是在存储、查询两个角度上保证数据的一致: 存储角度：如果使用 remote write 远程存储， A 和 B后面可以都加一个 adapter，adapter做选主逻辑，只有一份数据能推送到 tsdb，这样可以保证一个异常，另一个也能推送成功，数据不丢，同时远程存储只有一份，是共享数据。方案可以参考这篇文章 查询角度：上边的方案实现很复杂且有一定风险，因此现在的大多数方案在查询层面做文章，比如thanos 或者victoriametrics，仍然是两份数据，但是查询时做数据去重和join。只是 thanos是通过 sidecar 把数据放在对象存储，victoriametrics是把数据remote write 到自己的 server 实例，但查询层 thanos-query 和victor的 promxy的逻辑基本一致。 我们采用了thanos来支持多地域监控数据，具体方案可以看https://xie.infoq.cn/article/e723b90fabb9b00437d0de96b 参考文档（1）高可用prometheus常见问题：https://yasongxu.gitbook.io/container-monitor/yi-.-kai-yuan-fang-an/di-2-zhang-prometheus/prometheus-use","link":"/2023/09/25/k8s_monitor/"},{"title":"网络基础知识点","text":"1. HTTP vs HTTPS（1）HTTP 协议介绍 HTTP 是应用层协议，全称超文本传输协议（Hypertext Transfer Protocol），它以 TCP（传输层）作为底层协议，默认端口为 80。 通信过程主要如下： 服务器在 80 端口等待客户的请求。 浏览器发起到服务器的 TCP 连接（创建套接字 Socket）。 服务器接收来自浏览器的 TCP 连接。 浏览器（HTTP 客户端）与 Web 服务器（HTTP 服务器）交换 HTTP 消息。 关闭 TCP 连接。 （2）HTTPS 协议介绍 HTTPS 协议（Hyper Text Transfer Protocol Secure），是 HTTP 的加强安全版本。HTTPS 是基于 HTTP 的，也是用 TCP 作为底层协议，并额外使用 SSL/TLS 协议用作加密和安全认证。默认端口号是 443。 注：SSL 和 TLS 没有太大的区别。SSL 指安全套接字协议（Secure Sockets Layer），首次发布与 1996 年。SSL 的首次发布其实已经是他的 3.0 版本，SSL 1.0 从未面世，SSL 2.0 则具有较大的缺陷（DROWN 缺陷——Decrypting RSA with Obsolete and Weakened eNcryption）。很快，在 1999 年，SSL 3.0 进一步升级，新版本被命名为 TLS 1.0。因此，TLS 是基于 SSL 之上的，但由于习惯叫法，通常把 HTTPS 中的核心加密协议混称为 SSL/TLS。 （3）对比 端口号 ：HTTP 默认是 80，HTTPS 默认是 443。 URL 前缀 ：HTTP 的 URL 前缀是 http://，HTTPS 的 URL 前缀是 https://。 安全性和资源消耗 ：HTTP 协议运行在 TCP 之上，所有传输的内容都是明文，客户端和服务器端都无法验证对方的身份。HTTPS 是运行在 SSL/TLS 之上的 HTTP 协议，SSL/TLS 运行在 TCP 之上。所有传输的内容都经过加密，加密采用对称加密，但对称加密的密钥用服务器方的证书进行了非对称加密。所以说，HTTP 安全性没有 HTTPS 高，但是 HTTPS 比 HTTP 耗费更多服务器资源。 2. HTTP/1.0, /1.1, /2.0（1）HTTP/1.0 vs HTTP/1.1 连接方式 : HTTP/1.0 为短连接，HTTP/1.1 支持长连接。 状态响应码 : HTTP/1.1 中新加入了大量的状态码，光是错误响应状态码就新增了 24 种。比如说，100 (Continue)——在请求大资源前的预热请求，206 (Partial Content)——范围请求的标识码，409 (Conflict)——请求与当前资源的规定冲突，410 (Gone)——资源已被永久转移，而且没有任何已知的转发地址。 缓存机制 : 在 HTTP/1.0 中主要使用 Header 里的 If-Modified-Since,Expires 来做为缓存判断的标准，HTTP/1.1 则引入了更多的缓存控制策略例如 Entity tag，If-Unmodified-Since, If-Match, If-None-Match 等更多可供选择的缓存头来控制缓存策略。 带宽 ：HTTP/1.0 中，存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象送过来了，并且不支持断点续传功能，HTTP/1.1 则在请求头引入了 range 头域，它允许只请求资源的某个部分，即返回码是 206（Partial Content），这样就方便了开发者自由的选择以便于充分利用带宽和连接。 Host 头（Host Header）处理 :HTTP/1.1 引入了 Host 头字段，允许在同一 IP 地址上托管多个域名，从而支持虚拟主机的功能。而 HTTP/1.0 没有 Host 头字段，无法实现虚拟主机。 （2）HTTP/1.1 vs HTTP/2.0 多路复用（Multiplexing） ：HTTP/2.0 在同一连接上可以同时传输多个请求和响应（可以看作是 HTTP/1.1 中长链接的升级版本），互不干扰。HTTP/1.1 则使用串行方式，每个请求和响应都需要独立的连接，而浏览器为了控制资源会有 6-8 个 TCP 连接都限制。。这使得 HTTP/2.0 在处理多个请求时更加高效，减少了网络延迟和提高了性能。 二进制帧（Binary Frames） ：HTTP/2.0 使用二进制帧进行数据传输，而 HTTP/1.1 则使用文本格式的报文。二进制帧更加紧凑和高效，减少了传输的数据量和带宽消耗。 头部压缩（Header Compression） ：HTTP/1.1 支持 Body压缩，Header不支持压缩。HTTP/2.0 支持对 Header压缩，使用了专门为 Header压缩而设计的 HPACK 算法，减少了网络开销。 服务器推送（Server Push） ：HTTP/2.0 支持服务器推送，可以在客户端请求一个资源时，将其他相关资源一并推送给客户端，从而减少了客户端的请求次数和延迟。而 HTTP/1.1 需要客户端自己发送请求来获取相关资源。 可以看到，HTTP/2.0 的多路复用使得不同的请求可以共用一个 TCP 连接，避免建立多个连接带来不必要的额外开销，而 HTTP/1.1 中的每个请求都会建立一个单独的连接。 3. TCP3.1 TCP状态转移图 3.2 TCP传输可靠性保证（1）TCP 如何保证传输的可靠性？ 基于数据块传输 ：应用数据被分割成 TCP 认为最适合发送的数据块，再传输给网络层，数据块被称为报文段或段。 对失序数据包重新排序以及去重 ：TCP 为了保证不发生丢包，就给每个包一个序列号，有了序列号能够将接收到的数据根据序列号排序，并且去掉重复序列号的数据就可以实现数据包去重。 校验和 : TCP 将保持它首部和数据的检验和。这是一个端到端的检验和，目的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错，TCP 将丢弃这个报文段和不确认收到此报文段。 超时重传 : 当发送方发送数据之后，它启动一个定时器，等待目的端确认收到这个报文段。接收端实体对已成功收到的包发回一个相应的确认信息（ACK）。如果发送端实体在合理的往返时延（RTT）内未收到确认消息，那么对应的数据包就被假设为已丢失并进行重传。 流量控制 : TCP 连接的每一方都有固定大小的缓冲空间，TCP 的接收端只允许发送端发送接收端缓冲区能接纳的数据。当接收方来不及处理发送方的数据，能提示发送方降低发送的速率，防止包丢失。TCP 使用的流量控制协议是可变大小的滑动窗口协议（TCP 利用滑动窗口实现流量控制）。 拥塞控制 : 当网络拥塞时，减少数据的发送。 （2） 参考文档（1）HTTP vs HTTPS（应用层）：https://javaguide.cn/cs-basics/network/http-vs-https.html TODO（1）完善慢启动、拥塞避免、快重传、快恢复内容。","link":"/2023/09/25/network_base/"},{"title":"分布式基础知识点","text":"1. 什么是分布式？（1）含义 分布式系统是由一组通过网络进行通信、为了完成共同的任务而协调工作的计算机节点组成的系统。分布式系统的出现是为了用廉价的、普通的机器完成单个计算机无法完成的计算、存储任务。其目的是 利用更多的机器，处理更多的数据 。 首先需要明确的是，只有当单个节点的处理能力无法满足日益增长的计算、存储任务的时候，且硬件的提升（加内存、加磁盘、使用更好的CPU）高昂到得不偿失的时候，应用程序也不能进一步优化的时候，我们才需要考虑分布式系统。因为，分布式系统要解决的问题本身就是和单机系统一样的，而由于分布式系统多节点、通过网络通信的拓扑结构，会引入很多单机系统没有的问题，为了解决这些问题又会引入更多的机制、协议，带来更多的问题。 （2）挑战 分布式系统需要大量机器协作，面临诸多的挑战： 异构的机器与网络：分布式系统中的机器，配置不一样，其上运行的服务也可能由不同的语言、架构实现，因此处理能力也不一样；节点间通过网络连接，而不同网络运营商提供的网络的带宽、延时、丢包率又不一样。怎么保证大家齐头并进，共同完成目标，这四个不小的挑战。 普遍的节点故障：虽然单个节点的故障概率较低，但节点数目达到一定规模，出故障的概率就变高了。分布式系统需要保证故障发生的时候，系统仍然是可用的，这就需要监控节点的状态，在节点故障的情况下将该节点负责的计算、存储任务转移到其他节点 不可靠的网络：节点间通过网络通信，而网络是不可靠的。可能的网络问题包括：网络分割、延时、丢包、乱序。相比单机过程调用，网络通信最让人头疼的是超时：节点A向节点B发出请求，在约定的时间内没有收到节点B的响应，那么B是否处理了请求，这个是不确定的，这个不确定会带来诸多问题，最简单的，是否要重试请求，节点B会不会多次处理同一个请求。 总而言之，分布式的挑战来自 不确定性 ，不确定计算机什么时候crash、断电，不确定磁盘什么时候损坏，不确定每次网络通信要延迟多久，也不确定通信对端是否处理了发送的消息。而分布式的规模放大了这个不确定性，不确定性是令人讨厌的，所以有诸多的分布式理论、协议来保证在这种不确定性的情况下，系统还能继续正常工作。 而且，很多在实际系统中出现的问题，来源于设计时的盲目乐观，觉得这个、那个应该不会出问题。 0、基本原则CAP指的是在一个分布式系统中： 一致性（Consistency） （等同于所有节点访问同一份最新的数据副本） 可用性（Availability）（每次请求都能获取到非错的响应——但是不保证获取的数据为最新数据） 分区容错性（Partition tolerance）（以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在 C 和 A 之间做出选择） 这三个要素最多只能同时实现两点，不可能三者兼顾。 如果你的实际业务场景，更需要的是保证数据一致性。那么请使用CP类型的分布式锁，比如：zookeeper，它是基于磁盘的，性能可能没那么好，但数据一般不会丢。 如果你的实际业务场景，更需要的是保证数据高可用性。那么请使用AP类型的分布式锁，比如：redis，它是基于内存的，性能比较好，但有丢失数据的风险。 其实，在我们绝大多数分布式业务场景中，使用redis分布式锁就够了，真的别太较真。因为数据不一致问题，可以通过最终一致性方案解决。但如果系统不可用了，对用户来说是暴击一万点伤害。 一般来说，用Redis控制共享资源并且还要求数据安全要求较高的话，最终的保底方案是对业务数据做幂等控制，这样一来，即使出现多个客户端获得锁的情况也不会影响数据的一致性。 #1、分布式服务接口幂等性 其实保证幂等性（比如不能重复扣款）主要是三点： （1）对于每个请求必须有一个唯一的标识，举个例子：订单支付请求，肯定得包含订单ID，一个订单ID最多支付一次，对吧 （2）每次处理完请求之后，必须有一个记录标识这个请求处理过了，比如说常见得方案是再mysql中记录个状态啥得，比如支付之前记录一条这个订单得支付流水，而且支付流水采用order id作为唯一键（unique key）。只有成功插入这个支付流水，才可以执行实际得支付扣款 （3）每次接收请求需要进行判断之前是否处理过得逻辑处理，比如说，如果有一个订单已经支付了，就已经有了一条支付流水，那么如果重复发送这个请求，则此时先插入支付流水，order id已经存在了，唯一键约束生效，报错插入不进去得。然后你就不用再扣款了。 #2、zk的使用场景 （1）分布式协调：这个其实就是zk很经典的一个用法，简单来说，就好比，你系统A发送个请求到mq，然后B消费了之后处理。那A系统如何指导B系统的处理结果？用zk就可以实现分布式系统之间的协调工作。A系统发送请求之后可以在zk上对某个节点的值注册个监听器，一旦B系统处理完了就修改zk那个节点的值，A立马就可以收到通知，完美解决。 （2）分布所锁：对某一个数据联系发出两个修改操作，两台机器同时收到请求，但是只能一台机器先执行另外一个机器再执行，那么此时就可以使用zk分布式锁，一个机器接收到了请求之后先获取zk上的一把分布式锁，就是可以去创建一个znode，接着执行操作，然后另外一个机器也尝试去创建那个znode，结果发现自己创建不了，因为被别人创建了，那只能等着，等等一个机器执行完了自己再执行。 zk分布式锁，其实做的比较简单，就是某个节点尝试创建临时znode（防止死锁呗），此时创建成功了就获取了这个锁，这个时候别的客户端来创建锁会失败，只能注册监听器来监听这个锁，释放锁就是删除这个znode，一旦释放掉就会反向通知客户端，然后等待着的客户端就可以再次尝试重新加锁。 （3）配置信息管理：zk可以用作很多系统的配置信息的管理，比如kafka，storm等等很多分布式系统都会选用zk来做一些元数据，配置信息的管理 （4）HA高可用性：这个应该是很常见的，比如hdfs，yarn等很多大数据系统，都选择基于zk来开发HA高可用机制，就是一个重要进程一般会主备两个，主进程挂了立马通过zk感知到切换到备份进程 3、分布式锁##（1）分布式锁特性 互斥性：在任何时刻，对于同一条数据，只有一台应用可以获取到分布式锁； 高可用性：在分布式场景下，一小部分服务器宕机不影响正常使用，这种情况就需要将提供分布式锁的服务以集群的方式部署； 防止锁超时：如果客户端没有主动释放锁，服务器会在一段时间之后自动释放锁，防止客户端宕机或者网络不可达时产生死锁； 独占性：加锁解锁必须由同一台服务器进行，也就是锁的持有者才可以释放锁，不能出现你加的锁，别人给你解锁了； ##（2）分布式锁的缺陷 客户端长时间阻塞导致锁失效问题：客户端1得到了锁，因为网络问题或者GC等原因导致长时间阻塞，然后业务程序还没执行完锁就过期了，这时候客户端2也能正常拿到锁，可能会导致线程安全的问题。 redis服务器时钟漂移问题：如果redis服务器的机器时钟发生了向前跳跃，就会导致这个key过早超时失效，比如说客户端1拿到锁后，key的过期时间是12:02分，但redis服务器本身的时钟比客户端快了2分钟，导致key在12:00的时候就失效了，这时候，如果客户端1还没有释放锁的话，就可能导致多个客户端同时持有同一把锁的问题。 单点实例安全问题：如果redis是单master模式的，当这台机宕机的时候，那么所有的客户端都获取不到锁了，为了提高可用性，可能就会给这个master加一个slave，但是因为redis的主从同步是异步进行的，可能会出现客户端1设置完锁后，master挂掉，slave提升为master，因为异步复制的特性，客户端1设置的锁丢失了，这时候客户端2设置锁也能够成功，导致客户端1和客户端2同时拥有锁。 （2）Redis中lua脚本保证操作原子性redis中使用lua脚本，可以保证操作的原子性，原因： 12“Atomicity of scriptsRedis uses the same Lua interpreter to run all the commands. Also Redis guarantees that a script is executed in an atomic way: no other script or Redis command will be executed while a script is being executed. This semantic is similar to the one of MULTI / EXEC. From the point of view of all the other clients the effects of a script are either still not visible or already completed.” （）Redis分布式锁说道Redis分布式锁大部分人都会想到：setnx+lua，或者知道 set key value px milliseconds nx。后一种方式的核心实现命令如下： 123456789- 获取锁（unique_value可以是UUID等）SET resource_name unique_value NX PX 30000- 释放锁（lua脚本中，一定要比较value，防止误解锁）if redis.call(&quot;get&quot;,KEYS[1]) == ARGV[1] then return redis.call(&quot;del&quot;,KEYS[1])else return 0end 这种实现方式有3大要点（也是面试概率非常高的地方）： set命令要用 set key value px milliseconds nx； value要具有唯一性； 释放锁时要验证value值，不能误解锁； 事实上这类琐最大的缺点就是它加锁时只作用在一个Redis节点上，即使Redis通过sentinel保证高可用，如果这个master节点由于某些原因发生了主从切换，那么就会出现锁丢失的情况： 在Redis的master节点上拿到了锁； 但是这个加锁的key还没有同步到slave节点； master故障，发生故障转移，slave节点升级为master节点； 导致锁丢失。 ##（）Redlock算法 在Redis的分布式环境中，我们假设有N个Redis master。这些节点完全互相独立，不存在主从复制或者其他集群协调机制。我们确保将在N个实例上使用与在Redis单实例下相同方法获取和释放锁。现在我们假设有5个Redis master节点，同时我们需要在5台服务器上面运行这些Redis实例，这样保证他们不会同时都宕掉。 为了取到锁，客户端应该执行以下操作: 获取当前Unix时间，以毫秒为单位。 依次尝试从5个实例，使用相同的key和具有唯一性的value（例如UUID）获取锁。当向Redis请求获取锁时，客户端应该设置一个网络连接和响应超时时间，这个超时时间应该小于锁的失效时间。例如你的锁自动失效时间为10秒，则超时时间应该在5-50毫秒之间。这样可以避免服务器端Redis已经挂掉的情况下，客户端还在死死地等待响应结果。如果服务器端没有在规定时间内响应，客户端应该尽快尝试去另外一个Redis实例请求获取锁。 客户端使用当前时间减去开始获取锁时间（步骤1记录的时间）就得到获取锁使用的时间。当且仅当从大多数（N/2+1，这里是3个节点）的Redis节点都取到锁，并且使用的时间小于锁失效时间时，锁才算获取成功。 如果取到了锁，key的真正有效时间等于有效时间减去获取锁所使用的时间（步骤3计算的结果）。 如果因为某些原因，获取锁失败（没有在至少N/2+1个Redis实例取到锁或者取锁时间已经超过了有效时间），客户端应该在所有的Redis实例上进行解锁（即便某些Redis实例根本就没有加锁成功，防止某些节点获取到锁但是客户端没有得到响应而导致接下来的一段时间不能被重新获取锁）。 ##（）Redis可重入锁 加锁主要是通过以下脚本实现的： 123456789101112if (redis.call('exists', KEYS[1]) == 0) then redis.call('hset', KEYS[1], ARGV[2], 1); redis.call('pexpire', KEYS[1], ARGV[1]); return nil; end;if (redis.call('hexists', KEYS[1], ARGV[2]) == 1) then redis.call('hincrby', KEYS[1], ARGV[2], 1); redis.call('pexpire', KEYS[1], ARGV[1]); return nil; end;return redis.call('pttl', KEYS[1]);1.2.3.4.5.6.7.8.9.10.11.12. 其中： KEYS[1]：锁名 ARGV[1]：过期时间 ARGV[2]：uuid + “:” + threadId，可认为是requestId 先判断如果锁名不存在，则加锁。 接下来，判断如果锁名和requestId值都存在，则使用hincrby命令给该锁名和requestId值计数，每次都加1。注意一下，这里就是重入锁的关键，锁重入一次值就加1。 如果锁名存在，但值不是requestId，则返回过期时间。 释放锁主要是通过以下脚本实现的： 123456789101112131415if (redis.call('hexists', KEYS[1], ARGV[3]) == 0) then return nilendlocal counter = redis.call('hincrby', KEYS[1], ARGV[3], -1);if (counter &gt; 0) then redis.call('pexpire', KEYS[1], ARGV[2]); return 0; else redis.call('del', KEYS[1]); redis.call('publish', KEYS[2], ARGV[1]); return 1; end; return nil1.2.3.4.5.6.7.8.9.10.11.12.13.14.15. 先判断如果锁名和requestId值不存在，则直接返回。 如果锁名和requestId值存在，则重入锁减1。 如果减1后，重入锁的value值还大于0，说明还有引用，则重试设置过期时间。 如果减1后，重入锁的value值还等于0，则可以删除锁，然后发消息通知等待线程抢锁。 （）读写锁与锁分段提升redis分布式锁性能 区分读写锁 将大锁分段：在java中ConcurrentHashMap，就是将数据分为16段，每一段都有单独的锁，并且处于不同锁段的数据互不干扰，以此来提升锁的性能 （）自动续期自动续期的功能是获取锁之后开启一个定时任务，每隔10秒判断一下锁是否存在，如果存在，则刷新过期时间。如果续期3次，也就是30秒之后，业务方法还是没有执行完，就不再续期了。 在实现自动续期功能时，还需要设置一个总的过期时间，可以跟redisson保持一致，设置成30秒。如果业务代码到了这个总的过期时间，还没有执行完，就不再自动续期了。 ##（）Redis分布式锁与ZK比较 redis分布式锁，其实需要自己不断去尝试获取锁，比较消耗性能 zk分布式锁，获取不到锁，注册个监听器即可，不需要不断主动尝试获取锁，性能开销较低 另外一点就是，如果redis获取锁的那个客户端bug了，或者挂了，那么等待超时时间之后才能释放锁，而zk的话，因为创建的是临时节点，只要客户端挂了，znode就没了，此时就会自动释放锁 （）乐观锁和悲观锁乐观锁可以使用CAS和版本号机制来实施 （1）CAS（Compare And Swap）：CAS操作包括了3个操作数： 需要读写的内存位置(V) 进行比较的预期值(A) 拟写入的新值(B) （2）版本号机制 除了CAS，版本号机制也可以用来实现乐观锁。版本号机制的基本思路是在数据中增加一个字段version，表示该数据的版本号，每当数据被修改，版本号加1。当某个线程查询数据时，将该数据的版本号一起查出来；当该线程更新数据时，判断当前版本号与之前读取的版本号是否一致，如果一致才进行操作。 乐观锁和悲观锁比较： 1、功能限制与悲观锁相比，乐观锁适用的场景受到了更多的限制，无论是CAS还是版本号机制。 例如，CAS只能保证单个变量操作的原子性，当涉及到多个变量时，CAS是无能为力的，而synchronized则可以通过对整个代码块加锁来处理。再比如版本号机制，如果query的时候是针对表1，而update的时候是针对表2，也很难通过简单的版本号来实现乐观锁。 2、竞争激烈程度如果悲观锁和乐观锁都可以使用，那么选择就要考虑竞争的激烈程度： 当竞争不激烈 (出现并发冲突的概率小)时，乐观锁更有优势，因为悲观锁会锁住代码块或数据，其他线程无法同时访问，影响并发，而且加锁和释放锁都需要消耗额外的资源。当竞争激烈(出现并发冲突的概率大)时，悲观锁更有优势，因为乐观锁在执行更新时频繁失败，需要不断重试，浪费CPU资源。 乐观锁加锁吗？ （1）乐观锁本身是不加锁的，只是在更新时判断一下数据是否被其他线程更新了；AtomicInteger便是一个例子。 （2）有时乐观锁可能与加锁操作合作，例如，在前述updateCoins()的例子中，MySQL在执行update时会加排它锁。但这只是乐观锁与加锁操作合作的例子，不能改变“乐观锁本身不加锁”这一事实。 4、分布式事务5、分布式一致性算法Raft 参考文档（1）什么是分布式系统，如何学习分布式系统：https://www.cnblogs.com/xybaby/p/7787034.html （1）分布式锁：https://juejin.cn/post/6996915693811662884 （2）乐观锁和悲观锁：https://www.cnblogs.com/kismetv/p/10787228.html","link":"/2023/09/25/distributed_base/"},{"title":"Java基础知识点","text":"Java学习 1 基础1、Java 虚拟机（JVM）是运行 Java 字节码的虚拟机。JVM 并不是只有一种！只要满足 JVM 规范，每个公司、组织或者个人都可以开发自己的专属 JVM。 JDK 是 Java Development Kit 缩写，它是功能齐全的 Java SDK。它拥有 JRE 所拥有的一切，还有编译器（javac）和工具（如 javadoc 和 jdb）。它能够创建和编译程序。 JRE 是 Java 运行时环境。它是运行已编译 Java 程序所需的所有内容的集合，包括 Java 虚拟机（JVM），Java 类库，java 命令和其他的一些基础构件。但是，它不能用于创建新程序。 2、在 Java 中，JVM 可以理解的代码就叫做字节码（即扩展名为 .class 的文件），它不面向任何特定的处理器，只面向虚拟机。 1我们需要格外注意的是 .class-&gt;机器码 这一步。在这一步 JVM 类加载器首先加载字节码文件，然后通过解释器逐行解释执行，这种方式的执行速度会相对比较慢。而且，有些方法和代码块是经常需要被调用的(也就是所谓的热点代码)，所以后面引进了 JIT（just-in-time compilation） 编译器，而 JIT 属于运行时编译。当 JIT 编译器完成第一次编译后，其会将字节码对应的机器码保存下来，下次可以直接使用。而我们知道，机器码的运行效率肯定是高于 Java 解释器的。这也解释了我们为什么经常会说 Java 是编译与解释共存的语言 。 3、静态方法是属于类的，在类加载的时候就会分配内存，可以通过类名直接访问。而非静态成员属于实例对象，只有在对象实例化之后才存在，需要通过类的实例对象去访问。 4、可变参数只能作为函数的最后一个参数，但其前面可以有也可以没有任何其他参数。 5、基本类型和包装类型的区别？ 包装类型不赋值就是 null ，而基本类型有默认值且不是 null。 包装类型可用于泛型，而基本类型不可以。 基本数据类型的局部变量存放在 Java 虚拟机栈中的局部变量表中，基本数据类型的成员变量（未被 static 修饰 ）存放在 Java 虚拟机的堆中。包装类型属于对象类型，我们知道几乎所有对象实例都存在于堆中。 相比于对象类型， 基本数据类型占用的空间非常小。 为什么说是几乎所有对象实例呢？ 这是因为 HotSpot 虚拟机引入了 JIT 优化之后，会对对象进行逃逸分析，如果发现某一个对象并没有逃逸到方法外部，那么就可能通过标量替换来实现栈上分配，而避免堆上分配内存 6、两种浮点数类型的包装类 Float,Double 并没有实现常量池技术。 7、什么是自动拆装箱？ 装箱：将基本类型用它们对应的引用类型包装起来； 拆箱：将包装类型转换为基本数据类型； 8、new 运算符，new 创建对象实例（对象实例在堆内存中），对象引用指向对象实例（对象引用存放在栈内存中）。 9、面向对象三大特征 封装 封装是指把一个对象的状态信息（也就是属性）隐藏在对象内部，不允许外部对象直接访问对象的内部信息。但是可以提供一些可以被外界访问的方法来操作属性。就好像我们看不到挂在墙上的空调的内部的零件信息（也就是属性），但是可以通过遥控器（方法）来控制空调。如果属性不想被外界访问，我们大可不必提供方法给外界访问。但是如果一个类没有提供给外界访问的方法，那么这个类也没有什么意义了。就好像如果没有空调遥控器，那么我们就无法操控空凋制冷，空调本身就没有意义了（当然现在还有很多其他方法 ，这里只是为了举例子）。 继承 不同类型的对象，相互之间经常有一定数量的共同点。例如，小明同学、小红同学、小李同学，都共享学生的特性（班级、学号等）。同时，每一个对象还定义了额外的特性使得他们与众不同。例如小明的数学比较好，小红的性格惹人喜爱；小李的力气比较大。继承是使用已存在的类的定义作为基础建立新类的技术，新类的定义可以增加新的数据或新的功能，也可以用父类的功能，但不能选择性地继承父类。通过使用继承，可以快速地创建新的类，可以提高代码的重用，程序的可维护性，节省大量创建新类的时间 ，提高我们的开发效率。 关于继承如下 3 点请记住： 子类拥有父类对象所有的属性和方法（包括私有属性和私有方法），但是父类中的私有属性和方法子类是无法访问，只是拥有。 子类可以拥有自己属性和方法，即子类可以对父类进行扩展。 子类可以用自己的方式实现父类的方法。（以后介绍）。 多态 多态，顾名思义，表示一个对象具有多种的状态，具体表现为父类的引用指向子类的实例。 多态的特点: 对象类型和引用类型之间具有继承（类）/实现（接口）的关系； 引用类型变量发出的方法调用的到底是哪个类中的方法，必须在程序运行期间才能确定； 多态不能调用“只在子类存在但在父类不存在”的方法； 如果子类重写了父类的方法，真正执行的是子类覆盖的方法，如果子类没有覆盖父类的方法，执行的是父类的方法。 10、接口和抽象类有什么共同点和区别？ 共同点 ： 都不能被实例化。 都可以包含抽象方法。 都可以有默认实现的方法（Java 8 可以用 default 关键在接口中定义默认方法）。 区别 ： 接口主要用于对类的行为进行约束，你实现了某个接口就具有了对应的行为。抽象类主要用于代码复用，强调的是所属关系（比如说我们抽象了一个发送短信的抽象类，）。 一个类只能继承一个类，但是可以实现多个接口。 接口中的成员变量只能是 public static final 类型的，不能被修改且必须有初始值，而抽象类的成员变量默认 default，可在子类中被重新定义，也可被重新赋值。 区别可参考：https://tobebetterjavaer.com/oo/interface.html#_04%E3%80%81%E6%8A%BD%E8%B1%A1%E7%B1%BB%E5%92%8C%E6%8E%A5%E5%8F%A3%E7%9A%84%E5%8C%BA%E5%88%AB 1）语法层面上 抽象类可以提供成员方法的实现细节，而接口中只能存在 public abstract 方法； 抽象类中的成员变量可以是各种类型的，而接口中的成员变量只能是 public static final 类型的； 接口中不能含有静态代码块，而抽象类可以有静态代码块； 一个类只能继承一个抽象类，而一个类却可以实现多个接口。 2）设计层面上 抽象类是对一种事物的抽象，即对类抽象，继承抽象类的子类和抽象类本身是一种 is-a 的关系。而接口是对行为的抽象。抽象类是对整个类整体进行抽象，包括属性、行为，但是接口却是对类局部（行为）进行抽象。 举个简单的例子，飞机和鸟是不同类的事物，但是它们都有一个共性，就是都会飞。那么在设计的时候，可以将飞机设计为一个类 Airplane，将鸟设计为一个类 Bird，但是不能将 飞行 这个特性也设计为类，因此它只是一个行为特性，并不是对一类事物的抽象描述。 此时可以将 飞行 设计为一个接口 Fly，包含方法 fly()，然后 Airplane 和 Bird 分别根据自己的需要实现 Fly 这个接口。然后至于有不同种类的飞机，比如战斗机、民用飞机等直接继承 Airplane 即可，对于鸟也是类似的，不同种类的鸟直接继承 Bird 类即可。从这里可以看出，继承是一个 “是不是”的关系，而 接口 实现则是 “有没有”的关系。如果一个类继承了某个抽象类，则子类必定是抽象类的种类，而接口实现则是有没有、具备不具备的关系，比如鸟是否能飞（或者是否具备飞行这个特点），能飞行则可以实现这个接口，不能飞行就不实现这个接口。 接口是对类的某种行为的一种抽象，接口和类之间并没有很强的关联关系，举个例子来说，所有的类都可以实现 Serializable 接口)，从而具有序列化的功能，但不能说所有的类和 Serializable 之间是 is-a 的关系。 抽象类作为很多子类的父类，它是一种模板式设计。而接口是一种行为规范，它是一种辐射式设计。什么是模板式设计？最简单例子，大家都用过 ppt 里面的模板，如果用模板 A 设计了 ppt B 和 ppt C，ppt B 和 ppt C 公共的部分就是模板 A 了，如果它们的公共部分需要改动，则只需要改动模板 A 就可以了，不需要重新对 ppt B 和 ppt C 进行改动。而辐射式设计，比如某个电梯都装了某种报警器，一旦要更新报警器，就必须全部更新。也就是说对于抽象类，如果需要添加新的方法，可以直接在抽象类中添加具体的实现，子类可以不进行变更；而对于接口则不行，如果接口进行了变更，则所有实现这个接口的类都必须进行相应的改动。 11、== 和 equals 区别 == 是 Java 中一种操作符，它有两种比较方式： 对于 基本数据类型来说， == 判断的是两边的 值是否相等 对于 引用类型来说， == 判断的是两边的 引用是否相等，也就是判断两个对象是否指向了同一块内存区域 equals 是 Java 中所有对象的父类，即 Object 类定义的一个方法。它只能比较对象，它表示的是引用双方的值是否相等。 所以记住，并不是说 == 比较的就是引用是否相等，equals 比较的就是值，这需要区分来说的。 12、在 JDK1.7 及以后会判断运行时常量池中是否有指定的字符串，如果没有的话，就把字符串添加到常量池中，并返回常量池中的对象。 12345678910private void StringOverrideEquals(){ String s1 = &quot;aaa&quot;; String s2 = &quot;aa&quot; + new String(&quot;a&quot;); String s3 = new String(&quot;aaa&quot;); System.out.println(s1.intern().equals(s1)); //true，因为 s1 字符串创建出来就已经在常量池中存在了 System.out.println(s1.intern().equals(s2)); //false，因为 s1 返回的是常量池中的对象，而 s2 返回的是堆中的对象 System.out.println(s3.intern().equals(s1)); //true ，因为 s3 对象虽然在堆中创建了一个对象，但是 s3 中的 &quot;aaa&quot; 返回的是常量池中的对象} 13、基本类型和包装类型的区别 用途：除了定义一些常量和局部变量之外，我们在其他地方比如方法参数、对象属性中很少会使用基本类型来定义变量。并且，包装类型可用于泛型，而基本类型不可以。 存储方式：基本数据类型的局部变量存放在 Java 虚拟机栈中的局部变量表中，基本数据类型的成员变量（未被 static 修饰 ）存放在 Java 虚拟机的堆中。包装类型属于对象类型，我们知道几乎所有对象实例都存在于堆中。 占用空间：相比于包装类型（对象类型）， 基本数据类型占用的空间往往非常小。 默认值：成员变量包装类型不赋值就是 null ，而基本类型有默认值且不是 null。 比较方式：对于基本数据类型来说，== 比较的是值。对于包装数据类型来说，== 比较的是对象的内存地址。所有整型包装类对象之间值的比较，全部使用 equals() 方法。 为什么说是几乎所有对象实例都存在于堆中呢？ 这是因为 HotSpot 虚拟机引入了 JIT 优化之后，会对对象进行逃逸分析，如果发现某一个对象并没有逃逸到方法外部，那么就可能通过标量替换来实现栈上分配，而避免堆上分配内存 ⚠️ 注意：基本数据类型存放在栈中是一个常见的误区！ 基本数据类型的成员变量如果没有被 static 修饰的话（不建议这么使用，应该要使用基本数据类型对应的包装类型），就存放在堆中。 123class BasicTypeVar{ private int x;} 14、包装类型的缓存机制 Java 基本数据类型的包装类型的大部分都用到了缓存机制来提升性能。 Byte,Short,Integer,Long 这 4 种包装类默认创建了数值 [-128，127] 的相应类型的缓存数据，Character 创建了数值在 [0,127] 范围的缓存数据，Boolean 直接返回 True or False。 15、String、StringBuilder、StringBuffer 每次对 String 类型进行改变的时候，都会生成一个新的 String 对象，然后将指针指向新的 String 对象。StringBuffer 每次都会对 StringBuffer 对象本身进行操作，而不是生成新的对象并改变对象引用。相同情况下使用 StringBuilder 相比使用 StringBuffer 仅能获得 10%~15% 左右的性能提升，但却要冒多线程不安全的风险。 对于三者使用的总结： 操作少量的数据: 适用 String 单线程操作字符串缓冲区下操作大量数据: 适用 StringBuilder 多线程操作字符串缓冲区下操作大量数据: 适用 StringBuffer 16、 transient 关键字 transient 关键字的作用是：阻止实例中那些用此关键字修饰的的变量序列化；当对象被反序列化时，被 transient 修饰的变量值不会被持久化和恢复。 关于 transient 还有几点注意： transient 只能修饰变量，不能修饰类和方法。 transient 修饰的变量，在反序列化后变量值将会被置成类型的默认值。例如，如果是修饰 int 类型，那么反序列后结果就是 0。 static 变量因为不属于任何对象(Object)，所以无论有没有 transient 关键字修饰，均不会被序列化。 17、final保证可见性的前提是未发生this引用逃逸，参考：https://zhuanlan.zhihu.com/p/477481115 18、java命令的参数，传入的是main函数所在的类的名字，而不是class文件；java会根据类名自动去找class文件。 增加了package名，所以class名也变了，行不改名坐不改姓，自然要带上姓（即所谓全限定名）。 Java 会根据包名对应出目录结构，并从class path搜索该目录去找class文件。由于默认的class path是当前目录， 2. 动态代理2.1 JDK 动态代理在 Java 动态代理机制中 InvocationHandler 接口和 Proxy 类是核心。 Proxy 类中使用频率最高的方法是：newProxyInstance() ，这个方法主要用来生成一个代理对象。 1234567public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) throws IllegalArgumentException{ ......} 这个方法一共有 3 个参数： loader :类加载器，用于加载代理对象。 interfaces : 被代理类实现的一些接口； h : 实现了 InvocationHandler 接口的对象； 要实现动态代理的话，还必须需要实现 InvocationHandler 来自定义处理逻辑。 当我们的动态代理对象调用一个方法时，这个方法的调用就会被转发到实现 InvocationHandler 接口类的 invoke 方法来调用。 12345678public interface InvocationHandler { /** * 当你使用代理对象调用方法的时候实际会调用到这个方法 */ public Object invoke(Object proxy, Method method, Object[] args) throws Throwable;} invoke() 方法有下面三个参数： proxy :动态生成的代理类 method : 与代理类对象调用的方法相对应 args : 当前 method 方法的参数 也就是说：你通过 Proxy 类的 newProxyInstance() 创建的代理对象在调用方法的时候，实际会调用到实现 InvocationHandler 接口的类的 invoke()方法。 你可以在 invoke() 方法中自定义处理逻辑，比如在方法执行前后做什么事情。 注：Mybatis中只用声明接口，不需要手动指定实现类，也是通过JDK Proxy来实现的。 2.2 CGLIB 动态代理JDK 动态代理有一个最致命的问题是其只能代理实现了接口的类。 为了解决这个问题，我们可以用 CGLIB 动态代理机制来避免。 CGLIBopen in new window( Code Generation Library )是一个基于ASMopen in new window的字节码生成库（ASM API基于访问者模式，为我们提供了ClassVisitor，MethodVisitor，FieldVisitor API接口，每当ASM扫描到类字段是会回调visitField方法，扫描到类方法是会回调MethodVisitor)，它允许我们在运行时对字节码进行修改和动态生成。CGLIB 通过继承方式实现代理。很多知名的开源框架都使用到了CGLIBopen in new window， 例如 Spring 中的 AOP 模块中：如果目标对象实现了接口，则默认采用 JDK 动态代理，否则采用 CGLIB 动态代理。 在 CGLIB 动态代理机制中 MethodInterceptor 接口和 Enhancer 类是核心。 你需要自定义 MethodInterceptor 并重写 intercept 方法，intercept 用于拦截增强被代理类的方法。 123456public interface MethodInterceptorextends Callback{ // 拦截被代理类中的方法 public Object intercept(Object obj, java.lang.reflect.Method method, Object[] args,MethodProxy proxy) throws Throwable;} obj : 被代理的对象（需要增强的对象） method : 被拦截的方法（需要增强的方法） args : 方法入参 proxy : 用于调用原始方法 你可以通过 Enhancer类来动态获取被代理类，当代理类调用方法的时候，实际调用的是 MethodInterceptor 中的 intercept 方法。 CGLIB 动态代理类使用步骤如下： 定义一个类； 自定义 MethodInterceptor 并重写 intercept 方法，intercept 用于拦截增强被代理类的方法，和 JDK 动态代理中的 invoke 方法类似； 通过 Enhancer 类的 create()创建代理类； 2.3 JDK 动态代理和 CGLIB 动态代理对比 JDK 动态代理只能代理实现了接口的类或者直接代理接口，而 CGLIB 可以代理未实现任何接口的类。因为Java中不支持多继承，而JDK的动态代理在创建代理对象时，默认让代理对象继承了Proxy类，所以JDK只能通过接口去实现动态代理。基于接口代理，凡是类的方法非public修饰，或者用了static关键字修饰，那这些方法都不能被Spring AOP增强。另外， CGLIB 动态代理是通过生成一个被代理类的子类来拦截被代理类的方法调用，因此不能代理声明为 final 类型的类和方法。基于子类代理，凡是类的方法使用了private、static、final修饰，那这些方法都不能被Spring AOP增强 由于是继承关系，无法代理final的类和方法(无法继承)，或是private的方法(对子类不可见)。 就二者的效率来说，大部分情况都是 JDK 动态代理更优秀，随着 JDK 版本的升级，这个优势更加明显。 参考资料（1）https://javaguide.cn/java/basis/java-basic-questions-01.html","link":"/2023/09/25/java_base/"},{"title":"Java并发","text":"1.线程基础（1）线程状态图 注意：在操作系统层面，线程有 READY 和 RUNNING 状态；而在 JVM 层面，只能看到 RUNNABLE 状态，因为目前时间分片调度下，线程切换非常快，没有必要区分这两种状态。 死锁的发生必须具备以下四个必要条件： 互斥条件 请求和保持条件 不剥夺条件 环路等待条件 （2）Thread 类的 run、start区别 new 一个 Thread，线程进入了新建状态。调用 start()方法，会启动一个线程并使线程进入了就绪状态，当分配到时间片后就可以开始运行了。 start() 会执行线程的相应准备工作，然后自动执行 run() 方法的内容，这是真正的多线程工作。 但是，直接执行 run() 方法，会把 run() 方法当成一个 main 线程下的普通方法去执行，并不会在某个线程中执行它，所以这并不是多线程工作。 总结：调用 start() 方法方可启动线程并使线程进入就绪状态，直接执行 run() 方法的话不会以多线程的方式执行。 （3）sleep() 方法和 wait() 方法对比 共同点：两者都可以暂停线程的执行。 区别： sleep() 方法没有释放锁，而 wait() 方法释放了锁 。 wait() 通常被用于线程间交互/通信，sleep()通常被用于暂停执行。 wait() 方法被调用后，线程不会自动苏醒，需要别的线程调用同一个对象上的 notify()或者 notifyAll() 方法。sleep()方法执行完成后，线程会自动苏醒，或者也可以使用 wait(long timeout) 超时后线程会自动苏醒。 sleep() 是 Thread 类的静态本地方法，wait() 则是 Object 类的本地方法。为什么这样设计呢？wait() 是让获得对象锁的线程实现等待，会自动释放当前线程占有的对象锁。每个对象（Object）都拥有对象锁，既然要释放当前线程占有的对象锁并让其进入 WAITING 状态，自然是要操作对应的对象（Object）而非当前的线程（Thread）。类似的问题：为什么 sleep() 方法定义在 Thread 中？因为 sleep() 是让当前线程暂停执行，不涉及到对象类，也不需要获得对象锁。 2. JMM（Java 内存模型）（1）基本介绍 对于 Java 来说，你可以把 JMM 看作是 Java 定义的并发编程相关的一组规范，除了抽象了线程和主内存之间的关系之外，其还规定了从 Java 源代码到 CPU 可执行指令的这个转化过程要遵守哪些和并发相关的原则和规范，其主要目的是为了简化多线程编程，增强程序可移植性的。 JMM 说白了就是定义了一些规范来解决这些问题，开发者可以利用这些规范更方便地开发多线程程序。对于 Java 开发者说，你不需要了解底层原理，直接使用并发相关的一些关键字和类（比如 volatile、synchronized、各种 Lock）即可开发出并发安全的程序。 （2） happens-before 原则 JSR-133 对 happens-before 原则的定义： 如果一个操作 happens-before 另一个操作，那么第一个操作的执行结果将对第二个操作可见，并且第一个操作的执行顺序排在第二个操作之前。 两个操作之间存在 happens-before 关系，并不意味着 Java 平台的具体实现必须要按照 happens-before 关系指定的顺序来执行。如果重排序之后的执行结果，与按 happens-before 关系来执行的结果一致，那么 JMM 也允许这样的重排序。 happens-before 原则表达的意义其实并不是一个操作发生在另外一个操作的前面，虽然这从程序员的角度上来说也并无大碍。更准确地来说，它更想表达的意义是前一个操作的结果对于后一个操作是可见的，无论这两个操作是否在同一个线程里。 happens-before 的规则就 8 条，说多不多，重点了解下面列举的 5 条即可。 程序顺序规则 ：一个线程内，按照代码顺序，书写在前面的操作 happens-before 于书写在后面的操作； 解锁规则 ：解锁 happens-before 于加锁； volatile 变量规则 ：对一个 volatile 变量的写操作 happens-before 于后面对这个 volatile 变量的读操作。说白了就是对 volatile 变量的写操作的结果对于发生于其后的任何操作都是可见的。 传递规则 ：如果 A happens-before B，且 B happens-before C，那么 A happens-before C； 线程启动规则 ：Thread 对象的 start()方法 happens-before 于此线程的每一个动作。 （3）再看并发编程三个重要特性 原子性：是指在一个操作中就是cpu不可以在中途暂停然后再调度，既不被中断操作，要不执行完成，要不就不执行。synchronized 和各种 Lock 可以保证任一时刻只有一个线程访问该代码块，因此可以保障原子性。各种原子类是利用 CAS (compare and swap) 操作（可能也会用到 volatile或者final关键字）来保证原子操作。 可见性：是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。在 Java 中，可以借助synchronized、volatile 以及各种 Lock 实现可见性。如果我们将变量声明为 volatile ，这就指示 JVM，这个变量是共享且不稳定的，每次使用它都到主存中进行读取。 有序性：即程序执行的顺序按照代码的先后顺序执行。在 Java 中，volatile 关键字可以禁止指令进行重排序优化。 3. Java锁（1）公平锁和非公平锁 公平锁 : 锁被释放之后，先申请的线程先得到锁。性能较差一些，因为公平锁为了保证时间上的绝对顺序，上下文切换更频繁。 非公平锁 ：锁被释放之后，后申请的线程可能会先获取到锁，是随机或者按照其他优先级排序的。性能更好，但可能会导致某些线程永远无法获取到锁。 （2）可中断锁和不可中断锁 可中断锁 ：获取锁的过程中可以被中断，不需要一直等到获取锁之后 才能进行其他逻辑处理。ReentrantLock 就属于是可中断锁。 不可中断锁 ：一旦线程申请了锁，就只能等到拿到锁以后才能进行其他的逻辑处理。 synchronized 就属于是不可中断锁。 （3）共享锁和独占锁 共享锁 ：一把锁可以被多个线程同时获得。 独占锁 ：一把锁只能被一个线程获得。 （4）读锁与写锁 一般锁进行并发控制的规则：读读互斥、读写互斥、写写互斥。 读写锁进行并发控制的规则：读读不互斥、读写互斥、写写互斥（只有读读不互斥）。 线程持有读锁还能获取写锁吗？ 在线程持有读锁的情况下，该线程不能取得写锁(因为获取写锁的时候，如果发现当前的读锁被占用，就马上获取失败，不管读锁是不是被当前线程持有)。 在线程持有写锁的情况下，该线程可以继续获取读锁（获取读锁时如果发现写锁被占用，只有写锁没有被当前线程占用的情况才会获取失败）。 读锁为什么不能升级为写锁？ 写锁可以降级为读锁，但是读锁却不能升级为写锁。这是因为读锁升级为写锁会引起线程的争夺，毕竟写锁属于是独占锁，这样的话，会影响性能。 另外，还可能会有死锁问题发生。举个例子：假设两个线程的读锁都想升级写锁，则需要对方都释放自己锁，而双方都不释放，就会产生死锁。 3.1 乐观锁和悲观锁（1） 悲观锁 悲观锁总是假设最坏的情况，认为共享资源每次被访问的时候就会出现问题(比如共享数据被修改)，所以每次在获取资源操作的时候都会上锁，这样其他线程想拿到这个资源就会阻塞直到锁被上一个持有者释放。也就是说，共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程。 像 Java 中 synchronized和 ReentrantLock等独占锁就是悲观锁思想的实现。 （2）乐观锁 乐观锁总是假设最好的情况，认为共享资源每次被访问的时候不会出现问题，线程可以不停地执行，无需加锁也无需等待，只是在提交修改的时候去验证对应的资源（也就是数据）是否被其它线程修改了（具体方法可以使用版本号机制或 CAS 算法）。 在 Java 中 java.util.concurrent.atomic包下面的原子变量类（比如 AtomicInteger、LongAdder）就是使用了乐观锁的一种实现方式 CAS 实现的。 CAS 是一个原子操作，底层依赖于一条 CPU 的原子指令。 原子操作 即最小不可拆分的操作，也就是说操作一旦开始，就不能被打断，直到操作完成。 CAS 涉及到三个操作数： V ：要更新的变量值(Var) E ：预期值(Expected) N ：拟写入的新值(New) 乐观锁存在问题： ABA问题：如果一个变量 V 初次读取的时候是 A 值，并且在准备赋值的时候检查到它仍然是 A 值，那我们就能说明它的值没有被其他线程修改过了吗？很明显是不能的，因为在这段时间它的值可能被改为其他值，然后又改回 A，那 CAS 操作就会误认为它从来没有被修改过。这个问题被称为 CAS 操作的 “ABA”问题。ABA 问题的解决思路是在变量前面追加上 版本号或者时间戳 。 循环时间长开销大：CAS 经常会用到自旋操作来进行重试，也就是不成功就一直循环执行直到成功。如果长时间不成功，会给 CPU 带来非常大的执行开销。 （3）对比 理论上来说： 悲观锁通常多用于写比较多的情况（多写场景，竞争激烈），这样可以避免频繁失败和重试影响性能，悲观锁的开销是固定的。不过，如果乐观锁解决了频繁失败和重试这个问题的话（比如 LongAdder），也是可以考虑使用乐观锁的，要视实际情况而定。 乐观锁通常多用于写比较少的情况（多读场景，竞争较少），这样可以避免频繁加锁影响性能。不过，乐观锁主要针对的对象是单个共享变量（参考 java.util.concurrent.atomic包下面的原子变量类）。 3.2 volatile 可见性：被volatile修饰的变量在被修改后可以立即同步到主内存，被其修饰的变量在每次是用之前都从主内存刷新 有序性：volatile可以禁止指令重排，这就保证了代码的程序会严格按照代码的先后顺序执行 注意：volatile不能保证原子性。 3.3 synchronized（1）使用 synchronized是Java提供的一个并发控制的关键字。主要有两种用法，分别是同步方法和同步代码块。也就是说，synchronized既可以修饰方法也可以修饰代码块。被 synchronized修饰的代码块及方法，在同一时间，只能被单个线程访问。 原子性：线程1在执行 monitorenter指令的时候，会对对象监视器 Monitor进行加锁，加锁后其他线程无法获得锁，除非线程1主动解锁。即使在执行过程中，由于某种原因，比如CPU时间片用完，线程1放弃了CPU，但是，他并没有进行解锁。而由于 synchronized的锁是可重入的，下一个时间片还是只能被他自己获取到，还是会继续执行代码。直到所有代码执行完。这就保证了原子性 可见性：被 synchronized修饰的代码，在开始执行时会加锁，执行完成后会进行解锁。而为了保证可见性，有一条规则是这样的：对一个变量解锁之前，必须先把此变量同步回主存中。这样解锁后，后续线程就可以访问到被修改后的值。所以，synchronized关键字锁住的对象，其值是具有可见性的 有序性：synchronized是无法禁止指令重排和处理器优化的，但由于 as-if-serial语义的限制，被 synchronized修饰的代码，同一时间只能被同一线程访问。那么也就是单线程执行的。所以，可以保证其有序性 as-if-serial语义：保证了单线程中，指令重排是有一定的限制的，而只要编译器和处理器都遵守了这个语义，那么就可以认为单线程程序是按照顺序执行的。当然，实际上还是有重排的，只不过我们无须关心这种重排的干扰 总结： synchronized 关键字加到 static 静态方法和 synchronized(class) 代码块上都是是给 Class 类上锁； synchronized 关键字加到实例方法上是给对象实例上锁； 尽量不要使用 synchronized(String a) 因为 JVM 中，字符串常量池具有缓存功能。 构造方法不能使用 synchronized 关键字修饰。构造方法本身就属于线程安全的，不存在同步的构造方法一说。 （2）实现原理 对于同步方法，JVM采用 ACC_SYNCHRONIZED标记符来实现同步。 对于同步代码块。JVM采用 monitorenter、monitorexit两个指令来实现同步。 Method-level synchronization is performed implicitly, as part of method invocation and return. A synchronized method is distinguished in the run-time constant pool’s method_info structure by the ACC_SYNCHRONIZED flag, which is checked by the method invocation instructions. When invoking a method for which ACC_SYNCHRONIZED is set, the executing thread enters a monitor, invokes the method itself, and exits the monitor whether the method invocation completes normally or abruptly. During the time the executing thread owns the monitor, no other thread may enter it. If an exception is thrown during invocation of the synchronized method and the synchronized method does not handle the exception, the monitor for the method is automatically exited before the exception is rethrown out of the synchronized method. 主要说的是： 方法级的同步是隐式的。同步方法的常量池中会有一个 ACC_SYNCHRONIZED标志。当某个线程要访问某个方法的时候，会检查是否有 ACC_SYNCHRONIZED，如果有设置，则需要先获得监视器锁，然后开始执行方法，方法执行之后再释放监视器锁。这时如果其他线程来请求执行方法，会因为无法获得监视器锁而被阻断住。值得注意的是，如果在方法执行过程中，发生了异常，并且方法内部并没有处理该异常，那么在异常被抛到方法外面之前监视器锁会被自动释放。 3.4 ReentrantLockReentrantLock 比 synchronized 增加了一些高级功能 相比 synchronized，ReentrantLock增加了一些高级功能。主要来说主要有三点： 等待可中断 : ReentrantLock提供了一种能够中断等待锁的线程的机制，通过 lock.lockInterruptibly() 来实现这个机制。也就是说正在等待的线程可以选择放弃等待，改为处理其他事情。 可实现公平锁 : ReentrantLock可以指定是公平锁还是非公平锁。而 synchronized只能是非公平锁。所谓的公平锁就是先等待的线程先获得锁。ReentrantLock默认情况是非公平的，可以通过 ReentrantLock类的 ReentrantLock(boolean fair)构造方法来指定是否是公平的。 可实现选择性通知（锁可以绑定多个条件） : synchronized关键字与 wait()和 notify()/notifyAll()方法相结合可以实现等待/通知机制。ReentrantLock类当然也可以实现，但是需要借助于 Condition接口与 newCondition()方法。 3.5 Future （1）Future FutureTask 提供了 Future 接口的基本实现，常用来封装 Callable 和 Runnable，具有取消任务、查看任务是否执行完成以及获取任务执行结果的方法。ExecutorService.submit() 方法返回的其实就是 Future 的实现类 FutureTask 。 （2）CompletableFuture Future 在实际使用过程中存在一些局限性比如不支持异步任务的编排组合、获取计算结果的 get() 方法为阻塞调用。 CompletableFuture 除了提供了更为好用和强大的 Future 特性之外，还提供了函数式编程、异步任务编排组合（可以将多个异步任务串联起来，组成一个完整的链式调用）等能力。 3.6 AQS（1）原理 AQS 核心思想是，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制 AQS 是用 CLH 队列锁 实现的，即将暂时获取不到锁的线程加入到队列中。 CLH(Craig,Landin,and Hagersten) 队列是一个虚拟的双向队列（虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系）。AQS 是将每条请求共享资源的线程封装成一个 CLH 锁队列的一个结点（Node）来实现锁的分配。在 CLH 同步队列中，一个节点表示一个线程，它保存着线程的引用（thread）、 当前节点在队列中的状态（waitStatus）、前驱节点（prev）、后继节点（next）。 AQS 定义两种资源共享方式：Exclusive（独占，只有一个线程能执行，如ReentrantLock）和Share（共享，多个线程可同时执行，如Semaphore/CountDownLatch）。 AQS 使用 int 成员变量 state 表示同步状态 ，以 ReentrantLock 为例，state 初始值为 0，表示未锁定状态。A 线程 lock() 时，会调用 tryAcquire() 独占该锁并将 state+1 。此后，其他线程再 tryAcquire() 时就会失败，直到 A 线程 unlock() 到 state=0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放锁之前，A 线程自己是可以重复获取此锁的（state 会累加），这就是可重入的概念。但要注意，获取多少次就要释放多少次，这样才能保证 state 是能回到零态的。 再以 CountDownLatch 以例，任务分为 N 个子线程去执行，state 也初始化为 N（注意 N 要与线程个数一致）。这 N 个子线程是并行执行的，每个子线程执行完后 countDown() 一次，state 会 CAS(Compare and Swap) 减 1。等到所有子线程都执行完后(即 state=0 )，会 unpark() 主调用线程，然后主调用线程就会从 await() 函数返回，继续后余动作。 3.7 锁优化（1）适应性自旋锁 自旋锁和阻塞锁最大的区别就是，到底要不要放弃处理器的执行时间。对于阻塞锁和自旋锁来说，都是要等待获得共享资源。但是阻塞锁是放弃了CPU时间，进入了等待区，等待被唤醒。而自旋锁是一直“自旋”在那里，时刻的检查共享资源是否可以被访问。 由于自旋锁只是将当前线程不停地执行循环体，不进行线程状态的改变，所以响应速度更快。但当线程数不停增加时，性能下降明显，因为每个线程都需要执行，占用CPU时间。如果线程竞争不激烈，并且保持锁的时间段。适合使用自旋锁 （2）锁消除 在动态编译同步块的时候，JIT编译器可以借助一种被称为逃逸分析（Escape Analysis）的技术来判断同步块所使用的锁对象是否只能够被一个线程访问而没有被发布到其他线程。 如果同步块所使用的锁对象通过这种分析被证实只能够被一个线程访问，那么JIT编译器在编译这个同步块的时候就会取消对这部分代码的同步。 （3）锁粗化 如果在一段代码中连续的对同一个对象反复加锁解锁，其实是相对耗费资源的，这种情况可以适当放宽加锁的范围，减少性能消耗。 当JIT发现一系列连续的操作都对同一个对象反复加锁和解锁，甚至加锁操作出现在循环体中的时候，会将加锁同步的范围扩散（粗化）到整个操作序列的外部。 （4）偏向锁、轻量锁、重量锁 背景：在自旋锁中，阻塞或唤醒一个Java线程需要操作系统切换CPU状态来完成，这种状态转换需要耗费处理器时间。如果同步代码块中的内容过于简单，状态转换消耗的时间有可能比用户代码执行的时间还要长。这种方式就是synchronized最初实现同步的方式，这就是JDK 6之前synchronized效率低的原因。这种依赖于操作系统Mutex Lock所实现的锁我们称之为“重量级锁”，JDK 6中为了减少获得锁和释放锁带来的性能消耗，引入了“偏向锁”和“轻量级锁”。 目前锁一共有4种状态，级别从低到高依次是：无锁、偏向锁、轻量级锁和重量级锁。锁状态只能升级不能降级。 无锁：无锁没有对资源进行锁定，所有的线程都能访问并修改同一个资源，但同时只有一个线程能修改成功。无锁的特点就是修改操作在循环内进行，线程会不断的尝试修改共享资源。如果没有冲突就修改成功并退出，否则就会继续循环尝试。如果有多个线程修改同一个值，必定会有一个线程能修改成功，而其他修改失败的线程会不断重试直到修改成功。CAS原理及应用即是无锁的实现。无锁无法全面代替有锁，但无锁在某些场合下的性能是非常高的。 偏向锁：偏向锁是指一段同步代码一直被一个线程所访问，那么该线程会自动获取锁，降低获取锁的代价。在大多数情况下，锁总是由同一线程多次获得，不存在多线程竞争，所以出现了偏向锁。其目标就是在只有一个线程执行同步代码块时能够提高性能。当一个线程访问同步代码块并获取锁时，会在Mark Word里存储锁偏向的线程ID。在线程进入和退出同步块时不再通过CAS操作来加锁和解锁，而是检测Mark Word里是否存储着指向当前线程的偏向锁。引入偏向锁是为了在无多线程竞争的情况下尽量减少不必要的轻量级锁执行路径，因为轻量级锁的获取及释放依赖多次CAS原子指令，而偏向锁只需要在置换ThreadID的时候依赖一次CAS原子指令即可。偏向锁只有遇到其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁，线程不会主动释放偏向锁。偏向锁的撤销，需要等待全局安全点（在这个时间点上没有字节码正在执行），它会首先暂停拥有偏向锁的线程，判断锁对象是否处于被锁定状态。撤销偏向锁后恢复到无锁（标志位为“01”）或轻量级锁（标志位为“00”）的状态。 轻量级锁：是指当锁是偏向锁的时候，被另外的线程所访问，偏向锁就会升级为轻量级锁，其他线程会通过自旋的形式尝试获取锁，不会阻塞，从而提高性能。在代码进入同步块的时候，如果同步对象锁状态为无锁状态（锁标志位为“01”状态，是否为偏向锁为“0”），虚拟机首先将在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间，用于存储锁对象目前的Mark Word的拷贝，然后拷贝对象头中的Mark Word复制到锁记录中。拷贝成功后，虚拟机将使用CAS操作尝试将对象的Mark Word更新为指向Lock Record的指针，并将Lock Record里的owner指针指向对象的Mark Word。如果这个更新动作成功了，那么这个线程就拥有了该对象的锁，并且对象Mark Word的锁标志位设置为“00”，表示此对象处于轻量级锁定状态。如果轻量级锁的更新操作失败了，虚拟机首先会检查对象的Mark Word是否指向当前线程的栈帧，如果是就说明当前线程已经拥有了这个对象的锁，那就可以直接进入同步块继续执行，否则说明多个线程竞争锁。若当前只有一个等待线程，则该线程通过自旋进行等待。但是当自旋超过一定的次数，或者一个线程在持有锁，一个在自旋，又有第三个来访时，轻量级锁升级为重量级锁。 重量级锁：升级为重量级锁时，此时Mark Word中存储的是指向重量级锁的指针，此时等待锁的线程都会进入阻塞状态。 总结：偏向锁通过对比Mark Word解决加锁问题，避免执行CAS操作。而轻量级锁是通过用CAS操作和自旋来解决加锁问题，避免线程阻塞和唤醒而影响性能。重量级锁是将除了拥有锁的线程以外的线程都阻塞。 4. 多线程参考资料（1）https://javaguide.cn/java/concurrent/java-concurrent-questions-01.html （2）JMM（Java 内存模型）详解：https://javaguide.cn/java/concurrent/jmm.html （3）不可不说的Java“锁”事：https://tech.meituan.com/2018/11/15/java-lock.html TODO（1）构造方法为什么是线程安全的？ （2）CompletableFuture深入了解？ （3）完成多线程部分","link":"/2023/09/25/java_concurrent/"},{"title":"ThreadLocal","text":"1. 作用ThreadLocal类主要解决的就是让每个线程绑定自己的值，每一个线程都有自己的专属本地变量，避免线程安全问题。 注意：阿里巴巴Java开发手册里规定： 【参考】ThreadLocal无法解决共享对象的更新问题，ThreadLocal对象建议使用static修饰。这个变量是针对一个线程内所有操作共享的，所以设置为静态变量，所有此类实例共享此静态变量 ，也就是说在类第一次被使用时装载，只分配一块存储空间，所有此类的对象(只要是这个线程内定义的)都可以操控这个变量。 2. 原理Thread 类中有一个 threadLocals 和 一个 inheritableThreadLocals 变量，它们都是 ThreadLocalMap 类型的变量，我们可以把 ThreadLocalMap 理解为 ThreadLocal 类实现的定制化的 HashMap。默认情况下这两个变量都是 null，只有当前线程调用 ThreadLocal 类的 set或 get方法时才创建它们，实际上调用这两个方法的时候，我们调用的是 ThreadLocalMap类对应的 get()、set()方法。 123456789public class Thread implements Runnable { //...... //与此线程有关的ThreadLocal值。由ThreadLocal类维护 ThreadLocal.ThreadLocalMap threadLocals = null; //与此线程有关的InheritableThreadLocal值。由InheritableThreadLocal类维护 ThreadLocal.ThreadLocalMap inheritableThreadLocals = null; //......} ThreadLocal 可以理解为只是 ThreadLocalMap的封装，传递了变量值。每个 Thread中都具备一个 ThreadLocalMap，而 ThreadLocalMap可以存储以 ThreadLocal为 key ，Object 对象为 value 的键值对。 注意：ThreadLocalMap有点类似 HashMap的结构，只是 HashMap是由数组+链表实现的，而 ThreadLocalMap中并没有链表结构。 3. ThreadLocalMap3.1 Hash 算法既然是 Map结构，那么 ThreadLocalMap当然也要实现自己的 hash算法来解决散列表数组冲突问题。 1int i = key.threadLocalHashCode &amp; (len-1); 每当创建一个 ThreadLocal对象，这个 ThreadLocal.nextHashCode 这个值就会增长 0x61c88647 。 这个值很特殊，它是斐波那契数 也叫 黄金分割数 。hash增量为 这个数字，带来的好处就是 hash 分布非常均匀 。 3.2 Hash 冲突整体是线性探测再散列的方法，即对应数组下标无数据则插入，有数据则向后查找，直至找到无数据或key为null的下标。 注：什么情况下桶才是可以使用的呢？ k = key 说明是替换操作，可以使用 碰到一个过期的桶，执行替换逻辑，占用过期桶 查找过程中，碰到桶中 Entry=null的情况，直接使用 3.3 Set 源码解析12345678910111213141516171819202122232425262728293031323334353637383940414243/** * Set the value associated with key. * * @param key the thread local object * @param value the value to be set */private void set(ThreadLocal&lt;?&gt; key, Object value) { // We don't use a fast path as with get() because it is at // least as common to use set() to create new entries as // it is to replace existing ones, in which case, a fast // path would fail more often than not. // 通过key来计算在散列表中的对应位置 Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) { ThreadLocal&lt;?&gt; k = e.get(); // k = key，说明当前set操作是一个替换操作，做替换逻辑，直接返回 if (k == key) { e.value = value; return; } // key = null，说明当前桶位置的Entry是过期数据，需要清理数据 if (k == null) { replaceStaleEntry(key, value, i); return; } } // 在Entry为null的桶中创建一个新的Entry对象 tab[i] = new Entry(key, value); int sz = ++size; // 清理有限范围内的失效条目，如果找到了失效条目肯定会清理掉同时size--，这时一定不需要扩容。 // 如果没有清理任何失效条目，则size是可能达到阈值(数组长度的 2/3)的，达到阈值则扩容。 if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) rehash();} 接着重点看下 replaceStaleEntry()方法，replaceStaleEntry()方法提供替换过期数据的功能。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798/** * 环形数组下标计算(下一个数组元素下标) */private static int nextIndex(int i, int len) { return ((i + 1 &lt; len) ? i + 1 : 0);}/** * 环形数组下标计算(上一个数组元素下标) */private static int prevIndex(int i, int len) { return ((i - 1 &gt;= 0) ? i - 1 : len - 1);}/** * Replace a stale entry encountered during a set operation * with an entry for the specified key. The value passed in * the value parameter is stored in the entry, whether or not * an entry already exists for the specified key. * * As a side effect, this method expunges all stale entries in the * &quot;run&quot; containing the stale entry. (A run is a sequence of entries * between two null slots.) * * @param key the key * @param value the value to be associated with key * @param staleSlot index of the first stale entry encountered while * searching for key. */private void replaceStaleEntry(ThreadLocal&lt;?&gt; key, Object value, int staleSlot) { Entry[] tab = table; int len = tab.length; Entry e; // Back up to check for prior stale entry in current run. // We clean out whole runs at a time to avoid continual // incremental rehashing due to garbage collector freeing // up refs in bunches (i.e., whenever the collector runs). // slotToExpunge表示开始探测式清理过期数据的开始下标，默认从当前的staleSlot开始。 // 以当前的staleSlot开始，向前迭代查找，找到没有过期的数据，for循环一直碰到Entry为null才会结束。 // 如果向前找到了过期数据，更新探测清理过期数据的开始下标为 i，即slotToExpunge=i int slotToExpunge = staleSlot; for (int i = prevIndex(staleSlot, len); (e = tab[i]) != null; i = prevIndex(i, len)) if (e.get() == null) slotToExpunge = i; // Find either the key or trailing null slot of run, whichever // occurs first // 接着开始从staleSlot向后查找，也是碰到Entry为null的桶结束。 for (int i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) { ThreadLocal&lt;?&gt; k = e.get(); // If we find key, then we need to swap it // with the stale entry to maintain hash table order. // The newly stale slot, or any other stale slot // encountered above it, can then be sent to expungeStaleEntry // to remove or rehash all of the other entries in run. // 如果迭代过程中，碰到 k == key，这说明这里是替换逻辑，替换新数据并且交换当前staleSlot位置 if (k == key) { e.value = value; tab[i] = tab[staleSlot]; tab[staleSlot] = e; // Start expunge at preceding stale entry if it exists // 如果slotToExpunge == staleSlot，这说明replaceStaleEntry()一开始向前查找过期数据时并未找到过期的Entry数据， // 接着向后查找过程中也未发现过期数据，修改开始探测式清理过期数据的下标为当前循环的 index，即slotToExpunge = i if (slotToExpunge == staleSlot) slotToExpunge = i; // 调用cleanSomeSlots(expungeStaleEntry(slotToExpunge), len);进行过期数据清理 cleanSomeSlots(expungeStaleEntry(slotToExpunge), len); return; } // If we didn't find stale entry on backward scan, the // first stale entry seen while scanning for key is the // first still present in the run. // 如果 k != key则会接着往下走，k == null说明当前遍历的Entry是一个过期数据， // slotToExpunge == staleSlot说明，一开始的向前查找数据并未找到过期的Entry。 // 如果条件成立，则更新slotToExpunge 为当前位置，这个前提是前驱节点扫描时未发现过期数据。 if (k == null &amp;&amp; slotToExpunge == staleSlot) slotToExpunge = i; } // If key not found, put new entry in stale slot // 往后迭代的过程中如果没有找到k == key的数据，且碰到Entry为null的数据，则结束当前的迭代操作。 // 此时说明这里是一个添加的逻辑，将新的数据添加到table[staleSlot] 对应的slot中。 tab[staleSlot].value = null; tab[staleSlot] = new Entry(key, value); // If there are any other stale entries in run, expunge them // 最后判断除了staleSlot以外，还发现了其他过期的slot数据，就要开启清理数据的逻辑 if (slotToExpunge != staleSlot) cleanSomeSlots(expungeStaleEntry(slotToExpunge), len);} 3.4 探测式清理探测式清理，也就是 expungeStaleEntry方法，遍历散列数组，从开始位置向后探测清理过期数据，将过期数据的 Entry设置为 null，沿途中碰到未过期的数据则将此数据 rehash后重新在 table数组中定位，如果定位的位置已经有了数据，则会将未过期的数据放到最靠近此位置的 Entry=null的桶中，使 rehash后的 Entry数据距离正确的桶的位置更近一些，查询的时候效率才会更高。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * Expunge a stale entry by rehashing any possibly colliding entries * lying between staleSlot and the next null slot. This also expunges * any other stale entries encountered before the trailing null. See * Knuth, Section 6.4 * * @param staleSlot index of slot known to have null key * @return the index of the next null slot after staleSlot * (all between staleSlot and this slot will have been checked * for expunging). */private int expungeStaleEntry(int staleSlot) { Entry[] tab = table; int len = tab.length; // expunge entry at staleSlot // 将tab[staleSlot]槽位的数据清空 tab[staleSlot].value = null; tab[staleSlot] = null; size--; // Rehash until we encounter null Entry e; int i; // 以staleSlot位置往后迭代遍历 for (i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) { ThreadLocal&lt;?&gt; k = e.get(); // k==null的过期数据，清空该槽位数据 if (k == null) { e.value = null; tab[i] = null; size--; } else { // key没有过期，重新计算当前key的下标位置 int h = k.threadLocalHashCode &amp; (len - 1); // 不是当前槽位下标位置，那么说明产生了hash冲突， if (h != i) { tab[i] = null; // Unlike Knuth 6.4 Algorithm R, we must scan until // null because multiple entries could have been stale. // 此时以新计算出来正确的槽位位置往后迭代，找到最近一个可以存放entry的位置 while (tab[h] != null) h = nextIndex(h, len); tab[h] = e; } } } return i;} 3.5 启发式清理12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * Heuristically scan some cells looking for stale entries. * This is invoked when either a new element is added, or * another stale one has been expunged. It performs a * logarithmic number of scans, as a balance between no * scanning (fast but retains garbage) and a number of scans * proportional to number of elements, that would find all * garbage but would cause some insertions to take O(n) time. * * @param i a position known NOT to hold a stale entry. The * scan starts at the element after i. * * @param n scan control: {@code log2(n)} cells are scanned, * unless a stale entry is found, in which case * {@code log2(table.length)-1} additional cells are scanned. * When called from insertions, this parameter is the number * of elements, but when from replaceStaleEntry, it is the * table length. (Note: all this could be changed to be either * more or less aggressive by weighting n instead of just * using straight log n. But this version is simple, fast, and * seems to work well.) * * @return true if any stale entries have been removed. *//** * 主要作用： * 从i位置开始搜索，如果在log2n个连续节点内发现了失效条目(key=null，value&lt;&gt;null)则进行清理失效条目， * 并且重置n的值为len，如此循环，直到连续log2n个节点都是正常节点时才会跳出while。 * (和expungeStaleEntry(i)的区别就是它的清理范围更长，只要在log2n个连续节点有失效条目， * 那么你的清理范围就可以一直延长，极端情况下可能会清理整个数组，也可能没有清理掉任何节点) */ private boolean cleanSomeSlots(int i, int n) { boolean removed = false; Entry[] tab = table; int len = tab.length; do { i = nextIndex(i, len); Entry e = tab[i]; if (e != null &amp;&amp; e.get() == null) { n = len; removed = true; i = expungeStaleEntry(i); } } while ( (n &gt;&gt;&gt;= 1) != 0); return removed;} 3.6 扩容在 ThreadLocalMap.set()方法的最后，如果执行完启发式清理工作后，未清理到任何数据，且当前散列数组中 Entry的数量已经达到了列表的扩容阈值 (len*2/3)，就开始执行 rehash()逻辑。 在真正扩容之前，先尝试回收一次key为null的值，腾出一些空间。 如果回收之后的size大于等于threshold的3/4时，才需要真正的扩容。 也就是说添加数据后，新的size大于等于老size的1/2 （1/2 = 2/3 * 3/4）时，才需要扩容。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/** * Re-pack and/or re-size the table. First scan the entire * table removing stale entries. If this doesn't sufficiently * shrink the size of the table, double the table size. */private void rehash() { expungeStaleEntries(); // Use lower threshold for doubling to avoid hysteresis // 清理完成之后，table中可能有一些key为null的Entry数据被清理掉， // 所以此时通过判断size &gt;= threshold - threshold / 4 也就是size &gt;= threshold * 3/4 来决定是否扩容。 if (size &gt;= threshold - threshold / 4) resize();}/** * Double the capacity of the table. */private void resize() { Entry[] oldTab = table; int oldLen = oldTab.length; int newLen = oldLen * 2; Entry[] newTab = new Entry[newLen]; int count = 0; for (int j = 0; j &lt; oldLen; ++j) { Entry e = oldTab[j]; if (e != null) { ThreadLocal&lt;?&gt; k = e.get(); if (k == null) { e.value = null; // Help the GC } else { int h = k.threadLocalHashCode &amp; (newLen - 1); while (newTab[h] != null) h = nextIndex(h, newLen); newTab[h] = e; count++; } } } setThreshold(newLen); size = count; table = newTab;}/** * Expunge all stale entries in the table. */private void expungeStaleEntries() { Entry[] tab = table; int len = tab.length; for (int j = 0; j &lt; len; j++) { Entry e = tab[j]; if (e != null &amp;&amp; e.get() == null) expungeStaleEntry(j); }} 3.7 Get源码解析12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * Get the entry associated with key. This method * itself handles only the fast path: a direct hit of existing * key. It otherwise relays to getEntryAfterMiss. This is * designed to maximize performance for direct hits, in part * by making this method readily inlinable. * * @param key the thread local object * @return the entry associated with key, or null if no such */private Entry getEntry(ThreadLocal&lt;?&gt; key) { int i = key.threadLocalHashCode &amp; (table.length - 1); Entry e = table[i]; // 命中直接返回 if (e != null &amp;&amp; e.get() == key) return e; else //存在哈希冲突 return getEntryAfterMiss(key, i, e);}/** * Version of getEntry method for use when key is not found in * its direct hash slot. * * @param key the thread local object * @param i the table index for key's hash code * @param e the entry at table[i] * @return the entry associated with key, or null if no such */private Entry getEntryAfterMiss(ThreadLocal&lt;?&gt; key, int i, Entrye) { Entry[] tab = table; int len = tab.length; while (e != null) { ThreadLocal&lt;?&gt; k = e.get(); if (k == key) //命中 return e; if (k == null) // 清理过期 Entry expungeStaleEntry(i); else // 线性探测：下一个元素 i = nextIndex(i, len); e = tab[i]; } return null;} 4. InheritableThreadLocal在父线程中调用 new Thread()方法来创建子线程时，Thread#init方法在 Thread的构造方法中被调用，在 init方法中拷贝父线程数据到子线程中。 12345678910111213private void init(ThreadGroup g, Runnable target, String name, long stackSize, AccessControlContext acc, boolean inheritThreadLocals) { if (name == null) { throw new NullPointerException(&quot;name cannot be null&quot;); } if (inheritThreadLocals &amp;&amp; parent.inheritableThreadLocals != null) this.inheritableThreadLocals = ThreadLocal.createInheritedMap(parent.inheritableThreadLocals); this.stackSize = stackSize; tid = nextThreadID();} 但 InheritableThreadLocal仍然有缺陷，一般我们做异步化处理都是使用的线程池，而 InheritableThreadLocal是在 new Thread中的 init()方法给赋值的，而线程池是线程复用的逻辑，所以这里会存在问题。 当然，有问题出现就会有解决问题的方案，阿里巴巴开源了一个 TransmittableThreadLocal组件就可以解决这个问题。 5. 存在的问题5.1 内存泄漏ThreadLocalMap 中使用的 key 为 ThreadLocal 的弱引用，而 value 是强引用。所以，如果 ThreadLocal 没有被外部强引用的情况下，在垃圾回收的时候，key 会被清理掉，而 value 不会被清理掉。 这样一来，ThreadLocalMap 中就会出现 key 为 null 的 Entry。假如我们不做任何措施的话，value 永远无法被 GC 回收，这个时候就可能会产生内存泄露。ThreadLocalMap 实现中已经考虑了这种情况，在调用 set()、get()、remove() 方法的时候，会清理掉 key 为 null 的记录，但这种操作具有滞后性，且不是全数组扫描，可能会存在漏掉的情况。 根本解决内存泄漏的问题，就是在使用完 ThreadLocal方法后手动调用 remove()方法。 注： Java的 四种引用类型 ： 强引用 ：我们常常 new 出来的对象就是强引用类型，只要强引用存在，垃圾回收器将永远不会回收被引用的对象，哪怕内存不足的时候 软引用 ：使用 SoftReference 修饰的对象被称为软引用，软引用指向的对象在内存要溢出的时候被回收 弱引用 ：使用 WeakReference 修饰的对象被称为弱引用，只要发生垃圾回收，若这个对象只被弱引用指向，那么就会被回收 虚引用 ：虚引用是最弱的引用，在 Java 中使用 PhantomReference 进行定义。虚引用中唯一的作用就是用队列接收对象即将死亡的通知 （1）如果key为强引用会有什么问题？ 我们都知道ThreadLocal变量对ThreadLocal对象是有强引用存在的。 即使ThreadLocal变量生命周期完了，设置成null了，但由于key对ThreadLocal还是强引用。 此时，如果执行该代码的线程使用了线程池，一直长期存在，不会被销毁。 就会存在这样的强引用链：Thread变量 -&gt; Thread对象 -&gt; ThreadLocalMap -&gt; Entry -&gt; key -&gt; ThreadLocal对象。 那么，ThreadLocal对象和ThreadLocalMap都将不会被GC回收，于是产生了内存泄露问题。 （2）Entry的value为什么不设计成弱引用？ Entry的value假如只是被Entry引用，有可能没被业务系统中的其他地方引用。如果将value改成了弱引用，被GC贸然回收了（数据突然没了），可能会导致业务系统出现异常。 参考文献（1）ThreadLocal 详解：https://javaguide.cn/java/concurrent/threadlocal.html （2）threadlocal value为什么不是弱引用？https://www.zhihu.com/question/399087116/answer/2679320042","link":"/2023/11/02/java_threadlocal/"},{"title":"Redis","text":"1. Redis 为什么这么快？ Redis 内部做了非常多的性能优化，比较重要的有下面 3 点 （1）Redis 基于内存，内存的访问速度是磁盘的上千倍 （2）Redis 基于 Reactor 模式设计开发了一套高效的事件处理模型，主要是单线程事件循环和 IO 多路复用 （3）Redis 内置了多种优化过后的数据结构实现，性能非常高 2. 为什么要用 Redis（1）高性能 假如用户第一次访问数据库中的某些数据的话，这个过程是比较慢，毕竟是从硬盘中读取的。但是，如果说，用户访问的数据属于高频数据并且不会经常改变的话，那么我们就可以很放心地将该用户访问的数据存在缓存中。这样有什么好处呢？ 那就是保证用户下一次再访问这些数据的时候就可以直接从缓存中获取了。操作缓存就是直接操作内存，所以速度相当快。 （2）高并发 一般像 MySQL 这类的数据库的 QPS 大概都在 1w 左右（4 核 8g） ，但是使用 Redis 缓存之后很容易达到 10w+，甚至最高能达到 30w+（就单机 Redis 的情况，Redis 集群的话会更高）。 1QPS（Query Per Second）：服务器每秒可以执行的查询次数 由此可见，直接操作缓存能够承受的数据库请求数量是远远大于直接访问数据库的，所以我们可以考虑把数据库中的部分数据转移到缓存中去，这样用户的一部分请求会直接到缓存这里而不用经过数据库。进而，我们也就提高了系统整体的并发。 Redis 除了做缓存，还能做什么？ 分布式锁 ：通过 Redis 来做分布式锁是一种比较常见的方式。通常情况下，我们都是基于 Redisson 来实现分布式锁。关于 Redis 实现分布式锁的详细介绍，可以看我写的这篇文章：分布式锁详解open in new window 。 限流 ：一般是通过 Redis + Lua 脚本的方式来实现限流。相关阅读：《我司用了 6 年的 Redis 分布式限流器，可以说是非常厉害了！》open in new window。 消息队列 ：Redis 自带的 list 数据结构可以作为一个简单的队列使用。Redis 5.0 中增加的 stream 类型的数据结构更加适合用来做消息队列。它比较类似于 Kafka，有主题和消费组的概念，支持消息持久化以及 ACK 机制。 延时队列 ：Redisson 内置了延时队列（基于 sorted set 实现的）。 分布式 Session ：利用 string 或者 hash 保存 Session 数据，所有的服务器都可以访问。 复杂业务场景 ：通过 Redis 以及 Redis 扩展（比如 Redisson）提供的数据结构，我们可以很方便地完成很多复杂的业务场景比如通过 bitmap 统计活跃用户、通过 sorted set 维护排行榜。 …… 3. Redis数据结构Redis 共有 5 种基本数据结构：String（字符串）、List（列表）、Set（集合）、Hash（散列）、Zset（有序集合），底层实现主要依赖这 8 种数据结构：简单动态字符串（SDS）、LinkedList（双向链表）、Dict（哈希表/字典）、SkipList（跳跃表）、Intset（整数集合）、ZipList（压缩列表）、QuickList（快速列表）。 Redis 基本数据结构的底层数据结构实现如下： String List Hash Set Zset SDS LinkedList/ZipList/QuickList Dict、ZipList Dict、Intset ZipList、SkipList 3.1 String虽然 Redis 是用 C 语言写的，但是 Redis 并没有使用 C 的字符串表示，而是自己构建了一种 简单动态字符串 （Simple Dynamic String， SDS ）。相比于 C 的原生字符串，Redis 的 SDS 不光可以保存文本数据还可以保存二进制数据，并且获取字符串长度复杂度为 O(1)（C 字符串为 O(N)）,除此之外，Redis 的 SDS API 是安全的，不会造成缓冲区溢出。 常用命令 命令 介绍 SET key value 设置指定 key 的值 SETNX key value 只有在 key 不存在时设置 key 的值 GET key 获取指定 key 的值 MSET key1 value1 key2 value2 … 设置一个或多个指定 key 的值 MGET key1 key2 … 获取一个或多个指定 key 的值 STRLEN key 返回 key 所储存的字符串值的长度 INCR key 将 key 中储存的数字值增一 DECR key 将 key 中储存的数字值减一 EXISTS key 判断指定 key 是否存在 DEL key（通用） 删除指定的 key EXPIRE key seconds（通用） 给指定 key 设置过期时间 3.2 ListRedis 3.2 之前，List 底层实现是 LinkedList 或者 ZipList。 Redis 3.2 之后，引入了 LinkedList 和 ZipList 的结合 QuickList，List 的底层实现变为 QuickList。从 Redis 7.0 开始， ZipList 被 ListPack 取代。 常用命令 命令 介绍 RPUSH key value1 value2 … 在指定列表的尾部（右边）添加一个或多个元素 LPUSH key value1 value2 … 在指定列表的头部（左边）添加一个或多个元素 LSET key index value 将指定列表索引 index 位置的值设置为 value LPOP key 移除并获取指定列表的第一个元素(最左边) RPOP key 移除并获取指定列表的最后一个元素(最右边) LLEN key 获取列表元素数量 LRANGE key start end 获取列表 start 和 end 之间 的元素 3.3 SetRedis 中的 Set 类型是一种无序集合，集合中的元素没有先后顺序但都唯一，有点类似于 Java 中的 HashSet 。当你需要存储一个列表数据，又不希望出现重复数据时，Set 是一个很好的选择，并且 Set 提供了判断某个元素是否在一个 Set 集合内的重要接口，这个也是 List 所不能提供的。 常用命令 命令 介绍 SADD key member1 member2 … 向指定集合添加一个或多个元素 SMEMBERS key 获取指定集合中的所有元素 SCARD key 获取指定集合的元素数量 SISMEMBER key member 判断指定元素是否在指定集合中 SINTER key1 key2 … 获取给定所有集合的交集 SINTERSTORE destination key1 key2 … 将给定所有集合的交集存储在 destination 中 SUNION key1 key2 … 获取给定所有集合的并集 SUNIONSTORE destination key1 key2 … 将给定所有集合的并集存储在 destination 中 SDIFF key1 key2 … 获取给定所有集合的差集 SDIFFSTORE destination key1 key2 … 将给定所有集合的差集存储在 destination 中 SPOP key count 随机移除并获取指定集合中一个或多个元素 SRANDMEMBER key count 随机获取指定集合中指定数量的元素 3.4 Hash常用命令 命令 介绍 HSET key field value 设置指定哈希表中指定字段的值 HSETNX key field value 只有指定字段不存在时设置指定字段的值 HMSET key field1 value1 field2 value2 … 同时将一个或多个 field-value (域-值)对设置到指定哈希表中 HGET key field 获取指定哈希表中指定字段的值 HMGET key field1 field2 … 获取指定哈希表中一个或者多个指定字段的值 HGETALL key 获取指定哈希表中所有的键值对 HEXISTS key field 查看指定哈希表中指定的字段是否存在 HDEL key field1 field2 … 删除一个或多个哈希表字段 HLEN key 获取指定哈希表中字段的数量 HINCRBY key field increment 对指定哈希中的指定字段做运算操作（正数为加，负数为减） 3.5 ZsetSorted Set 类似于 Set，但和 Set 相比，Sorted Set 增加了一个权重参数 score，使得集合中的元素能够按 score 进行有序排列，还可以通过 score 的范围来获取元素的列表。有点像是 Java 中 HashMap 和 TreeSet 的结合体。 常用命令 命令 介绍 ZADD key score1 member1 score2 member2 … 向指定有序集合添加一个或多个元素 ZCARD KEY 获取指定有序集合的元素数量 ZSCORE key member 获取指定有序集合中指定元素的 score 值 ZINTERSTORE destination numkeys key1 key2 … 将给定所有有序集合的交集存储在 destination 中，对相同元素对应的 score 值进行 SUM 聚合操作，numkeys 为集合数量 ZUNIONSTORE destination numkeys key1 key2 … 求并集，其它和 ZINTERSTORE 类似 ZDIFFSTORE destination numkeys key1 key2 … 求差集，其它和 ZINTERSTORE 类似 ZRANGE key start end 获取指定有序集合 start 和 end 之间的元素（score 从低到高） ZREVRANGE key start end 获取指定有序集合 start 和 end 之间的元素（score 从高到底） ZREVRANK key member 获取指定有序集合中指定元素的排名(score 从大到小排序) 3.6 HyperLogLogs（基数统计）3.7 Bitmap（位存储）3.8 Geospatial(地理位置)4. Redis持久化机制Redis 不同于 Memcached 的很重要一点就是，Redis 支持持久化，而且支持 3 种持久化方式: 快照（snapshotting，RDB） 只追加文件（append-only file, AOF） RDB 和 AOF 的混合持久化(Redis 4.0 新增) 4.1 RDBRedis 可以通过创建快照来获得存储在内存里面的数据在 某个时间点 上的副本。Redis 创建快照之后，可以对快照进行备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本（Redis 主从结构，主要用来提高 Redis 性能），还可以将快照留在原地以便重启服务器的时候使用。 Redis 提供了两个命令来生成 RDB 快照文件： save : 同步保存操作，会阻塞 Redis 主线程； bgsave : fork 出一个子进程，子进程执行，不会阻塞 Redis 主线程，默认选项。 4.2 AOF（1）介绍 开启 AOF 持久化后每执行一条会更改 Redis 中的数据的命令，Redis 就会将该命令写入到 AOF 缓冲区 server.aof_buf 中，然后再写入到 AOF 文件中（此时还在系统内核缓存区未同步到磁盘），最后再根据持久化方式（ fsync策略）的配置来决定何时将系统内核缓存区的数据同步到硬盘中的。 只有同步到磁盘中才算持久化保存了，否则依然存在数据丢失的风险，比如说：系统内核缓存区的数据还未同步，磁盘机器就宕机了，那这部分数据就算丢失了。 （2）工作流程 AOF 持久化功能的实现可以简单分为 5 步： 命令追加（append） ：所有的写命令会追加到 AOF 缓冲区中。 文件写入（write） ：将 AOF 缓冲区的数据写入到 AOF 文件中。这一步需要调用 write函数（系统调用），write将数据写入到了系统内核缓冲区之后直接返回了（延迟写）。注意！！！此时并没有同步到磁盘。 文件同步（fsync） ：AOF 缓冲区根据对应的持久化方式（ fsync 策略）向硬盘做同步操作。这一步需要调用 fsync 函数（系统调用）， fsync 针对单个文件操作，对其进行强制硬盘同步，fsync 将阻塞直到写入磁盘完成后返回，保证了数据持久化。 文件重写（rewrite） ：随着 AOF 文件越来越大，需要定期对 AOF 文件进行重写，达到压缩的目的。 重启加载（load） ：当 Redis 重启时，可以加载 AOF 文件进行数据恢复。 （3）持久化方式 在 Redis 的配置文件中存在三种不同的 AOF 持久化方式（ fsync策略），主要区别在于 fsync 同步 AOF 文件的时机（刷盘） 。它们分别是： appendfsync always：主线程调用 write 执行写操作后，后台线程（ aof_fsync 线程）立即会调用 fsync 函数同步 AOF 文件（刷盘），fsync 完成后线程返回，这样会严重降低 Redis 的性能（write + fsync）。 appendfsync everysec：主线程调用 write 执行写操作后立即返回，由后台线程（ aof_fsync 线程）每秒钟调用 fsync 函数（系统调用）同步一次 AOF 文件（write+fsync，fsync间隔为 1 秒） appendfsync no：主线程调用 write 执行写操作后立即返回，让操作系统决定何时进行同步，Linux 下一般为 30 秒一次（write但不 fsync，fsync 的时机由操作系统决定）。 为了兼顾数据和写入性能，可以考虑 appendfsync everysec 选项 ，让 Redis 每秒同步一次 AOF 文件，Redis 性能受到的影响较小。而且这样即使出现系统崩溃，用户最多只会丢失一秒之内产生的数据。当硬盘忙于执行写入操作的时候，Redis 还会优雅的放慢自己的速度以便适应硬盘的最大写入速度。 从 Redis 7.0.0 开始，Redis 使用了 Multi Part AOF 机制。顾名思义，Multi Part AOF 就是将原来的单个 AOF 文件拆分成多个 AOF 文件。 （4）执行时机 关系型数据库（如 MySQL）通常都是执行命令之前记录日志（方便故障恢复），而 Redis AOF 持久化机制是在执行完命令之后再记录日志。 为什么是在执行完命令之后记录日志呢？ 避免额外的检查开销，AOF 记录日志不会对命令进行语法检查； 在命令执行完之后再记录，不会阻塞当前的命令执行。 这样也带来了风险（我在前面介绍 AOF 持久化的时候也提到过）： 如果刚执行完命令 Redis 就宕机会导致对应的修改丢失； 可能会阻塞后续其他命令的执行（AOF 记录日志是在 Redis 主线程中进行的）。 （5）AOF重写 当 AOF 变得太大时，Redis 能够在后台自动重写 AOF 产生一个新的 AOF 文件，这个新的 AOF 文件和原有的 AOF 文件所保存的数据库状态一样，但体积更小。 由于 AOF 重写会进行大量的写入操作，为了避免对 Redis 正常处理命令请求造成影响，Redis 将 AOF 重写程序放到子进程里执行。 AOF 文件重写期间，Redis 还会维护一个 AOF 重写缓冲区 ，该缓冲区会在子进程创建新 AOF 文件期间，记录服务器执行的所有写命令。当子进程完成创建新 AOF 文件的工作之后，服务器会将重写缓冲区中的所有内容追加到新 AOF 文件的末尾，使得新的 AOF 文件保存的数据库状态与现有的数据库状态一致。最后，服务器用新的 AOF 文件替换旧的 AOF 文件，以此来完成 AOF 文件重写操作。 （6）AOF校验机制 AOF 校验机制是 Redis 在启动时对 AOF 文件进行检查，以判断文件是否完整，是否有损坏或者丢失的数据。这个机制的原理其实非常简单，就是通过使用一种叫做 校验和（checksum） 的数字来验证 AOF 文件。这个校验和是通过对整个 AOF 文件内容进行 CRC64 算法计算得出的数字。 4.3 RDB vs AOF关于 RDB 和 AOF 的优缺点，官网上面也给了比较详细的说明Redis persistenceopen in new window，这里结合自己的理解简单总结一下。 RDB 比 AOF 优秀的地方 ： RDB 文件存储的内容是经过压缩的二进制数据， 保存着某个时间点的数据集，文件很小，适合做数据的备份，灾难恢复。AOF 文件存储的是每一次写命令，类似于 MySQL 的 binlog 日志，通常会比 RDB 文件大很多。当 AOF 变得太大时，Redis 能够在后台自动重写 AOF。新的 AOF 文件和原有的 AOF 文件所保存的数据库状态一样，但体积更小。不过， Redis 7.0 版本之前，如果在重写期间有写入命令，AOF 可能会使用大量内存，重写期间到达的所有写入命令都会写入磁盘两次。 使用 RDB 文件恢复数据，直接解析还原数据即可，不需要一条一条地执行命令，速度非常快。而 AOF 则需要依次执行每个写命令，速度非常慢。也就是说，与 AOF 相比，恢复大数据集的时候，RDB 速度更快。 AOF 比 RDB 优秀的地方 ： RDB 的数据安全性不如 AOF，没有办法实时或者秒级持久化数据。生成 RDB 文件的过程是比较繁重的， 虽然 BGSAVE 子进程写入 RDB 文件的工作不会阻塞主线程，但会对机器的 CPU 资源和内存资源产生影响，严重的情况下甚至会直接把 Redis 服务干宕机。AOF 支持秒级数据丢失（取决 fsync 策略，如果是 everysec，最多丢失 1 秒的数据），仅仅是追加命令到 AOF 文件，操作轻量。 RDB 文件是以特定的二进制格式保存的，并且在 Redis 版本演进中有多个版本的 RDB，所以存在老版本的 Redis 服务不兼容新版本的 RDB 格式的问题。 AOF 以一种易于理解和解析的格式包含所有操作的日志。你可以轻松地导出 AOF 文件进行分析，你也可以直接操作 AOF 文件来解决一些问题。比如，如果执行 FLUSHALL命令意外地刷新了所有内容后，只要 AOF 文件没有被重写，删除最新命令并重启即可恢复之前的状态。 综上 ： Redis 保存的数据丢失一些也没什么影响的话，可以选择使用 RDB。 不建议单独使用 AOF，因为时不时地创建一个 RDB 快照可以进行数据库备份、更快的重启以及解决 AOF 引擎错误。 如果保存的数据要求安全性比较高的话，建议同时开启 RDB 和 AOF 持久化或者开启 RDB 和 AOF 混合持久化。 5. Redis线程模型对于读写命令来说，Redis 一直是单线程模型。不过，在 Redis 4.0 版本之后引入了多线程来执行一些大键值对的异步删除操作， Redis 6.0 版本之后引入了多线程来处理网络请求（提高网络 IO 读写性能）。 5.1 单线程模型Redis 基于 Reactor 模式设计开发了一套高效的事件处理模型 （Netty 的线程模型也基于 Reactor 模式，Reactor 模式不愧是高性能 IO 的基石），这套事件处理模型对应的是 Redis 中的文件事件处理器（file event handler）。由于文件事件处理器（file event handler）是单线程方式运行的，所以我们一般都说 Redis 是单线程模型。 Redis 基于 Reactor 模式开发了自己的网络事件处理器：这个处理器被称为文件事件处理器（file event handler）。 文件事件处理器使用 I/O 多路复用（multiplexing）程序来同时监听多个套接字，并根据套接字目前执行的任务来为套接字关联不同的事件处理器。 当被监听的套接字准备好执行连接应答（accept）、读取（read）、写入（write）、关 闭（close）等操作时，与操作相对应的文件事件就会产生，这时文件事件处理器就会调用套接字之前关联好的事件处理器来处理这些事件。 虽然文件事件处理器以单线程方式运行，但通过使用 I/O 多路复用程序来监听多个套接字 ，文件事件处理器既实现了高性能的网络通信模型，又可以很好地与 Redis 服务器中其他同样以单线程方式运行的模块进行对接，这保持了 Redis 内部单线程设计的简单性。 Reactor模式称为反应器模式或应答者模式，是基于事件驱动的设计模式，拥有一个或多个并发输入源，有一个服务处理器和多个请求处理器，服务处理器会同步的将输入的请求事件以多路复用的方式分发给相应的请求处理器。 Reactor设计模式是一种为处理并发服务请求，并将请求提交到一个或多个服务处理程序的事件设计模式。当客户端请求抵达后，服务处理程序使用多路分配策略，由一个非阻塞的线程来接收所有请求，然后将请求派发到相关的工作线程并进行处理的过程。 在事件驱动的应用中，将一个或多个客户端的请求分离和调度给应用程序，同步有序地接收并处理多个服务请求。对于高并发系统经常会使用到Reactor模式，用来替代常用的多线程处理方式以节省系统资源并提高系统的吞吐量。 5.2 Redis6.0 多线程（1）Redis6.0之前为什么不使用多线程？ 主要原因有 3 点： 单线程编程容易并且更容易维护； Redis 的性能瓶颈不在 CPU ，主要在内存和网络； 多线程就会存在死锁、线程上下文切换等问题，甚至会影响性能。 （2）Redis6.0引入多线程 Redis6.0 引入多线程主要是为了提高网络 IO 读写性能 ，因为这个算是 Redis 中的一个性能瓶颈（Redis 的瓶颈主要受限于内存和网络）。 虽然，Redis6.0 引入了多线程，但是 Redis 的多线程只是在网络数据的读写这类耗时操作上使用了，执行命令仍然是单线程顺序执行。因此，你也不需要担心线程安全问题。 但是官网描述开启多线程读并不能有太大提升，因此一般情况下并不建议开启。 6. Redis内存管理6.1 设置过期时间一般情况下，我们设置保存的缓存数据的时候都会设置一个过期时间。Redis 中除了字符串类型有自己独有设置过期时间的命令 setex 外，其他方法都需要依靠 expire 命令来设置过期时间 。另外， persist 命令可以移除一个键的过期时间。 6.2 判断数据过期Redis 通过一个叫做过期字典（可以看作是 hash 表）来保存数据过期的时间。过期字典的键指向 Redis 数据库中的某个 key(键)，过期字典的值是一个 long long 类型的整数，这个整数保存了 key 所指向的数据库键的过期时间（毫秒精度的 UNIX 时间戳）。 6.3 删除策略 &amp; 内存淘汰常用的过期数据的删除策略就两个（重要！自己造缓存轮子的时候需要格外考虑的东西）： 惰性删除 ：只会在取出 key 的时候才对数据进行过期检查。这样对 CPU 最友好，但是可能会造成太多过期 key 没有被删除。 定期删除 ：每隔一段时间抽取一批 key 执行删除过期 key 操作。并且，Redis 底层会通过限制删除操作执行的时长和频率来减少删除操作对 CPU 时间的影响。 定期删除对内存更加友好，惰性删除对 CPU 更加友好。两者各有千秋，所以 Redis 采用的是 定期删除+惰性/懒汉式删除 。 但是，仅仅通过给 key 设置过期时间还是有问题的。因为还是可能存在定期删除和惰性删除漏掉了很多过期 key 的情况。这样就导致大量过期 key 堆积在内存里，然后就 Out of memory 了。Redis通过内存淘汰机制来保障。 Redis 提供 6 种数据淘汰策略： volatile-lru（least recently used） ：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰。 volatile-ttl ：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰。 volatile-random ：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰。 allkeys-lru（least recently used） ：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key（这个是最常用的）。 allkeys-random ：从数据集（server.db[i].dict）中任意选择数据淘汰。 no-eviction ：禁止驱逐数据，也就是说当内存不足以容纳新写入数据时，新写入操作会报错。这个应该没人使用吧！ 4.0 版本后增加以下两种： volatile-lfu（least frequently used） ：从已设置过期时间的数据集（server.db[i].expires）中挑选最不经常使用的数据淘汰。 allkeys-lfu（least frequently used） ：当内存不足以容纳新写入数据时，在键空间中，移除最不经常使用的 key。 7. Redis事务（1）介绍 Redis 事务提供了一种将多个命令请求打包的功能。然后，再按顺序执行打包的所有命令，并且不会被中途打断。（Redis的单线程特性保证了执行事务时不会执行其他命令） Redis 事务实际开发中使用的非常少，功能比较鸡肋，不要将其和我们平时理解的关系型数据库的事务混淆了。 除了不满足原子性和持久性之外，事务中的每条命令都会与 Redis 服务器进行网络交互，这是比较浪费资源的行为。明明一次批量执行多个命令就可以了，这种操作实在是看不懂。 因此，Redis 事务是不建议在日常开发中使用的。 （2）使用 Redis 可以通过 MULTI，EXEC，DISCARD 和 WATCH 等命令来实现事务(Transaction)功能。 123456789&gt; MULTIOK&gt; SET PROJECT &quot;JavaGuide&quot;QUEUED&gt; GET PROJECTQUEUED&gt; EXEC1) OK2) &quot;JavaGuide&quot; MULTI命令后可以输入多个命令，Redis 不会立即执行这些命令，而是将它们放到队列，当调用了 EXEC命令后，再执行所有的命令。 这个过程是这样的： 开始事务（MULTI）； 命令入队(批量操作 Redis 的命令，先进先出（FIFO）的顺序执行)； 执行事务(EXEC)。 你也可以通过 DISCARD命令取消一个事务，它会清空事务队列中保存的所有命令。 WATCH命令会在事务开始之前盯住一个或多个关键变量，当事务执行时，也就是服务器收到了exec指令要顺序执行缓存的事务队列时，Redis会检查关键变量自wathc之后是否被修改了（包括当前事务所在的客户端）。如果关键变量被修改了，exec指令就会返回NULL告诉客户端事务执行失败。 （3）不满足原子性和持久性 Redis 事务在运行错误的情况下，除了执行过程中出现错误的命令外，其他命令都能正常执行。并且，Redis 事务是不支持回滚（roll back）操作的。因此，Redis 事务其实是不满足原子性的。 AOF 持久化的 fsync策略为 no、everysec 时都会存在数据丢失的情况 。always 下可以基本是可以满足持久性要求的，但性能太差，实际开发过程中不会使用。因此，Redis 事务的持久性也是没办法保证的。 8. Redis性能优化8.1 使用批量操作减少网络传输一个 Redis 命令的执行可以简化为以下 4 步： 发送命令 命令排队 命令执行 返回结果 其中，第 1 步和第 4 步耗费时间之和称为 Round Trip Time (RTT,往返时间) ，也就是数据在网络上传输的时间。 使用批量操作可以减少网络传输次数，进而有效减小网络开销，大幅减少 RTT。 另外，除了能减少 RTT 之外，发送一次命令的 socket I/O 成本也比较高（涉及上下文切换，存在 read()和 write()系统调用），批量操作还可以减少 socket I/O 成本 （1）原生批量操作指令 Redis 中有一些原生支持批量操作的命令，比如： MGET(获取一个或多个指定 key 的值)、MSET(设置一个或多个指定 key 的值)、 HMGET(获取指定哈希表中一个或者多个指定字段的值)、HMSET(同时将一个或多个 field-value 对设置到指定哈希表中)、 SADD（向指定集合添加一个或多个元素） …… 注意：在 Redis 官方提供的分片集群解决方案 Redis Cluster 下，使用这些原生批量操作命令可能会存在一些小问题需要解决。就比如说 MGET 无法保证所有的 key 都在同一个 hash slot （哈希槽）上，MGET可能还是需要多次网络传输，原子操作也无法保证了。不过，相较于非批量操作，还是可以节省不少网络传输次数。 （2）pipeline 对于不支持批量操作的命令，我们可以利用 pipeline（流水线) 将一批 Redis 命令封装成一组，这些 Redis 命令会被一次性提交到 Redis 服务器，只需要一次网络传输。不过，需要注意控制一次批量操作的 元素个数 (例如 500 以内，实际也和元素字节数有关)，避免网络传输的数据量过大。另外，pipeline 不适用于执行顺序有依赖关系的一批命令。就比如说，你需要将前一个命令的结果给后续的命令使用，pipeline 就没办法满足你的需求了。对于这种需求，我们可以使用 Lua 脚本 。 原生批量操作命令和 pipeline 的是有区别的，使用的时候需要注意： 原生批量操作命令是原子操作，pipeline 是非原子操作。 pipeline 可以打包不同的命令，原生批量操作命令不可以。 原生批量操作命令是 Redis 服务端支持实现的，而 pipeline 需要服务端和客户端的共同实现。 顺带补充一下 pipeline 和 Redis 事务的对比： 事务是原子操作，pipeline 是非原子操作。两个不同的事务不会同时运行，而 pipeline 可以同时以交错方式执行。 Redis 事务中每个命令都需要发送到服务端，而 Pipeline 只需要发送一次，请求次数更少。 （3）lua脚本 Lua 脚本同样支持批量操作多条命令。一段 Lua 脚本可以视作一条命令执行，可以看作是 原子操作 。也就是说，一段 Lua 脚本执行过程中不会有其他脚本或 Redis 命令同时执行，保证了操作不会被其他指令插入或打扰，这是 pipeline 所不具备的。 并且，Lua 脚本中支持一些简单的逻辑处理比如使用命令读取值并在 Lua 脚本中进行处理，这同样是 pipeline 所不具备的。 不过， Lua 脚本依然存在下面这些缺陷： 如果 Lua 脚本运行时出错并中途结束，之后的操作不会进行，但是之前已经发生的写操作不会撤销，所以即使使用了 Lua 脚本，也不能实现类似数据库回滚的原子性。 Redis Cluster 下 Lua 脚本的原子操作也无法保证了，原因同样是无法保证所有的 key 都在同一个 hash slot （哈希槽）上。 8.2 大量 key 集中过期定期删除执行过程中，如果突然遇到大量过期 key 的话，客户端请求必须等待定期清理过期 key 任务线程执行完成，因为这个这个定期任务线程是在 Redis 主线程中执行的。这就导致客户端请求没办法被及时处理，响应速度会比较慢。 如何解决呢？ 下面是两种常见的方法： 给 key 设置随机过期时间。 开启 lazy-free（惰性删除/延迟释放） 。lazy-free 特性是 Redis 4.0 开始引入的，指的是让 Redis 采用异步方式延迟释放 key 使用的内存，将该操作交给单独的子线程处理，避免阻塞主线程。 8.3 bigkey（大 Key）（1）介绍 简单来说，如果一个 key 对应的 value 所占用的内存比较大，那这个 key 就可以看作是 bigkey。具体多大才算大呢？有一个不是特别精确的参考标准：string 类型的 value 超过 10 kb，复合类型的 value 包含的元素超过 5000 个（对于复合类型的 value 来说，不一定包含的元素越多，占用的内存就越多）。 bigkey 除了会消耗更多的内存空间和带宽，还会对性能造成比较大的影响。因此，我们应该尽量避免 Redis 中存在 bigkey。 （2）如何发现 bigkey 使用 Redis 自带的 --bigkeys 参数来查找。这种方式只能找出每种数据结构 top 1 bigkey（占用内存最大的 string 数据类型，包含元素最多的复合数据类型）。然而，一个 key 的元素多并不代表占用内存也多，需要我们根据具体的业务情况来进一步判断。 借助开源工具分析 RDB 文件 借助公有云的 Redis 分析服务 （3）bigkey 处理 bigkey 的常见处理以及优化办法如下（这些方法可以配合起来使用）： 分割 bigkey ：将一个 bigkey 分割为多个小 key。这种方式需要修改业务层的代码，一般不推荐这样做。 手动清理 ：Redis 4.0+ 可以使用 UNLINK 命令来异步删除一个或多个指定的 key。Redis 4.0 以下可以考虑使用 SCAN 命令结合 DEL 命令来分批次删除。 采用合适的数据结构 ：比如使用 HyperLogLog 统计页面 UV。 开启 lazy-free（惰性删除/延迟释放） ：lazy-free 特性是 Redis 4.0 开始引入的，指的是让 Redis 采用异步方式延迟释放 key 使用的内存，将该操作交给单独的子线程处理，避免阻塞主线程。 8.4 hotkey（热 Key）（1）介绍 简单来说，如果一个 key 的访问次数比较多且明显多于其他 key 的话，那这个 key 就可以看作是 hotkey。例如在 Redis 实例的每秒处理请求达到 5000 次，而其中某个 key 的每秒访问量就高达 2000 次，那这个 key 就可以看作是 hotkey。 处理 hotkey 会占用大量的 CPU 和带宽，可能会影响 Redis 实例对其他请求的正常处理。此外，如果突然访问 hotkey 的请求超出了 Redis 的处理能力，Redis 就会直接宕机。这种情况下，大量请求将落到后面的数据库上，可能会导致数据库崩溃。 因此，hotkey 很可能成为系统性能的瓶颈点，需要单独对其进行优化，以确保系统的高可用性和稳定性。 （2）如何发现hotkey 使用 Redis 自带的 --hotkeys 参数来查找 使用MONITOR 命令。MONITOR 命令是 Redis 提供的一种实时查看 Redis 的所有操作的方式，可以用于临时监控 Redis 实例的操作情况，包括读写、删除等操作。由于该命令对 Redis 性能的影响比较大，因此禁止长时间开启 MONITOR（生产环境中建议谨慎使用该命令） 借助开源项目 根据业务情况提前预估 业务代码中记录分析 借助公有云的 Redis 分析服务 （3）如何解决 hotkey？ hotkey 的常见处理以及优化办法如下（这些方法可以配合起来使用）： 读写分离 ：主节点处理写请求，从节点处理读请求。 使用 Redis Cluster ：将热点数据分散存储在多个 Redis 节点上。 二级缓存 ：hotkey 采用二级缓存的方式进行处理，将 hotkey 存放一份到 JVM 本地内存中（可以用 Caffeine）。 8.5 慢查询（1）介绍 Redis 中的大部分命令都是 O(1)时间复杂度，但也有少部分 O(n) 时间复杂度的命令，例如： KEYS *：会返回所有符合规则的 key。 HGETALL：会返回一个 Hash 中所有的键值对。 LRANGE：会返回 List 中指定范围内的元素。 SMEMBERS：返回 Set 中的所有元素。 SINTER/SUNION/SDIFF：计算多个 Set 的交集/并集/差集。 …… 由于这些命令时间复杂度是 O(n)，有时候也会全表扫描，随着 n 的增大，执行耗时也会越长。不过， 这些命令并不是一定不能使用，但是需要明确 N 的值。另外，有遍历的需求可以使用 HSCAN、SSCAN、ZSCAN 代替。 （2）如何找到慢查询命令？ 在 redis.conf 文件中，我们可以使用 slowlog-log-slower-than 参数设置耗时命令的阈值，并使用 slowlog-max-len 参数设置耗时命令的最大记录条数。 当 Redis 服务器检测到执行时间超过 slowlog-log-slower-than阈值的命令时，就会将该命令记录在慢查询日志(slow log) 中，这点和 MySQL 记录慢查询语句类似。当慢查询日志超过设定的最大记录条数之后，Redis 会把最早的执行命令依次舍弃。 ⚠️注意：由于慢查询日志会占用一定内存空间，如果设置最大记录条数过大，可能会导致内存占用过高的问题。 获取慢查询日志的内容很简单，直接使用 SLOWLOG GET 命令即可。慢查询日志中的每个条目都由以下六个值组成： 唯一渐进的日志标识符。 处理记录命令的 Unix 时间戳。 执行所需的时间量，以微秒为单位。 组成命令参数的数组。 客户端 IP 地址和端口。 客户端名称。 SLOWLOG GET 命令默认返回最近 10 条的的慢查询命令，你也自己可以指定返回的慢查询命令的数量 SLOWLOG GET N。 8.6 内存碎片（1）产生原因 Redis 存储存储数据的时候向操作系统申请的内存空间可能会大于数据实际需要的存储空间 频繁修改 Redis 中的数据也会产生内存碎片。当 Redis 中的某个数据删除时，Redis 通常不会轻易释放内存给操作系统。 （2）查看内存碎片情况 如果想要快速查看内存碎片率的话，你还可以通过下面这个命令： 1&gt; redis-cli -p 6379 info | grep mem_fragmentation_ratio 通常情况下，我们认为 mem_fragmentation_ratio &gt; 1.5 的话才需要清理内存碎片。 mem_fragmentation_ratio &gt; 1.5 意味着你使用 Redis 存储实际大小 2G 的数据需要使用大于 3G 的内存。 （3）清理内存碎片 Redis4.0-RC3 版本以后自带了内存整理，可以避免内存碎片率过大的问题。 12345678910111213config set activedefrag yes# 内存碎片占用空间达到 500mb 的时候开始清理config set active-defrag-ignore-bytes 500mb# 内存碎片率大于 1.5 的时候开始清理config set active-defrag-threshold-lower 50# 通过 Redis 自动内存碎片清理机制可能会对 Redis 的性能产生影响，我们可以通过下面两个参数来减少对 Redis 性能的影响# 内存碎片清理所占用 CPU 时间的比例不低于 20%config set active-defrag-cycle-min 20# 内存碎片清理所占用 CPU 时间的比例不高于 50%config set active-defrag-cycle-max 50 另外，重启节点可以做到内存碎片重新整理。如果你采用的是高可用架构的 Redis 集群的话，你可以将碎片率过高的主节点转换为从节点，以便进行安全重启。 9. Redis集群9.1 主从复制模式（1）介绍 主从复制模式中包含一个主数据库实例（master）与一个或多个从数据库实例（slave）。客户端可对主数据库进行读写操作，对从数据库进行读操作，主数据库写入的数据会实时自动同步给从数据库。 具体工作机制为： slave启动后，向master发送SYNC命令，master接收到SYNC命令后通过bgsave保存快照（即上文所介绍的RDB持久化），并使用缓冲区记录保存快照这段时间内执行的写命令 master将保存的快照文件发送给slave，并继续记录执行的写命令 slave接收到快照文件后，加载快照文件，载入数据 master快照发送完后开始向slave发送缓冲区的写命令，slave接收命令并执行，完成复制初始化 此后master每次执行一个写命令都会同步发送给slave，保持master与slave之间数据的一致性 （2）优缺点 优点： 支持主从复制，主机会自动将数据同步到从机，可以进行读写分离； 为了分载 Master 的读操作压力，Slave 服务器可以为客户端提供只读操作的服务，写服务仍然必须由Master来完成； Slave 同样可以接受其它 Slaves 的连接和同步请求，这样可以有效的分载 Master 的同步压力； Master Server 是以非阻塞的方式为 Slaves 提供服务。所以在 Master-Slave 同步期间，客户端仍然可以提交查询或修改请求； Slave Server 同样是以非阻塞的方式完成数据同步。在同步期间，如果有客户端提交查询请求，Redis则返回同步之前的数据； 缺点： Redis不具备自动容错和恢复功能，主机从机的宕机都会导致前端部分读写请求失败，需要等待机器重启或者手动切换前端的IP才能恢复（ 也就是要人工介入 ）； 主机宕机，宕机前有部分数据未能及时同步到从机，切换IP后还会引入数据不一致的问题，降低了系统的可用性； 如果多个 Slave 断线了，需要重启的时候，尽量不要在同一时间段进行重启。因为只要 Slave 启动，就会发送sync 请求和主机全量同步，当多个 Slave 重启的时候，可能会导致 Master IO 剧增从而宕机。 Redis 较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂； 9.2 哨兵模式（1）介绍 （2）优缺点 优点： 哨兵模式基于主从复制模式，所以主从复制模式有的优点，哨兵模式也有 哨兵模式下，master挂掉可以自动进行切换，系统可用性更高 缺点： 同样也继承了主从模式难以在线扩容的缺点，Redis的容量受限于单机配置 需要额外的资源来启动sentinel进程，实现相对复杂一点 9.3 Cluster模式10. Redis不建议作为消息队列（1）Redis 2.0 pubsub引入一个概念叫channel，生产者通过publish接口投递消息时会指定channel，消费者通过subscribe接口订阅它关心的channel，调用subscribe后这条连接会进入一个特殊的状态，通常不能在发送其他请求，当有消息投递到这个channel时Redis服务端会立刻通过该连接将消息推送到消费者。这里一个channel可以被多个应用订阅，消息会同时投递到每个订阅者，做到了消息的广播。 另一方面，消费者可以通过psubscribe订阅一批channel。 优势： 消息具备广播能力 psubscribe能按字符串通配符匹配，给予了业务逻辑的灵活性 能订阅特定key或特定命令的系统消息 不足： Redis异常、客户端断连都会导致消息丢失 消息缺乏堆积能力，不能削峰填谷。推送的方式缺乏背压机制，没有考虑消费者处理能力，推送的消息超过消费者处理能力后可能导致消息丢失或服务异常 （2）Redis 5.0 引入stream结构，这次考虑了list、pubsub在应用场景下的缺陷，对标kafka的模型重新设计全内存消息队列结构。 stream的改进分为多个方面 成本： 存储message数据使用了listpack结构，这是一个紧凑型的数据结构，不同于list的双向链表每个节点都要额外占用2个指针的存储空间，这使得小msg情况下stream的空间利用率更高。 功能： stream引入了消费者组的概念，一个消费者组内可以有多个消费者，同一个组内的消费者共享一个消息位点（last_delivered_id），这使得消费者能够水平的扩容，可以在一个组内加入多个消费者来线性的提升吞吐，对于一个消费者组，每条msg只会被其中一个消费者获取和处理，这是pubsub的广播模型不具备的。 不同消费者组之前是相互隔离的，他们各自维护自己的位点，这使得一条msg能被多个不同的消费者组重复消费，做到了消息广播的能力。 stream中消费者采用拉取的方式，并能设置timeout在没有消息时阻塞，通过这种长轮询机制保证了消息的实时性，而且消费速率是和消费者自身吞吐相匹配。 消息不丢失： stream的数据会存储在aof和rdb文件中，这使Redis重启后能够恢复stream的数据。而pubsub的数据是瞬时的，Redis重启意味着消息全部丢失。 stream中每个消费者组会存储一个last_delivered_id来标识已经读取到的位点，客户端连接断开后重连还是能从该位点继续读取，消息不会丢失。 stream引入了ack机制保证消息至少被处理一次。考虑一种场景，如果消费者应用已经读取了消息，但还没来得及处理应用就宕机了，对于这种已经读取但没有ack的消息，stream会标示这条消息的状态为pending，等客户端重连后通过xpending命令可以重新读取到pengind状态的消息，继续处理。如果这个应用永久宕机了，那么该消费者组内的其他消费者应用也能读取到这条消息，并通过xclaim命令将它归属到自己下面继续处理。 优势： stream的模型做到了消息的高效分发，而且保证了消息至少被处理一次，通过应用逻辑的改造能做到消息仅被处理一次，它的能力对标kafka，但吞吐高于kafka，在高吞吐场景下成本比kafka低。 不足： 首先消息队列很重要的一个功能就是削峰填谷，来匹配生产者和消费者吞吐的差异，生产者和消费者吞吐差异越大，持续时间越长，就意味着steam中需要堆积更多的消息，而Redis作为一个全内存的产品，数据堆积的成本比磁盘高。 （3）阿里Tair版本 优势： 引入了AEP作为存储介质，目前Tair持久内存版价格是社区版的70%。 保证了数据的实时持久化，并且通过半同步技术保证了HA不丢数据，大多数情况下做到消息不丢失（备库故障或主备网络异常时会降级为异步同步，优先保障可用性），消息至少被消费一次或仅被消费一次。 参考文档（1）https://javaguide.cn/database/redis/redis-questions-01.html （2）https://mp.weixin.qq.com/s/gCUT5TcCQRAxYkTJfTRjJw?poc_token=HJoJJGWjgzA_LiyHSQ2H-akNlFUfbGPewPbMtEeS （3）https://heapdump.cn/article/5133216 TODO（1）quicklist结构了解？","link":"/2023/09/25/distributed_redis/"},{"title":"JVM","text":"1. Java内存区域JDK1.8 内存区域如下图（与1.7的主要区别在于把堆上的的方法区转换为了本地内存的元空间）： （1）虚拟机栈 Java 虚拟机栈是线程私有的，它的生命周期和线程相同，随着线程的创建而创建，随着线程的死亡而死亡。栈由一个个栈帧组成，而每个栈帧中都拥有：局部变量表、操作数栈、动态链接、方法返回地址。 局部变量表：主要存放了编译期可知的各种数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference 类型，它不同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置）。 操作数栈：主要作为方法调用的中转站使用，用于存放方法执行过程中产生的中间计算结果。另外，计算过程中产生的临时变量也会放在操作数栈中。 动态链接：主要服务一个方法需要调用其他方法的场景。Class 文件的常量池里保存有大量的符号引用比如方法引用的符号引用。当一个方法要调用其他方法，需要将常量池中指向方法的符号引用转化为其在内存地址中的直接引用。动态链接的作用就是为了将符号引用转换为调用方法的直接引用，这个过程也被称为动态连接 。 方法返回：Java 方法有两种返回方式，一种是 return 语句正常返回，一种是抛出异常。不管哪种返回方式，都会导致栈帧被弹出。也就是说， 栈帧随着方法调用而创建，随着方法结束而销毁。无论方法正常完成还是异常完成都算作方法结束。 程序运行中栈可能会出现两种错误： StackOverFlowError： 若栈的内存大小不允许动态扩展，那么当线程请求栈的深度超过当前 Java 虚拟机栈的最大深度的时候，就抛出 StackOverFlowError 错误。 OutOfMemoryError： 如果栈的内存大小可以动态扩展， 如果虚拟机在动态扩展栈时无法申请到足够的内存空间，则抛出 OutOfMemoryError异常。 （2）本地方法栈 和虚拟机栈所发挥的作用非常相似，区别是：虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。 在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。 （3）堆 堆结构如下： 大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 S0 或者 S1，并且对象的年龄还会加 1(Eden 区-&gt;Survivor 区后对象的初始年龄变为 1)，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。 动态年龄计算：Hotspot 遍历所有对象时，按照年龄从小到大对其所占用的大小进行累积，当累积的某个年龄大小超过了 survivor 区的一半时，取这个年龄和 MaxTenuringThreshold 中更小的一个值，作为新的晋升年龄阈值。 （4）方法区 方法区属于是 JVM 运行时数据区域的一块逻辑区域，是各个线程共享的内存区域。当虚拟机要使用一个类时，它需要读取并解析 Class 文件获取相关信息，再将信息存入到方法区。方法区会存储已被虚拟机加载的 类信息、字段信息、方法信息、常量、静态变量、即时编译器编译后的代码缓存等数据。 方法区和永久代以及元空间是什么关系呢？ 方法区和永久代以及元空间的关系很像 Java 中接口和类的关系，类实现了接口，这里的类就可以看作是永久代和元空间，接口可以看作是方法区，也就是说永久代以及元空间是 HotSpot 虚拟机对虚拟机规范中方法区的两种实现方式。并且，永久代是 JDK 1.8 之前的方法区实现，JDK 1.8 及以后方法区的实现变成了元空间。 与永久代很大的不同就是，如果不指定大小的话，随着更多类的创建，虚拟机会耗尽所有可用的系统内存。 （5）运行时常量池 Class 文件中除了有类的版本、字段、方法、接口等描述信息外，还有用于存放编译期生成的各种字面量（Literal）和符号引用（Symbolic Reference）的 常量池表(Constant Pool Table) 。 字面量是源代码中的固定值的表示法，即通过字面我们就能知道其值的含义。字面量包括整数、浮点数和字符串字面量。常见的符号引用包括类符号引用、字段符号引用、方法符号引用、接口方法符号。 （6）字符串常量池 字符串常量池 是 JVM 为了提升性能和减少内存消耗针对字符串（String 类）专门开辟的一块区域，主要目的是为了避免字符串的重复创建。 JDK 1.7 为什么要将字符串常量池移动到堆中？主要是因为永久代（方法区实现）的 GC 回收效率太低，只有在整堆收集 (Full GC)的时候才会被执行 GC。Java 程序中通常会有大量的被创建的字符串等待回收，将字符串常量池放到堆中，能够更高效及时地回收字符串内存。 （7）直接内存 直接内存是一种特殊的内存缓冲区，并不在 Java 堆或方法区中分配的，而是通过 JNI 的方式在本地内存上分配的。 2. 垃圾回收Java 的自动内存管理主要是针对对象内存的回收和对象内存的分配。同时，Java 自动内存管理最核心的功能是堆内存中对象的分配与回收。 Java 堆是垃圾收集器管理的主要区域，因此也被称作 GC 堆（Garbage Collected Heap） 。 2.1 内存分配和回收原则（1）对象优先在 Eden 区分配 大多数情况下，对象在新生代中 Eden 区分配。当 Eden 区没有足够空间进行分配时，虚拟机将发起一次 Minor GC。 （2） 大对象直接进入老年代 大对象就是需要大量连续内存空间的对象（比如：字符串、数组）。 大对象直接进入老年代的行为是由虚拟机动态决定的，它与具体使用的垃圾回收器和相关参数有关。大对象直接进入老年代是一种优化策略，旨在避免将大对象放入新生代，从而减少新生代的垃圾回收频率和成本。 （3）长期存活的对象将进入老年代 既然虚拟机采用了分代收集的思想来管理内存，那么内存回收时就必须能识别哪些对象应放在新生代，哪些对象应放在老年代中。为了做到这一点，虚拟机给每个对象一个对象年龄（Age）计数器。 大部分情况，对象都会首先在 Eden 区域分配。如果对象在 Eden 出生并经过第一次 Minor GC 后仍然能够存活，并且能被 Survivor 容纳的话，将被移动到 Survivor 空间（s0 或者 s1）中，并将对象年龄设为 1(Eden 区-&gt;Survivor 区后对象的初始年龄变为 1)。 对象在 Survivor 中每熬过一次 MinorGC,年龄就增加 1 岁，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。同时，动态年龄计算也会实时修改年龄阈值。 （4）空间分配担保 空间分配担保是为了确保在 Minor GC 之前老年代本身还有容纳新生代所有对象的剩余空间。 《深入理解 Java 虚拟机》第三章对于空间分配担保的描述如下： JDK 6 Update 24 之前，在发生 Minor GC 之前，虚拟机必须先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果这个条件成立，那这一次 Minor GC 可以确保是安全的。如果不成立，则虚拟机会先查看 -XX:HandlePromotionFailure 参数的设置值是否允许担保失败(Handle Promotion Failure);如果允许，那会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试进行一次 Minor GC，尽管这次 Minor GC 是有风险的;如果小于，或者 -XX: HandlePromotionFailure 设置不允许冒险，那这时就要改为进行一次 Full GC。 JDK 6 Update 24 之后的规则变为只要老年代的连续空间大于新生代对象总大小或者历次晋升的平均大小，就会进行 Minor GC，否则将进行 Full GC。 2.2 死亡对象判断方法（1）引用计数法 给对象中添加一个引用计数器： 每当有一个地方引用它，计数器就加 1； 当引用失效，计数器就减 1； 任何时候计数器为 0 的对象就是不可能再被使用的。 这个方法实现简单，效率高，但是目前主流的虚拟机中并没有选择这个算法来管理内存，其最主要的原因是它很难解决对象之间循环引用的问题。 （2）可达性分析算法 这个算法的基本思想就是通过一系列的称为 “GC Roots” 的对象作为起点，从这些节点开始向下搜索，节点所走过的路径称为引用链，当一个对象到 GC Roots 没有任何引用链相连的话，则证明此对象是不可用的，需要被回收。 哪些对象可以作为 GC Roots 呢？ 虚拟机栈(栈帧中的局部变量表)中引用的对象 本地方法栈(Native 方法)中引用的对象 方法区中类静态属性引用的对象 方法区中常量引用的对象 所有被同步锁持有的对象 JNI（Java Native Interface）引用的对象 对象可以被回收，就代表一定会被回收吗？ 即使在可达性分析法中不可达的对象，也并非是“非死不可”的，这时候它们暂时处于“缓刑阶段”，要真正宣告一个对象死亡，至少要经历两次标记过程；可达性分析法中不可达的对象被第一次标记并且进行一次筛选，筛选的条件是此对象是否有必要执行 finalize 方法。当对象没有覆盖 finalize 方法，或 finalize 方法已经被虚拟机调用过时，虚拟机将这两种情况视为没有必要执行。 被判定为需要执行的对象将会被放在一个队列中进行第二次标记，除非这个对象与引用链上的任何一个对象建立关联，否则就会被真的回收。 （3）如何判断一个类是无用的类？ 类需要同时满足下面 3 个条件才能算是 “无用的类” ： 该类所有的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。 该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 虚拟机可以对满足上述 3 个条件的无用类进行回收，这里说的仅仅是“可以”，而并不是和对象一样不使用了就会必然被回收。 2.3 垃圾收集算法（1）标记-清除算法 标记-清除（Mark-and-Sweep）算法分为“标记（Mark）”和“清除（Sweep）”阶段：首先标记出所有不需要回收的对象，在标记完成后统一回收掉所有没有被标记的对象。 它是最基础的收集算法，后续的算法都是对其不足进行改进得到。这种垃圾收集算法会带来两个明显的问题： 效率问题：标记和清除两个过程效率都不高。 空间问题：标记清除后会产生大量不连续的内存碎片。 （2）复制算法 为了解决标记-清除算法的效率和内存碎片问题，复制（Copying）收集算法出现了。它可以将内存分为大小相同的两块，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。 虽然改进了标记-清除算法，但依然存在下面这些问题： 可用内存变小：可用内存缩小为原来的一半。 不适合老年代：如果存活对象数量比较大，复制性能会变得很差。 （3）标记-整理算法 标记-整理（Mark-and-Compact）算法是根据老年代的特点提出的一种标记算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象回收，而是让所有存活的对象向一端移动，然后直接清理掉端边界以外的内存。 由于多了整理这一步，因此效率也不高，适合老年代这种垃圾回收频率不是很高的场景。 （4）分代收集算法 当前虚拟机的垃圾收集都采用分代收集算法，这种算法没有什么新的思想，只是根据对象存活周期的不同将内存分为几块。一般将 Java 堆分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。 比如在新生代中，每次收集都会有大量对象死去，所以可以选择”标记-复制“算法，只需要付出少量对象的复制成本就可以完成每次垃圾收集。而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以我们必须选择“标记-清除”或“标记-整理”算法进行垃圾收集。 2.4 垃圾收集器以上是 HotSpot 虚拟机中的 7 个垃圾收集器，连线表示垃圾收集器可以配合使用。 单线程与多线程: 单线程指的是垃圾收集器只使用一个线程进行收集，而多线程使用多个线程； 串行与并行: 串行指的是垃圾收集器与用户程序交替执行，这意味着在执行垃圾收集的时候需要停顿用户程序；并形指的是垃圾收集器和用户程序同时执行。除了 CMS 和 G1 之外，其它垃圾收集器都是以串行的方式执行。 （1）Serial 收集器 Serial 翻译为串行，也就是说它以串行的方式执行。它是单线程的收集器，只会使用一个线程进行垃圾收集工作。 新生代采用标记-复制算法，老年代采用标记-整理算法。 它的优点是简单高效，对于单个 CPU 环境来说，由于没有线程交互的开销，因此拥有最高的单线程收集效率。 它是 Client 模式下的默认新生代收集器，因为在用户的桌面应用场景下，分配给虚拟机管理的内存一般来说不会很大。Serial 收集器收集几十兆甚至一两百兆的新生代停顿时间可以控制在一百多毫秒以内，只要不是太频繁，这点停顿是可以接受的。 （2）ParNew 收集器 ParNew 收集器其实就是 Serial 收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等等）和 Serial 收集器完全一样。 新生代采用标记-复制算法，老年代采用标记-整理算法。 它是许多运行在 Server 模式下的虚拟机的首要选择，除了 Serial 收集器外，只有它能与 CMS 收集器（真正意义上的并发收集器，后面会介绍到）配合工作。 并行和并发概念补充： 并行（Parallel） ：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。 并发（Concurrent） ：指用户线程与垃圾收集线程同时执行（但不一定是并行，可能会交替执行），用户程序在继续运行，而垃圾收集器运行在另一个 CPU 上。 （3）Parallel Scavenge 收集器 与 ParNew 一样是多线程收集器。这是 JDK1.8 默认收集器。 其它收集器关注点是尽可能缩短垃圾收集时用户线程的停顿时间，而它的目标是达到一个可控制的吞吐量，它被称为“吞吐量优先”收集器。这里的吞吐量指 CPU 用于运行用户代码的时间占总时间的比值。 停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户体验。而高吞吐量则可以高效率地利用 CPU 时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。 缩短停顿时间是以牺牲吞吐量和新生代空间来换取的: 新生代空间变小，垃圾回收变得频繁，导致吞吐量下降。 可以通过一个开关参数打开 GC 自适应的调节策略(GC Ergonomics)，就不需要手动指定新生代的大小(-Xmn)、Eden 和 Survivor 区的比例、晋升老年代对象年龄等细节参数了。虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量。 （4）Serial Old 收集器 是 Serial 收集器的老年代版本，也是给 Client 模式下的虚拟机使用。如果用在 Server 模式下，它有两大用途: 在 JDK 1.5 以及之前版本(Parallel Old 诞生以前)中与 Parallel Scavenge 收集器搭配使用。 作为 CMS 收集器的后备预案，在并发收集发生 Concurrent Mode Failure 时使用。 （5）Parallel Old 收集器 是 Parallel Scavenge 收集器的老年代版本。 在注重吞吐量以及 CPU 资源敏感的场合，都可以优先考虑 Parallel Scavenge 加 Parallel Old 收集器。 （6）CMS 收集器 CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。它非常符合在注重用户体验的应用上使用，是 HotSpot 虚拟机第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。 从名字中的Mark Sweep这两个词可以看出，CMS 收集器是一种 “标记-清除”算法实现的，它的运作过程相比于前面几种垃圾收集器来说更加复杂一些。整个过程分为四个步骤： 初始标记： 暂停所有的其他线程，并记录下直接与 root 相连的对象，速度很快 ； 并发标记： 同时开启 GC 和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以 GC 线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更新的地方。 重新标记： 重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短 并发清除： 开启用户线程，同时 GC 线程开始对未标记的区域做清扫。 在整个过程中耗时最长的并发标记和并发清除过程中，收集器线程都可以与用户线程一起工作，不需要进行停顿。具有以下缺点: 吞吐量低: 低停顿时间是以牺牲吞吐量为代价的，导致 CPU 利用率不够高。 无法处理浮动垃圾，可能出现 Concurrent Mode Failure。浮动垃圾是指并发清除阶段由于用户线程继续运行而产生的垃圾，这部分垃圾只能到下一次 GC 时才能进行回收。由于浮动垃圾的存在，因此需要预留出一部分内存，意味着 CMS 收集不能像其它收集器那样等待老年代快满的时候再回收。如果预留的内存不够存放浮动垃圾，就会出现 Concurrent Mode Failure，这时虚拟机将临时启用 Serial Old 来替代 CMS。 标记 - 清除算法导致的空间碎片，往往出现老年代空间剩余，但无法找到足够大连续空间来分配当前对象，不得不提前触发一次 Full GC。 （7）G1 收集器 G1(Garbage-First)，它是一款面向服务端应用的垃圾收集器，在多 CPU 和大内存的场景下有很好的性能。HotSpot 开发团队赋予它的使命是未来可以替换掉 CMS 收集器。从 JDK9 开始，G1 垃圾收集器成为了默认的垃圾收集器。 堆被分为新生代和老年代，其它收集器进行收集的范围都是整个新生代或者老年代，而 G1 可以直接对新生代和老年代一起回收。 G1 把堆划分成多个大小相等的独立区域(Region)，新生代和老年代不再物理隔离。每个 Region 都有一个 Remembered Set，用来记录该 Region 对象的引用对象所在的 Region。通过使用 Remembered Set，在做可达性分析的时候就可以避免全堆扫描。 通过引入 Region 的概念，从而将原来的一整块内存空间划分成多个的小空间，使得每个小空间可以单独进行垃圾回收。这种划分方法带来了很大的灵活性，使得可预测的停顿时间模型成为可能。通过记录每个 Region 垃圾回收时间以及回收所获得的空间(这两个值是通过过去回收的经验获得)，并维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的 Region。。这种使用 Region 划分内存空间以及有优先级的区域回收方式，保证了 G1 收集器在有限时间内可以尽可能高的收集效率（把内存化整为零）。 G1被视为 JDK1.7 中 HotSpot 虚拟机的一个重要进化特征。它具备以下特点： 并行与并发 ：G1 能充分利用 CPU、多核环境下的硬件优势，使用多个 CPU（CPU 或者 CPU 核心）来缩短 Stop-The-World 停顿时间。部分其他收集器原本需要停顿 Java 线程执行的 GC 动作，G1 收集器仍然可以通过并发的方式让 java 程序继续执行。 分代收集 ：虽然 G1 可以不需要其他收集器配合就能独立管理整个 GC 堆，但是还是保留了分代的概念。 空间整合 ：与 CMS 的“标记-清除”算法不同，G1 从整体来看是基于“标记-整理”算法实现的收集器；从局部(两个 Region 之间)上来看是基于“复制”算法实现的，这意味着运行期间不会产生内存空间碎片。 可预测的停顿 ：这是 G1 相对于 CMS 的另一个大优势，降低停顿时间是 G1 和 CMS 共同的关注点，但 G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 M 毫秒的时间片段内，消耗在垃圾收集上的时间不得超过 N 毫秒。 如果不计算维护 Remembered Set 的操作，G1 收集器的运作大致可划分为以下几个步骤: 初始标记 并发标记 最终标记: 为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程的 Remembered Set Logs 里面，最终标记阶段需要把 Remembered Set Logs 的数据合并到 Remembered Set 中。这阶段需要停顿线程，但是可并行执行。 筛选回收: 首先对各个 Region 中的回收价值和成本进行排序，根据用户所期望的 GC 停顿时间来制定回收计划。此阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分 Region，时间是用户可控制的，而且停顿用户线程将大幅度提高收集效率。 2.5 GC 分类针对 HotSpot VM 的实现，它里面的 GC 按照回收区域主要分为两大类：部分收集（Partial GC），整堆收集（Full GC） 部分收集：不是完整收集整个 Java 堆的垃圾收集。其中又分为： 新生代收集（Minor GC/Young GC）：只是新生代的垃圾收集 老年代收集（Major GC/Old GC）：只是老年代的垃圾收集 目前，只有 CMS会有单独收集老年代的行为 很多时候 Major GC 会和 Full GC 混合使用，需要具体分辨是老年代回收还是整堆回收 混合收集（Mixed GC）：收集整个新生代以及部分老年代的垃圾收集 目前只有 G1会有这种行为 整堆收集（Full GC）：收集整个 Java 堆和方法区的垃圾 注：对于 Minor GC，其触发条件非常简单，当 Eden 空间满时，就将触发一次 Minor GC。而 Full GC 则相对复杂，有以下条件: （1）调用 System.gc() 只是建议虚拟机执行 Full GC，但是虚拟机不一定真正去执行。不建议使用这种方式，而是让虚拟机管理内存。 （2）老年代空间不足 老年代空间不足的常见场景为前文所讲的大对象直接进入老年代、长期存活的对象进入老年代等。 为了避免以上原因引起的 Full GC，应当尽量不要创建过大的对象以及数组。除此之外，可以通过 -Xmn 虚拟机参数调大新生代的大小，让对象尽量在新生代被回收掉，不进入老年代。还可以通过 -XX:MaxTenuringThreshold 调大对象进入老年代的年龄，让对象在新生代多存活一段时间。 （3）空间分配担保失败 使用复制算法的 Minor GC 需要老年代的内存空间作担保，如果担保失败会执行一次 Full GC。 （4） JDK 1.7 及以前的永久代空间不足 在 JDK 1.7 及以前，HotSpot 虚拟机中的方法区是用永久代实现的，永久代中存放的为一些 Class 的信息、常量、静态变量等数据。 当系统中要加载的类、反射的类和调用的方法较多时，永久代可能会被占满，在未配置为采用 CMS GC 的情况下也会执行 Full GC。如果经过 Full GC 仍然回收不了，那么虚拟机会抛出 java.lang.OutOfMemoryError。 为避免以上原因引起的 Full GC，可采用的方法为增大永久代空间或转为使用 CMS GC。 2.6 JDK命令行工具常见GC查看工具如下： （1）jps jps(JVM Process Status) 命令类似 UNIX 的 ps 命令，用于查看所有 Java 进程。 1234567jps: 显示虚拟机执行主类名称以及这些进程的本地虚拟机唯一 ID（Local Virtual Machine Identifier,LVMID）参数：-l: 输出主类的全名，如果进程执行的是 Jar 包，输出 Jar 路径-v：输出虚拟机进程启动时 JVM 参数-m：输出传递给 Java 进程 main() 函数的参数 （2）jstat jstat（JVM Statistics Monitoring Tool） 使用于监视虚拟机各种运行状态信息的命令行工具。 它可以显示本地或者远程（需要远程主机提供 RMI 支持）虚拟机进程中的类信息、内存、垃圾收集、JIT 编译等运行数据，在没有 GUI，只提供了纯文本控制台环境的服务器上，它将是运行期间定位虚拟机性能问题的首选工具。 12345678910- `jstat -class vmid`：显示 ClassLoader 的相关信息；- `jstat -compiler vmid`：显示 JIT 编译的相关信息；- `jstat -gc vmid`：显示与 GC 相关的堆信息；- `jstat -gccapacity vmid`：显示各个代的容量及使用情况；- `jstat -gcnew vmid`：显示新生代信息；- `jstat -gcnewcapcacity vmid`：显示新生代大小与使用情况；- `jstat -gcold vmid`：显示老年代和永久代的行为统计，从 jdk1.8 开始,该选项仅表示老年代，因为永久代被移除了；- `jstat -gcoldcapacity vmid`：显示老年代的大小；- `jstat -gcpermcapacity vmid`：显示永久代大小，从 jdk1.8 开始,该选项不存在了，因为永久代被移除了；- `jstat -gcutil vmid`：显示垃圾收集信息； （3）jinfo jinfo可以实时地查看虚拟机各项参数，并且可以在不重启虚拟机的情况下，动态地修改 jvm 的参数，这一点在线上的环境特别有用。 123- jinfo vmid :输出当前 jvm 进程的全部参数和系统属性 (第一部分是系统的属性，第二部分是 JVM 的参数)。- jinfo -flag name vmid :输出对应名称的参数的具体值。- jinfo -flag [+|-]name vmid 开启或者关闭对应名称的参数。 （4）jmap jmap（Memory Map for Java）命令用于生成堆转储快照。 如果不使用 jmap 命令，要想获取 Java 堆转储，可以使用 “-XX:+HeapDumpOnOutOfMemoryError” 参数，可以让虚拟机在 OOM 异常出现之后自动生成 dump 文件，Linux 命令下可以通过 kill -3 发送进程退出信号也能拿到 dump 文件。 jmap 的作用并不仅仅是为了获取 dump 文件，它还可以查询 finalizer 执行队列、Java 堆和永久代的详细信息，如空间使用率、当前使用的是哪种收集器等。 （5）jstack jstack（Stack Trace for Java）命令用于生成虚拟机当前时刻的线程快照。线程快照就是当前虚拟机内每一条线程正在执行的方法堆栈的集合。 生成线程快照的目的主要是定位线程长时间出现停顿的原因，如线程间死锁、死循环、请求外部资源导致的长时间等待等都是导致线程长时间停顿的原因。线程出现停顿的时候通过jstack来查看各个线程的调用堆栈，就可以知道没有响应的线程到底在后台做些什么事情，或者在等待些什么资源。 （6）jhat jhat 用于分析 heapdump 文件，它会建立一个 HTTP/HTML 服务器，让用户可以在浏览器上查看分析结果。 3. 类加载类从被加载到虚拟机内存中开始到卸载出内存为止，它的整个生命周期可以简单概括为 7 个阶段：：加载（Loading）、验证（Verification）、准备（Preparation）、解析（Resolution）、初始化（Initialization）、使用（Using）和卸载（Unloading）。其中，验证、准备和解析这三个阶段可以统称为连接（Linking）。 类加载过程： 加载-&gt;连接-&gt;初始化 。 连接过程又可分为三步： 验证-&gt;准备-&gt;解析 。 加载是类加载过程的第一步，主要完成下面 3 件事情： 通过全类名获取定义此类的二进制字节流 将字节流所代表的静态存储结构转换为方法区的运行时数据结构 在内存中生成一个代表该类的 Class 对象，作为方法区这些数据的访问入口 3.1 类加载器（1）介绍 简单来说，类加载器的主要作用就是加载 Java 类的字节码（ .class 文件）到 JVM 中（在内存中生成一个代表该类的 Class 对象）。 字节码可以是 Java 源程序（.java文件）经过 javac 编译得来，也可以是通过工具动态生成或者通过网络下载得来。 其实除了加载类之外，类加载器还可以加载 Java 应用所需的资源如文本、图像、配置文件、视频等等文件资源。 需要注意的是： 每个 Java 类都有一个引用指向加载它的 ClassLoader。 数组类不是通过 ClassLoader 创建的（数组类没有对应的二进制字节流），是由 JVM 直接生成的。 （2）加载规则 JVM 启动的时候，并不会一次性加载所有的类，而是根据需要去动态加载。也就是说，大部分类在具体用到的时候才会去加载，这样对内存更加友好。 对于已经加载的类会被放在 ClassLoader 中。在类加载的时候，系统会首先判断当前类是否被加载过。已经被加载的类会直接返回，否则才会尝试加载。也就是说，对于一个类加载器来说，相同二进制名称的类只会被加载一次。 （3）类加载器分类 JVM 中内置了三个重要的 ClassLoader： BootstrapClassLoader(启动类加载器) ：最顶层的加载类，由 C++实现，通常表示为 null，并且没有父级，主要用来加载 JDK 内部的核心类库（ %JAVA_HOME%/lib目录下的 rt.jar、resources.jar、charsets.jar等 jar 包和类）以及被 -Xbootclasspath参数指定的路径下的所有类。 ExtensionClassLoader(扩展类加载器) ：主要负责加载 %JRE_HOME%/lib/ext 目录下的 jar 包和类以及被 java.ext.dirs 系统变量所指定的路径下的所有类。 AppClassLoader(应用程序类加载器) ：面向我们用户的加载器，负责加载当前应用 classpath 下的所有 jar 包和类。 除了这三种类加载器之外，用户还可以加入自定义的类加载器来进行拓展，以满足自己的特殊需求。就比如说，我们可以对 Java 类的字节码（ .class 文件）进行加密，加载时再利用自定义的类加载器对其解密。 拓展一下： rt.jar ：rt 代表“RunTime”，rt.jar是 Java 基础类库，包含 Java doc 里面看到的所有的类的类文件。也就是说，我们常用内置库 java.xxx.*都在里面，比如 java.util.*、java.io.*、java.nio.*、java.lang.*、java.sql.*、java.math.*。 Java 9 引入了模块系统，并且略微更改了上述的类加载器。扩展类加载器被改名为平台类加载器（platform class loader）。Java SE 中除了少数几个关键模块，比如说 java.base 是由启动类加载器加载之外，其他的模块均由平台类加载器所加载。 除了 BootstrapClassLoader 是 JVM 自身的一部分之外，其他所有的类加载器都是在 JVM 外部实现的，并且全都继承自 ClassLoader抽象类。这样做的好处是用户可以自定义类加载器，以便让应用程序自己决定如何去获取所需的类。 每个 ClassLoader 可以通过 getParent()获取其父 ClassLoader，如果获取到 ClassLoader 为 null的话，那么该类是通过 BootstrapClassLoader 加载的（这是因为 BootstrapClassLoader 由 C++ 实现，由于这个 C++ 实现的类加载器在 Java 中是没有与之对应的类的，所以拿到的结果是 null）。 （4）自定义类加载器 除了 BootstrapClassLoader 外，其他类加载器均由 Java 实现且全部继承自 java.lang.ClassLoader。如果我们要自定义自己的类加载器，很明显需要继承 ClassLoader抽象类。 ClassLoader 类有两个关键的方法： protected Class loadClass(String name, boolean resolve)：加载指定二进制名称的类，实现了双亲委派机制 。name 为类的二进制名称，resolve 如果为 true，在加载时调用 resolveClass(Class&lt;?&gt; c) 方法解析该类。 protected Class findClass(String name)：根据类的二进制名称来查找类，默认实现是空方法。 如果我们不想打破双亲委派模型，就重写 ClassLoader 类中的 findClass() 方法即可，无法被父类加载器加载的类最终会通过这个方法被加载。但是，如果想打破双亲委派模型则需要重写 loadClass() 方法。 3.2 双亲委派模型（1）介绍 双亲委派模型包含以下几方面： ClassLoader 类使用委托模型来搜索类和资源。 双亲委派模型要求除了顶层的启动类加载器外，其余的类加载器都应有自己的父类加载器。 ClassLoader 实例会在试图亲自查找类或资源之前，将搜索类或资源的任务委托给其父类加载器。 另外，类加载器之间的父子关系一般不是以继承的关系来实现的，而是通常使用组合关系来复用父加载器的代码。（组合优于继承，多用组合少用继承。） （2）执行流程 简单总结一下双亲委派模型的执行流程： 在类加载的时候，系统会首先判断当前类是否被加载过。已经被加载的类会直接返回，否则才会尝试加载（每个父类加载器都会走一遍这个流程）。 类加载器在进行类加载的时候，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成（调用父加载器 loadClass()方法来加载类）。这样的话，所有的请求最终都会传送到顶层的启动类加载器 BootstrapClassLoader 中。 只有当父加载器反馈自己无法完成这个加载请求（它的搜索范围中没有找到所需的类）时，子加载器才会尝试自己去加载（调用自己的 findClass() 方法来加载类）。 如果子类加载器也无法加载这个类，那么它会抛出一个 ClassNotFoundException 异常。 源码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778/** * Loads the class with the specified &lt;a href=&quot;#name&quot;&gt;binary name&lt;/a&gt;. The * default implementation of this method searches for classes in the * following order: * * &lt;ol&gt; * * &lt;li&gt;&lt;p&gt; Invoke {@link #findLoadedClass(String)} to check if the class * has already been loaded. &lt;/p&gt;&lt;/li&gt; * * &lt;li&gt;&lt;p&gt; Invoke the {@link #loadClass(String) &lt;tt&gt;loadClass&lt;/tt&gt;} method * on the parent class loader. If the parent is &lt;tt&gt;null&lt;/tt&gt; the class * loader built-in to the virtual machine is used, instead. &lt;/p&gt;&lt;/li&gt; * * &lt;li&gt;&lt;p&gt; Invoke the {@link #findClass(String)} method to find the * class. &lt;/p&gt;&lt;/li&gt; * * &lt;/ol&gt; * * &lt;p&gt; If the class was found using the above steps, and the * &lt;tt&gt;resolve&lt;/tt&gt; flag is true, this method will then invoke the {@link * #resolveClass(Class)} method on the resulting &lt;tt&gt;Class&lt;/tt&gt; object. * * &lt;p&gt; Subclasses of &lt;tt&gt;ClassLoader&lt;/tt&gt; are encouraged to override {@link * #findClass(String)}, rather than this method. &lt;/p&gt; * * &lt;p&gt; Unless overridden, this method synchronizes on the result of * {@link #getClassLoadingLock &lt;tt&gt;getClassLoadingLock&lt;/tt&gt;} method * during the entire class loading process. * * @param name * The &lt;a href=&quot;#name&quot;&gt;binary name&lt;/a&gt; of the class * * @param resolve * If &lt;tt&gt;true&lt;/tt&gt; then resolve the class * * @return The resulting &lt;tt&gt;Class&lt;/tt&gt; object * * @throws ClassNotFoundException * If the class could not be found */protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException{ synchronized (getClassLoadingLock(name)) { // First, check if the class has already been loaded Class&lt;?&gt; c = findLoadedClass(name); if (c == null) { long t0 = System.nanoTime(); try { if (parent != null) { c = parent.loadClass(name, false); } else { c = findBootstrapClassOrNull(name); } } catch (ClassNotFoundException e) { // ClassNotFoundException thrown if class not found // from the non-null parent class loader } if (c == null) { // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); } } if (resolve) { resolveClass(c); } return c; }} 注：JVM 判定两个 Java 类是否相同的具体规则 ：JVM 不仅要看类的全名是否相同，还要看加载此类的类加载器是否一样。只有两者都相同的情况，才认为两个类是相同的。即使两个类来源于同一个 Class 文件，被同一个虚拟机加载，只要加载它们的类加载器不同，那这两个类就必定不相同。 （3） 双亲委派模型的好处 双亲委派模型保证了 Java 程序的稳定运行，可以避免类的重复加载（JVM 区分不同类的方式不仅仅根据类名，相同的类文件被不同的类加载器加载产生的是两个不同的类），也保证了 Java 的核心 API 不被篡改。 如果没有使用双亲委派模型，而是每个类加载器加载自己的话就会出现一些问题，比如我们编写一个称为 java.lang.Object 类的话，那么程序运行的时候，系统就会出现两个不同的 Object 类。双亲委派模型可以保证加载的是 JRE 里的那个 Object 类，而不是你写的 Object 类。这是因为 AppClassLoader 在加载你的 Object 类时，会委托给 ExtClassLoader 去加载，而 ExtClassLoader 又会委托给 BootstrapClassLoader，BootstrapClassLoader 发现自己已经加载过了 Object 类，会直接返回，不会去加载你写的 Object 类。 （4）打破双亲委派模型方法 自定义加载器的话，需要继承 ClassLoader 。如果我们不想打破双亲委派模型，就重写 ClassLoader 类中的 findClass() 方法即可，无法被父类加载器加载的类最终会通过这个方法被加载。但是，如果想打破双亲委派模型则需要重写 loadClass() 方法。例如，子类加载器可以在委派给父类加载器之前，先自己尝试加载这个类，或者在父类加载器返回之后，再尝试从其他地方加载这个类。具体的规则由我们自己实现，根据项目需求定制化。 我们比较熟悉的 Tomcat 服务器为了能够优先加载 Web 应用目录下的类，然后再加载其他目录下的类，就自定义了类加载器 WebAppClassLoader 来打破双亲委托机制。这也是 Tomcat 下 Web 应用之间的类实现隔离的具体原理。 从图中的委派关系中可以看出： CommonClassLoader作为 CatalinaClassLoader 和 SharedClassLoader 的父加载器。CommonClassLoader 能加载的类都可以被 CatalinaClassLoader 和 SharedClassLoader 使用。因此，CommonClassLoader 是为了实现公共类库（可以被所有 Web 应用和 Tomcat 内部组件使用的类库）的共享和隔离。 CatalinaClassLoader 和 SharedClassLoader 能加载的类则与对方相互隔离。CatalinaClassLoader 用于加载 Tomcat 自身的类，为了隔离 Tomcat 本身的类和 Web 应用的类。SharedClassLoader 作为 WebAppClassLoader 的父加载器，专门来加载 Web 应用之间共享的类比如 Spring、Mybatis。 每个 Web 应用都会创建一个单独的 WebAppClassLoader，并在启动 Web 应用的线程里设置线程线程上下文类加载器为 WebAppClassLoader。各个 WebAppClassLoader 实例之间相互隔离，进而实现 Web 应用之间的类隔。 3.3 上下文类加载器单纯依靠自定义类加载器没办法满足某些场景的要求，例如，有些情况下，高层的类加载器需要加载低层的加载器才能加载的类。 比如，SPI 中，SPI 的接口（如 java.sql.Driver）是由 Java 核心库提供的，由 BootstrapClassLoader 加载。而 SPI 的实现（如 com.mysql.cj.jdbc.Driver）是由第三方供应商提供的，它们是由应用程序类加载器或者自定义类加载器来加载的。默认情况下，一个类及其依赖类由同一个类加载器加载。所以，加载 SPI 的接口的类加载器（BootstrapClassLoader）也会用来加载 SPI 的实现。按照双亲委派模型，BootstrapClassLoader 是无法找到 SPI 的实现类的，因为它无法委托给子类加载器去尝试加载。 再比如，假设我们的项目中有 Spring 的 jar 包，由于其是 Web 应用之间共享的，因此会由 SharedClassLoader 加载（Web 服务器是 Tomcat）。我们项目中有一些用到了 Spring 的业务类，比如实现了 Spring 提供的接口、用到了 Spring 提供的注解。所以，加载 Spring 的类加载器（也就是 SharedClassLoader）也会用来加载这些业务类。但是业务类在 Web 应用目录下，不在 SharedClassLoader 的加载路径下，所以 SharedClassLoader 无法找到业务类，也就无法加载它们。 如何解决这个问题呢？ 这个时候就需要用到 线程上下文类加载器（ThreadContextClassLoader） 了。 拿 Spring 这个例子来说，当 Spring 需要加载业务类的时候，它不是用自己的类加载器，而是用当前线程的上下文类加载器。还记得我上面说的吗？每个 Web 应用都会创建一个单独的 WebAppClassLoader，并在启动 Web 应用的线程里设置线程线程上下文类加载器为 WebAppClassLoader。这样就可以让高层的类加载器（SharedClassLoader）借助子类加载器（ WebAppClassLoader）来加载业务类，破坏了 Java 的类加载委托机制，让应用逆向使用类加载器。 线程线程上下文类加载器的原理是将一个类加载器保存在线程私有数据里，跟线程绑定，然后在需要的时候取出来使用。这个类加载器通常是由应用程序或者容器（如 Tomcat）设置的。 Java.lang.Thread 中的 getContextClassLoader()和 setContextClassLoader(ClassLoader cl)分别用来获取和设置线程的上下文类加载器。如果没有通过 setContextClassLoader(ClassLoader cl)进行设置的话，线程将继承其父线程的上下文类加载器。 参考文档（1）Java内存区域详解：https://javaguide.cn/java/jvm/memory-area.html （2）JVM垃圾回收详解：https://javaguide.cn/java/jvm/jvm-garbage-collection.html （3）GC - Java 垃圾回收基础知识：https://www.pdai.tech/md/java/jvm/java-jvm-gc.html （4）类加载器详解: https://javaguide.cn/java/jvm/classloader.htm 遗留TODO（1）方法区和运行时常量池深入了解 （2）hotspot虚拟机对象的创建、布局、访问定位等问题 （3）ZGC 垃圾收集器了解 （4）深入了解jvm常见问题排查思路（https://javaguide.cn/java/jvm/jvm-parameters-intro.html#%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90、https://www.pdai.tech/md/java/jvm/java-jvm-gc-g1.html#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99） （5）WebAppClassLoader深入了解？ （6）组合优于继承了解","link":"/2023/09/25/java_jvm/"},{"title":"git日常使用","text":"1. git revert线上（master分支）需要回滚代码，或者不小心把开发分支的代码合到了master分支，此时优先revert代码。 revert之后，再往master分支合代码，已revert的提交不会出现在master上（相关的文件修改也不会出现），此时需要从maste拉一个新分支branch1，在branch1上重新revert master， 1git revert -n revert-commit-id revert-commit-id是master上执行revert操作的commit id，如下图： 然后执行下述命令即可: 123git add -Agit commit -m &quot;revert master&quot; 接着正常往master上合feature代码即可。","link":"/2024/04/10/git/"}],"tags":[{"name":"数据库","slug":"数据库","link":"/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"todo","slug":"todo","link":"/tags/todo/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"设计模式","slug":"设计模式","link":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"分布式","slug":"分布式","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"缓存","slug":"缓存","link":"/tags/%E7%BC%93%E5%AD%98/"},{"name":"并发","slug":"并发","link":"/tags/%E5%B9%B6%E5%8F%91/"},{"name":"消息队列","slug":"消息队列","link":"/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"k8s","slug":"k8s","link":"/tags/k8s/"},{"name":"监控","slug":"监控","link":"/tags/%E7%9B%91%E6%8E%A7/"},{"name":"网络","slug":"网络","link":"/tags/%E7%BD%91%E7%BB%9C/"},{"name":"git","slug":"git","link":"/tags/git/"}],"categories":[{"name":"数据库","slug":"数据库","link":"/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"算法","slug":"算法","link":"/categories/%E7%AE%97%E6%B3%95/"},{"name":"设计模式","slug":"设计模式","link":"/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"分布式","slug":"分布式","link":"/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"Java","slug":"Java","link":"/categories/Java/"},{"name":"k8s","slug":"k8s","link":"/categories/k8s/"},{"name":"网络","slug":"网络","link":"/categories/%E7%BD%91%E7%BB%9C/"},{"name":"git","slug":"git","link":"/categories/git/"}],"pages":[{"title":"About me","text":"一枚小码农","link":"/about/index.html"}]}